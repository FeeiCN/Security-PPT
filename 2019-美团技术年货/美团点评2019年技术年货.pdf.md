
   2013  12  4   6   376 meituantech 20   2020   900      





 MTFlexbox  Litho  MTFlexbox   React Native  beeshell 2.0  React Native   Android  ProbeAndroid  OOM   Web  React Native  TSLint  ESLint  App   iOS  zsource    Bifrost 

iii
1
2 2 14 28 61 85 85 115 135 144 165 182 182 190 198 219

<v

 Litho  Android  Java 8  

235 235 252 270



290

 Java Unsafe  Java   JVM CPU Profiler  Java   ReentrantLock  AQS 

291 291 308 320 347 370 399

  Kubernetes   HULK   IDC  HIDS  Leaf ID   OCTO 

435 435 450 463 486 493

vi> 2019 

 OCTO2.0 
 XGBoost  Spring Boot ""    

499
515 515 524 536 547 565 580



592

 BERT 

593



616



648



671

AI Challenger 2018 695

WSDM Cup 2019 

704

 ETA 

716



729

ICDAR 2019 

745

<vii

CVPR 2019 

757

 StarNet 

764


Hadoop YARN  OneData SaaS   Jupyter 

774
775 794 812 829 843


MIT "" NLP  

865
865 881



890

The 26th ACM SIGSPATIAL International Conference on Advances

in Geographic Information Systems

891

2019 IEEE/RSJ International Conference on Intelligent Robots

and Systems

900

viii> 2019 

2019 INFORMS Annual Meeting

906

2019 International Joint Conference on Artificial Intelligence 915

The 25th ACM SIGKDD Conference on Knowledge Discovery

and Data Mining

922

The Web Conference 2019

932

The 15th International Conference on Document Analysis and

Recognition

943


  HTML  JS        App   

2> 2019 



MTFlexbox 



  App   "" Bug  ----MTFlexbox
MTFlexbox   MTFlexbox      MTFlexbox  MTFlexbox   MTFlexbox 
MTFlexbox 
MTFlexbox 
MTFlexbox  CSS3  Flexbox MTFlexbox  Flexbox 

<3  XML   JSON  JSON  Native  MTFlexbox 
MTFlexbox 
 MTFlexbox  XML   Native  Android  XML                                  MTFlexbox  XML  XML   UI  UI   XML   View MTFlexbox  XML  View  Node   XML  Node  Node  View MTFlexbox  3   XML  Node  View  View  XML  View rootView Node  View  JSON  Native 

4> 2019 
MTFlexbox 
MTFlexbox  Native  UI   MTFlexbox  XML  MTFlexbox   App  
 
  MTFlexbox 
MTFlexbox 
   MTFlexbox  UI MTFlexbox 

<5 
  Tag  MTFlexbox  
  Native   see-mge4-report  
<Container style="width:360pt;justify-content:center;" > <Var name="see_MGE4" type="json"> { "bid":"xxxxx", "cid":"yyyyy", "lab":{ "isDynamic":true, "gather_index":"{extra.gather_index}", "index":"{extra.index}"

6> 2019 
} } </Var> <Container
see-mge4-report="{see_MGE4}" click-url="{business.iUrl}" visibility="{{display.itemDynamic. imageUrl}?visible:displaynone}" > <Img style="width:331pt;height:106pt;justify-content:center;"
border-radius="5pt" scale-type="center-crop" src="{display.itemDynamic.imageUrl}" background="#FFF8F8F8" /> </Container> </Container>
MTFlexbox 
MTFlexbox 
 MTFlexbox   
  
 

<7   5PD 5PD  5  1PD     XML  30%        ""  MTFlexbox                                    MTFlexbox "" ""   


8> 2019 


   
Mixpanel
Mixpanel  IDE   
HubbleData
HubbleData  

<9
 XPath  View    XPath  ViewGroup  View 
  0       Button    XPath   LinearLayout[0]/ FrameLayout[0]/RelativeLayout[1]/Button[1]    XPath       ViewGroup     View                             0       Button  XPath LinearLayout[0]#rootView/FrameLayout[0]/ RelativeLayout[0]#container/Button[0]#btn      

10> 2019 


MTFlexbox  1.  XML  XML 
 MTFlexbox  2. MTFlexbox 
  3.  DPath    

<11

4.     RD 

5. MTFlexbox  XML  C 

12> 2019 
<?xml version="1.0" encoding="UTF-8"?> <Container>
<Var auto-mge="true" name="ff510aa110844bb78c0b86fb04b26460" type="json">
{ "bid" : "xxxxx", "cid" : "sssss", "lab" : { "index" : "{_index}", }
} </Var>
<!--  --> <Container background="#FFFFFF" border-radius="10pt"
click-mge4-report="{ff510aa110844bb78c0b86fb04b26460}" click-url="{_iUrl}" padding-left="10pt" padding-right="10pt"> <!--  --> <Container style="flex-direction:column;justify-content:flexstart;margin-top:15pt;">
6.  XML  

 MTFlexbox   80%  
 MTFlexbox  View   

[1]  HubbleData  Android  [2]  mixpanel [3] 

<13



  DAU   /  30  PM /    App   AndroidiOSReactNativeFlexbox   AndroidiOS   tech@meituan.com

14> 2019 
Litho  MTFlexbox 

MTFlexbox
MTFlexbox    CSS3     Flexbox           MTFlexbox      HTMLReact NativeWeex  MTFlexbox   MTFlexbox   MTFlexbox  Litho 
MTFlexbox 
MTFlexbox  DSL " " JSON MTFlexbox 
 1MTFlexbox 

<15    MTFlexbox  Flexbox  DSL
XML    XML 
  
   Flexbox 
 Native      MTFlexbox  MTFlexbox  XML  Java   JSON   Native  
 2

16> 2019 
MTFlexbox 
 MTFlexbox     
 
MTFlexbox     Flexbox  Flexbox        Android LinearLayout Flexbox     8 
 3
   FPS 

<17
 
RecyclerView  MTFlexbox    MTFlexbox  JSON   MTFlexbox  FPS  
 4
  RecyclerView  

18> 2019 
Litho
Litho 
Litho  UI  RecyclerView Litho   RecyclerView  Litho 
Litho 
 Litho  Litho  RecyclerView  
     

 MTFlexbox   MTFlexbox  ---- Litho   Litho  MTFlexbox  MTFlexbox  
Litho + MTFlexbox 

Litho  Litho  MTFlexbox  XML   Litho  Litho   Litho 

<19  LithoView LithoView  Litho  View   Litho 
 5Litho 
 4  
 ----  MTFlexbox    Text Text  Litho  Prop  Litho  Prop    State  Prop   Litho     Updater Updater  State 

20> 2019 
 6
 Litho    
 7
 
Litho  MTFlexbox  Flexbox  MTFlexbox ----Layer Layer  

<21  Layer  z   Layer   Litho  Flexbox  Layer   Layer Flexbox   Flexbox   Layer  Flexbox  Flexbox   Litho  Flexbox  Layer  Litho   Flexbox  Litho  Flexbox 
 8Litho 
Litho  Flexbox  Yoga  Litho   Yoga  Yoga   Yoga   Layer  Layer  Layer  
  

22> 2019    Yoga  Layer 
 9Layer 
 Layer  Layer  Yoga Yoga  Layer   Layer Layer   Layer 
Litho  Litho Litho   Litho   Litho     State  State   State   Litho  FrescoImage  FrescoImage  DraweeDrawable  DraweeDrawable  Drawable 

<23 DraweeDrawable    Glide GlideDrawable 
 MTFlexbox  Litho   Tag     MTFlexbox                    View            Litho                    MTFlexbox            Litho                  MTFlexbox   Litho  View  Litho  View  
 10Litho  View 
 Litho  Android  View 

24> 2019    
   MTFlexbox   Litho  MTFlexbox 
 MTFlexbox   20  Litho  MTFlexbox  MTFlexbox  30M  

<25

 1MTFlexbox  (2) MTFlexbox   "" Litho         Litho               Litho     RecyclerView   MTFlexbox  MTFlexbox   Litho  3.2 ""

26> 2019 
 MTFlexbox  Litho   LithoView 
  Litho  FPS   MTFlexbox  2  

Litho + MTFlexbox  App  

<27
  Litho  
  RecyclerView  API   View 


Litho Flexbox 



  DAU   /  30 
  /   App   AndroidiOSReactNativeFlexbox 
 AndroidiOS tech@ meituan.com

28> 2019 
 React Native  beeshell 2.0 


 React Native RN 
 RN   RN 2018  11   RN  beeshell 1.0 1.0   RN   beeshell  2.0  3833  5  
beeshell 2.0 

<29

 Web  Native GUI  Web  Native   MV* AngularJS ReactVue.js  
  RN  
   
 UI  UI  
  
   
 RN  iOSAndroidWeb  


30> 2019 

  
""  
""  



<31

32> 2019 
beeshell 2.0  1.0  RN  beeshell 2.0 


 1.0  2.0  MTD Roo  beeshell 
"" 
 MTD  
 Roo  MTD  UI  
 beeshell 2.0  Roo  Roo   
 beeshell  

  
   beeshell 1.0   F    ""

<33
  beeshell 1.0     ""
 RN iOSAndroidWeb   API   
RN 
 MRNMRN  RN  RN  
MRN  RN MRN   RN 
 RN MRN  
  
   MRN    
  TypeScript TS TS  

34> 2019 

 beeshell 1.0  beeshell 2.0  MTDRoo  Git Fork   
MTD  beeshell 1.0  Roo  MTDbeeshell 2.0  Roo  


<35
  beeshell 1.0  MTD  50%  
  MTD  MTD  Roo  Roo  
  Roo  beeshell 2.0   
  MTD  Roo   beeshell 
 beeshell  beeshell  

UI 
UI  
UI  
beeshell  Roo UI UI  PC  Web  RN  UI   UI UI 

36> 2019 
Roo Theme  UI  
Roo Theme  SasS   JavaScript JS CSSLessSasSStylus 
beeshell  Roo Theme  JS   UI 

<37
  Roo Theme    UI  " <  < " " "
   Android   


  App    App   App     beeshell    UI  beeshell  #ffd800

38> 2019 
 UI  beeshell   beeshell  #ffa000 
 
 beeshell 
 
 beeshell 

<39    beeshell Font Size 12141620  28  
 
Font Weightbeeshell NormalBold Normal Bold  Font Family
Line Height  RN  =  * 1.2
beeshell  

40> 2019   UI 
 
 App  beeshell  Button 

  UI  beeshell  RN  StyleSheet.hairlineWidth  2px   beeshell  

 

<41           beeshell      Animated          FadeAnimated  SlideAnimated   beeshell   Button  FadeAnimated 
Modal  FadeAnimated 

42> 2019  Dropdown  SlideAnimated 

<43

 C  B   M 
 API 

"" ""  ""
  
 BottomModal 

44> 2019 

""Brand Primary Dark beeshell 
 """"""
  

 LabelText String LabelTextStyle TextStyle 

<Text style={this.props.labelTextStyle}>{this.props.labelText || '  '}</Text>
LabelText  LabelTextStyle 

<45 
  Style  xxxColor  
 Style  Color fontSizefontWeight  
                               labelTextStyle


 Label  ReactElement  UI

46> 2019  

  
 / 
 beeshell 1.0   
 
 SlideModal   

<47
 BottomModal  SlideModal  

48> 2019 
 BottomModal   SlideModal 

<49
  ""
       RN                     

50> 2019  beeshell 
 "" 

 

   6  50  
beeshell  38 

<51
beeshell  Antd Mobile RN  NativeBase  React Native Elementbeeshell   



52> 2019  UI 
        UI  UI         SlideModal  
SlideModal   / 

 UI 

<53

 UI SlideModal  (offsetX, offsetY)   /  4   3  N-2  12 
 RN Slide-

54> 2019  Modal 
 RN  SlideModal 
SlideModal 

<55
beeshell Dropdown Popover 
 SlideModal Slider  Rate  UI  





  Web  App   Form  HTML Form Switch  iOS  UISwitch   UI                  InputRadioCheckboxSwitch   Value  onChange Checked  

 Style
Data
Value onChange onPress renderItem





ViewStyle/TextStyle 

Any[]

 { label: string, value: any, [props: string]: any } Label Value  

Any



Function



Function



Function



56> 2019 


         
  API  Props  Methods 
 Props  Name | Type | Required | Default | Description  Methods  RN 

beeshell "" 
  
   AppiOS  Android  

<57

  
beeshell 1.0 """"beeshell 2.0    70%  80% 
SonarQube 
beeshell 2.0 ""  Detox 
   

58> 2019 

 

 MRN JS  Native  beeshell  JS Native  APP   .appiOS .apkAndroid jsbundle  Native  Native  

<59


 beeshell  JS 
  beeshell 1.0  20+    React Native 
 50+            APP                
beeshell  100+   beeshell 2.0  38  15+   100+  

60> 2019 

Github  beeshell  

 beeshell 1.0   MATERIAL DESIGN

  -  Web  99%   30%  -  VoIP Push doze 
Shark Push  98%  -   Node 
 10% 

 26    FE JS CSS  Android  iOS  HybridFlutterReact Native  tech@meituan. com

<61
React Native 

MRN 
MRNMeituan React Native React Native   RN   Android/iOS/Web   MRN   App PV   1 
MRN  React Native 0.54.3  React Native  0.60.5 MRN   JSI  JS  Native JS React Hooks MRN    App 

 2013    3 
1  H5+Native  

62> 2019 
H5   Native  
2    Android  iOS   2   3  2018  2019  140%   2015 Facebook  React Native RNRN  Native JS  RN   RN  RN   RN   MRN  2018   MRN   



<63

 App  MRN  
  Android/iOS  MRN   MRN  App   Android  iOS 
         MRN            1     
Android  iOS 2     Android  iOS  RN  RN    MRN  Android  iOS  RN                               

64> 2019 
Android/iOS/RN  Debug 
     UI AppMock      Bundle  APK  Bundle  APK   MRN      MRN 

 MRN 
RN  30  Native  MRN  RN    MRN 

<65
 Android  iOS   MRN  Android  iOS   JS  JS  Native  Android  iOS  MRN  Bundle   JS   WM-RN 

66> 2019 
    WM-RN          RN interfaceRN Native     RN JS  RN Interface     Native    Bridge    Native  JS  Native   Android  iOS RN Native  Android  iOS  RN  Native   RN JS  JS  App  Android  iOS  MRN  Beeshell  Native  RN Interface  Native  JS   JS   MRN 
Native/MRN/H5 
 App NativeMRNH5 H5   Native  Native  RN  H5 

<67  MRN  Native   
 Native  2  
 MRN   MRN
 H5  

  MRN 

68> 2019 
   MRN  
MRN  & 
   2018  10  MRN  MRN  MRN    Git  Talos    SOP Git Talos   Horn     RN  4   R Talos  R   Talos  R  MRN  MRN  Native   RD  R  

<69
   Bundle   Bundle 
 Bundle   Bundle   RN Bundle  Bundle   ""   
 Bundle   Bundle  Git   MRN  package.jsonLint   RN Bundle  Bundle  Bundle   MRN  MRN    RD  Lint Git Hook 
 Bundle  =>  Bundle   MRN  Bundle  

70> 2019   Bundle 
 Bundle  Bundle   Bundle ""  Bundle " Bundle"  Bundle  RN Bundle   Native 
 MRN MRN  Bundle  mrn-pack2  Bundle  mrn-pack2   Bundle

<71
   package.json
    A  B      

72> 2019   Pre-Commit  MRN 


 
1  RD  Git  R   RN  CodeReview  RN  2  RN Bundle   Talos Talos  Git  LintBuild  / 

<73
QALeader   CD  ­Eva Eva 
 RN Bundle  App SDK   RN Bundle  CDN 
3  ---- App  App  MRN  RN     Cat  Talos  RDQA  RD  R  "" P1P0""" "    4 ""     B 

74> 2019 
 Horn  MRN  Native  H5   RD  QA  CDN Eva Bundle 
 MRN  5    Review  Bug   MRN    



<75

 +  MRN   QA  MRN     
   MRN  Native   MRN  Native RD  
  

76> 2019 
  MRN RD  QA  1 Bundle   2 Native  3 4
 MRN  
  Bundle MRN  App   MRN  MRN 
 Bundle   QA  

 MRN  2  MRN   Android  iOS    MRN 
1 MRN  2 MRN   H5 3 
  Android 

<77
" """"  React Native "" MRN  "" Android  iOS " MRN   MRN 

1  MRN  Native 
   Debug Release 
MRN  Native  MRN   MRN  JS  MRN     2   Native  MRN   3   iOSAndroid  MRN   iOSAndroid  MRN  

78> 2019 

""   App 
 App  C   App   C  MRN   App   App  MRN 
  
MRN 
 MRN   MRN Bundle  MRN  

<79
 MRN  MRN Bundle  CDN   MRN   App DNS   App  App   99.9%    Bundle   Common  MRN SDK  99.99%   Bundle MRN PageLoad Success MRN  MRN   JS   App   Bundle  Bundle   99.9%   Bundle  JS

80> 2019 
Native  Bug  JS ErrorNative Crash    RN 
 JS Error JS Error  MRN PageExit Success
JS Error  RN  JS  JS Error   App  Bundle   JS Error  PV  JS   JS Error  JS Error   JS Error  JS Error  Native Crash 
MRN PageExit Success  MRN    JS Error   MRN   App  Bundle   99.9%  MRN PageExit Success  
"" 

<81
MRN 
  
 Bundle   Bundle  Load   FPS
  Bundle     CommonJS iOS  50  90  0.3s  0.7s Android 50  90  1.3s  1.8s MRN  App   JS  Bundle    Bundle  RN   Bundle  JS StartApplication 

82> 2019 
iOS  Android  
 iOS 50  400ms JS   100ms  Bundle  JS Bundle  
 FPS  FPS FPS   MRN   BindingX  iOS   Android  55  JS-Native  MRN FPS  MRN  Android   50  90   FPS  

 MRN  RN    MRN  
 Android Native  +iOS Native  -RN  / Android Native  +iOS Native 
  : RN  - / RN  +Android native+iOS Native

<83  Android Native  +iOS Native 
 -RN / Android Native  +iOS Native   
  Native   Native Module  Native Module MRN   50%  Android  iOS   50% RN bundle  Native   20%  50%  JS   100% JS   Native Module  

   MRN

84> 2019 
 60  RN  PV  
 MRN    MRN

  618RN   React Native    Weex   State of React Native 2018   React Native   iOS  React Native   React Native  beeshell 2.0   ESLint   CAT 3.0 



 AndroidiOSFE  / Base   tech@meituan.com



<85

Android 

DevOps  CI(Continuous Integration)   Android   CheckStyleLintFindBugs    PR(Pull Request)  Jenkins   1~2min   Flavor  PR  8~9min  50% 




       

86> 2019   
FindBugs    Java1.0~1.8    SpotBugs         Java8 SpotBugs  FindBugs  SpotBugs  SpotBugs  Java 
                               CheckStyle FindBugs   Java  Lint   

<87

 

Android        Gradle                  Gradle Task Gradle  Module  CheckStyleFindBugsLint  Task Android Task     Variant     Variant = Flavor * BuildType     Module  Flavor * BuildType *LintCheckStyle Findbugs * 
 Task  Varint   

88> 2019 
 Task  FindBugs  Lint  Module CheckStyle   Module Variant 

  CheckStyle  Lint   FindBugs Class 

<89
BCEL    Module  Variant 
Module  Variant  PR  Module  Variant   Module  Variant  Variant   FindBugs  Class  Variant   Variant   PR  Variant  Jenkins Job  Variant  Variant   Module 
 Module   aar  Module   Module  CheckStyle  Java FindBugs  Java  Module   Module   Module 

 Module  Variant    PR    

90> 2019 
  Class     


 Module 
    Module  Module  Module  Gradle "Project"  Project  Variant  Module Variant  Project 
static Set<Project> collectDepProject(Project project, BaseVariant variant, Set<Project> result = null) {
if (result == null) { result = new HashSet<>()
} Set taskSet = variant.javaCompiler.taskDependencies. getDependencies(variant.javaCompiler) taskSet.each { Task task ->
if (task.project != project && hasAndroidPlugin(task.project)) { result.add(task.project) BaseVariant childVariant = getVariant(task.project) if (childVariant.name == variant.name || "${variant.
flavorName}${childVariant.buildType. name}".toLowerCase() == variant.name.toLowerCase()) {
collectDepProject(task.project, childVariant, result) } } } return result }
 

<91
projectSet.each { targetProject -> if (targetProject.plugins.hasPlugin(CodeDetectorPlugin) && GradleUtils.
hasAndroidPlugin(targetProject)) { GradleUtils.getAndroidExtension(targetProject).sourceSets.all {
AndroidSourceSet sourceSet -> if (!sourceSet.name.startsWith("test") && !sourceSet.name.
startsWith(SdkConstants.FD_TEST)) { source sourceSet.java.srcDirs
} } } }
 Source  CheckStyle Task 
//  class  static final Collection<String> defaultExcludes = (androidDataBindingExcludes + androidExcludes + butterKnifeExcludes + dagger2Excludes).asImmutable()
List<ConfigurableFileTree> allClassesFileTree = new ArrayList<>() ConfigurableFileTree currentProjectClassesDir = project.fileTree(dir: variant.javaCompile. destinationDir, excludes: defaultExcludes) allClassesFileTree.add(currentProjectClassesDir) GradleUtils.collectDepProject(project, variant).each { targetProject ->
if (targetProject.plugins.hasPlugin(CodeDetectorPlugin) && GradleUtils. hasAndroidPlugin(targetProject)) {
//  Flavor  buildType GradleUtils.getAndroidVariants(targetProject).each { BaseVariant
targetProjectVariant -> if (targetProjectVariant.name == variant.name ||
"${targetProjectVariant.name}". toLowerCase() == variant.buildType.name.toLowerCase()) {
allClassesFileTree.add(targetProject.fileTree(dir: targetProjectVariant.javaCompile. destinationDir, excludes: defaultExcludes))
} } } }
 FindBugsTask  Class 
 FindBugs Task 

92> 2019   Lint  Lint Task 
 Lint 

 CheckStyle  FindBugs   9min  5min 

 



<93
 Lint 3.x   CheckStyle  FindBugs 
 diff_cover     FindBugs   Class 

 git diff   git diff ­ name-only ­diff-filter=dr commitHash1 commitHash2   PR   Jenkins  - ${targetBranch} - ${sourceCommitHash} hash 

git remote add upstream ${upstreamGitUrl} git fetch upstream ${targetBranch} git diff --name-only --diff-filter=dr $sourceCommitHash upstream/$targetBranch
1.  UpStream upstreamGitUrl  
2. 

94> 2019  3.  
Lint 
 Lint  Lint 
App Source Files  JavaXMLproGuard 
lint.xml  Lint   lint.xml 
lint Tool  Android  IDEAGradle  lint 
lint Output Lint 
Lint Tool  Lint Tool 

<95 Android  IDEAGradle  LintDriver  
 build.gradle 
compile 'com.android.tools.build:gradle:3.1.1' compile 'com.android.tools.lint:lint-gradle:26.1.1'


96> 2019 
lint-api-26.1.1 Lint  API  Lint
lint-checks-26.1.1  Issue 
lint-26.1.1  jar  Gradle  jar 
lint-gradle-26.1.1  Gradle  lint-26.1.1 
lint-gradle-api-26.1.1  Gradle Lint 
 jar  Lint   Gradle   LintDriver  Analyze 
fun analyze() { ...  ...
for (project in projects) { fireEvent(EventType.REGISTERED_PROJECT, project = project)
} registerCustomDetectors(projects) ...  ...
try { for (project in projects) { phase = 1
val main = request.getMainProject(project)
// The set of available detectors varies between projects computeDetectors(project)
if (applicableDetectors.isEmpty()) {

<97
// No detectors enabled in this project: skip it continue }
checkProject(project, main) if (isCanceled) {
break }
runExtraPhases(project, main) } } catch (throwable: Throwable) { // Process canceled etc if (!handleDetectorError(null, this, throwable)) {
cancel() } } ...  ... }
  registerCustomDetectors(projects) Lint   Lint 
1.  Project  Library  client.findRuleJars   jar 
2.  client.findGlobalRuleJars  jar   Android 
3.  jarFiles  Registry   CompositeIssueRegistry Lint  jar   build/intermediaters/lint   ~/.android/lint/
 computeDetectors(project)  checkProject(project, main) 

98> 2019 
 runFileDetectors Lint 
 Android  Lint Lint 
 Scope 
fun infer(projects: Collection<Project>?): EnumSet<Scope> { if (projects == null || projects.isEmpty()) { return Scope.ALL }
// Infer the scope var scope = EnumSet.noneOf(Scope::class.java) for (project in projects) {
val subset = project.subset if (subset != null) {
for (file in subset) { val name = file.name if (name == ANDROID_MANIFEST_XML) { scope.add(MANIFEST) } else if (name.endsWith(DOT_XML)) { scope.add(RESOURCE_FILE) } else if (name.endsWith(DOT_JAVA) || name.
endsWith(DOT_KT)) { scope.add(JAVA_FILE)
} else if (name.endsWith(DOT_CLASS)) { scope.add(CLASS_FILE)
} else if (name.endsWith(DOT_GRADLE)) { scope.add(GRADLE_FILE)
} else if (name == OLD_PROGUARD_FILE || name == FN_PROJECT_PROGUARD_FILE) {
scope.add(PROGUARD_FILE) } else if (name.endsWith(DOT_PROPERTIES)) {
scope.add(PROPERTY_FILE) } else if (name.endsWith(DOT_PNG)) {
scope.add(BINARY_RESOURCE_FILE) } else if (name == RES_FOLDER || file.parent == RES_FOLDER) {
scope.add(ALL_RESOURCE_FILES) scope.add(RESOURCE_FILE) scope.add(BINARY_RESOURCE_FILE) scope.add(RESOURCE_FOLDER) } } } else { // Specified a full project: just use the full project scope scope = Scope.ALL

<99
break } } }
 Project  Subset  NullScope  Scope.ALL
 Detector 

 Project  Subset  Null Subset  Subset 
Subset 
 runFileDetectors
if(scope.contains(Scope.JAVA_FILE)||scope.contains(Scope.ALL_JAVA_FILES)){ val checks = union(scopeDetectors[Scope.JAVA_
FILE],scopeDetectors[Scope.ALL_JAVA_FILES]) if (checks != null && !checks.isEmpty()) { val files = project.subset if (files != null) { checkIndividualJavaFiles(project, main, checks, files) } else { val sourceFolders = project.javaSourceFolders val testFolders = if (scope.contains(Scope.TEST_SOURCES)) project.testSourceFolders else emptyList<File> () val generatedFolders = if (isCheckGeneratedSources) project.generatedSourceFolders else emptyList<File> () checkJava(project, main, sourceFolders, testFolders,
generatedFolders, checks) }
} }
 project.subset  Java 
 runFileDe-
tectors 

100> 2019 
1. Scope.MANIFEST
2. Scope.ALL_RESOURCE_FILES)|| scope.contains(Scope.RE-
SOURCE_FILE) || scope.contains(Scope.RESOURCE_FOLDER) ||
scope.contains(Scope.BINARY_RESOURCE_FILE)
3. scope.contains(Scope.JAVA_FILE) || scope.contains(Scope.ALL_
JAVA_FILES)
4. scope.contains(Scope.CLASS_FILE) || scope.contains(Scope.ALL_
CLASS_FILES) || scope.contains(Scope.JAVA_LIBRARIES)
5. scope.contains(Scope.GRADLE_FILE)
6. scope.contains(Scope.OTHER)
7. scope.contains(Scope.PROGUARD_FILE)
8. scope.contains(Scope.PROPERTY_FILE)

 project.subset 
/** * Adds the given file to the list of files which should be
checked in this * project. If no files are added, the whole project will be
checked. * * @param file the file to be checked */
public void addFile(@NonNull File file) { if (files == null) { files = new ArrayList<>(); } files.add(file);
}
/** * The list of files to be checked in this project. If null, the
whole * project should be checked. * * @return the subset of files to be checked, or null for the
whole project */

<101
@Nullable public List<File> getSubset() {
return files; }
 Files  Null 
Lint  Gradle 
 Lint  Gradle   ./gradlew lint  Lint  Android  Gradle  Verification  Lint 
 Android Gradle   Task
 lint->LintGlobalTask TaskManager   lintDebuglintReleaselintVitalRelease->LintPerVariantTask 
ApplicationTaskManager  LibraryTaskManager  lintVitalRelease  release 

102> 2019 
 Android Gradle  Lint  LintGlobalTask
 LintPerVariantTask Variant
 Variant Variant 
 LintGlobalTask  LintPerVariantTask
 LintBaseTask LintGradleExecution  runLint 
 lint-gradle-26.1.1  Lint  API 
Gradle 
/** Runs lint on the given variant and returns the set of warnings */ private Pair<List<Warning>, LintBaseline> runLint( @Nullable Variant variant, @NonNull VariantInputs variantInputs, boolean report, boolean isAndroid) { IssueRegistry registry = createIssueRegistry(isAndroid); LintCliFlags flags = new LintCliFlags(); LintGradleClient client = new LintGradleClient( descriptor.getGradlePluginVersion(), registry, flags, descriptor.getProject(), descriptor.getSdkHome(), variant, variantInputs, descriptor.getBuildTools(), isAndroid); boolean fatalOnly = descriptor.isFatalOnly(); if (fatalOnly) { flags.setFatalOnly(true); } LintOptions lintOptions = descriptor.getLintOptions(); if (lintOptions != null) { syncOptions( lintOptions, client, flags, variant, descriptor.getProject(), descriptor.getReportsDir(), report, fatalOnly); } else { // Set up some default reporters

<103
flags.getReporters().add(Reporter.createTextReporter(client, flags, null,
new PrintWriter(System.out, true), false)); File html = validateOutputFile(createOutputPath(descriptor. getProject(), null, ".html",
null, flags.isFatalOnly())); File xml = validateOutputFile(createOutputPath(descriptor. getProject(), null, DOT_XML,
null, flags.isFatalOnly())); try {
flags.getReporters().add(Reporter. createHtmlReporter(client, html, flags));
flags.getReporters().add(Reporter. createXmlReporter(client, xml, false));
} catch (IOException e) { throw new GradleException(e.getMessage(), e);
} } if (!report || fatalOnly) {
flags.setQuiet(true); } flags.setWriteBaselineIfMissing(report && !fatalOnly);
Pair<List<Warning>, LintBaseline> warnings; try {
warnings = client.run(registry); } catch (IOException e) {
throw new GradleException("Invalid arguments.", e); }
if (report && client.haveErrors() && flags.isSetExitCode()) { abort(client, warnings.getFirst(), isAndroid);
}
return warnings; }
           warnings = client.run(registry)    Lint 
 Lint 1. 
IssueRegistry    Lint    BuiltinIssueRegistry2.   LintCliFlags
3.  LintGradleClient Gradle Android 
4.  LintOptions build.gralde 
 Lint  DSL  LintCliFlags Lint 

104> 2019 
5.  Client  Run  
 client.run(registry) Client  Client   Client  Gradle  Lint API   LintBaseTask  
FindBugs 
FindBugs  JAR  Apache  BCEL  ClassFindBugs  3.0.1  300  FindBugs   Gradle  FindBugs 
Gradle FindBugs 
 Gradle  FindBugs  Task Gradle 

 Classes  Class   Class 
 Classpath  Class  Classes  
 Effort  MINDefaultMAX  findBugs ClasspathFinbugs   reportLevel       LowMediumHigh    Low  

<105 Bug  High High   Reports   FindBugs  Classes  
FindBugs 
 FindBugs IDEA  
 Analyze Current File 
 

106> 2019 
   Java res 
   Class  Build  Java 
 Class 

<107
 Aux Classpath Entries, 
 IDEA   Gradle  FindBugs lib   Task  FinBugs  
 AuxClasspath
ClassPath  Class  
FileCollection buildClasses = project.fileTree(dir: "${project. buildDir}/intermediates/classes/${variant.flavorName}/${variant. buildType.name}",includes: classIncludes)
FileCollection targetClasspath = project.files() GradleUtils.collectDepProject(project, variant).each { targetProject ->
GradleUtils.getAndroidVariants(targetProject).each { targetVariant -> if (targetVariant.name.capitalize().equalsIgnoreCase(variant.
name.capitalize())) { targetClasspath += targetVariant.javaCompile.classpath
} } }
classpath = variant.javaCompile.classpath + targetClasspath + buildClasses
FindBugs 
 
class A { public static String buildTime = ""; ....
}

108> 2019 
 buildTime  Final
         A        BUG_TYPE_MS_SHOULD_BE_
FINAL Findbugs-IDEA 
 A A 
 A 
 ASM 
void findAllScanClasses(ConfigurableFileTree allClass) { allScanFiles = [] as HashSet String buildClassDir = "${project.buildDir}/$FINDBUGS_ANALYSIS_
DIR/$FINDBUGS_ ANALYSIS_DIR_ORIGIN"
Set<File> moduleClassFiles = allClass.files for (File file : moduleClassFiles) {
String[] splitPath = file.absolutePath.split("$FINDBUGS_ ANALYSIS_DIR/$FINDBUGS_ ANALYSIS_DIR_ORIGIN/")
if (splitPath.length > 1) { String className = getFileNameNoFlag(splitPath[1],'.') String innerClassPrefix = "" if (className.contains('$')) { innerClassPrefix = className.split('\\$')[0] } if (diffClassNamePath.contains(className) ||
diffClassNamePath.contains(innerClassPrefix)) {
allScanFiles.add(file) } else {
Iterable<String> classToResolve = new ArrayList<String>()
classToResolve.add(file.absolutePath) Set<File> dependencyClasses = Dependencies. findClassDependencies(project, new ClassAcceptor(), buildClassDir, classToResolve) for (File dependencyClass : dependencyClasses) {
if (diffClassNamePath. contains(getPackagePathName(dependencyClass))) {
allScanFiles.add(file) break } } } } } }

<109
 IDEA  
CheckStyle 
CheckStyle CheckStyle   Source  
void configureIncrementScanSource() { boolean isCheckPR = false DiffFileFinder diffFileFinder
if (project.hasProperty(CodeDetectorExtension.CHECK_PR)) { isCheckPR = project.getProperties().get(CodeDetectorExtension.
CHECK_PR) }
if (isCheckPR) { diffFileFinder = new DiffFileFinderHelper.PRDiffFileFinder()
} else { diffFileFinder = new DiffFileFinderHelper.LocalDiffFileFinder()
}
source diffFileFinder.findDiffFiles(project)
if (getSource().isEmpty()) { println '  java  checkStyle  '
} }
 PR  50%+

110> 2019 

      

 FindBugs   PR   CI  Daily Build 

<111



apply plugin: 'code-detector'

codeDetector { //  reportRelativePath = rootProject.file('reports')

/** *  pr  */
upstreamGitUrl = "ssh://git@xxxxxxxx.git"

checkStyleConfig {

/** *  CheckStyle  * true * false

*/

enable = true

/** *  * false * true CheckStyleTask 


*/

ignoreFailures = false

/** *  * true * false

*/

showViolations = true

/** *  checkstyle.xml  checkstyle.xsl  uri * 

*

"${checkStyleUri}/checkstyle.xml"

*

"${checkStyleUri}/checkstyle.xsl"

* *  null CodeDetector 

*/

checkStyleUri = rootProject.file('codequality/checkstyle')

}

findBugsConfig { /** *  Findbugs  * true

112> 2019 

* false

*/

enable = true

/**

*  max * min, default, or max. max  bug  . min 

*/

effort = "max"

/** *  high

* low, medium, high.  low  bug

*/

reportLevel = "high"

/**

*  findbugs_include.xml  findbugs_exclude.xml  uri * 

*

"${findBugsUri}/findbugs_include.xml"

*

"${findBugsUri}/findbugs_exclude.xml"

*  null CodeDetector 

*/

findBugsUri = rootProject.file('codequality/findbugs')

}

lintConfig {

/** *  lint  * true * false */
enable = true

/** *  lint.xml  retrolambda_lint.xml  uri

* 

*

"${lintConfigUri}/lint.xml"

*

"${lintConfigUri}/retrolambda_lint.xml"

*  null CodeDetector 

*/

lintConfigUri = rootProject.file('codequality/lint')

}

}

 


<113
./gradlew ":${appModuleName}:assemble${ultimateVariantName}" -PdetectorEnable=true -PcheckStyleIncrement=true -PlintIncrement=true -PfindBugsIncrement=true -PcheckPR=${checkPR} -PsourceCommitHash=${sourceCommitHash} -PtargetBranch=${targetBranch} --stacktrace
 
def finalizedTaskArray = [lintTask,checkStyleTask,findbugsTask] checkCodeTask.finalizedBy finalizedTaskArray
"open ${reportPath}".execute()
 PR  PR   Flavor  CI  Job  Flavor   Job 
 LintFinbugsCheckStyle   Lint   

 CheckStyle   FindBugs   Android Lint   Lint   FindBugs    Android Lint   Gradle     Android Gradle 

114> 2019 



 AndroidJava  / Base   tech@meituan.com

<115
ProbeAndroid  OOM 

 App    App  App   Crash  Crash  OOM   App  App  App  OOM 
 OOM ----Probe Probe  Probe  OOM 
OOM 
   OOM          Android          OOM Android  OOM 

116> 2019 
Android  OutOfMemoryError  /art/runtime/thread.cc
void Thread::ThrowOutOfMemoryError(const char* msg)  msg  OOM 
                OutOfMemoryError      Android  OOM 

/art/runtime/gc/heap.cc
void Heap::ThrowOutOfMemoryError(Thread* self, size_t byte_count, AllocatorType allocator_ type) 
oss << "Failed to allocate a " << byte_count << " byte allocation with " << total_bytes_free << " free bytes and " << PrettySize(GetFreeMemoryUntilOOME()) << " until OOM";

<117
 OOM 
1.                   Runtime.getRuntime.MaxMemory()  Android   OOM Android  OOM 
2.                OOM        failed due to fragmentation (required continguous free"<< required_bytes <<
"bytes for a new buffer where largest contiguous free"<< largest_ continuous_free_pages <<" bytes)";       art/runtime/gc/ allocator/rosalloc.cc 

/art/runtime/thread.cc
void Thread::CreateNativeThread(JNIEnv* env, jobject java_peer, size_t stack_size, bool is_ daemon) 
"Could not allocate JNI Env" 
StringPrintf("pthread_create (%s stack) failed: %s", PrettySize(stack_size).c_str(), strerror(pthread_create_result)));
 OOM   Android  JNIEnv  OOM

118> 2019 
 JNI 
 JNIEnv     Andorid       Anonymous Shared Memory  4KB  page   Linux  mmap   /dev/ashmem  FD  FD  JNIEnv  
E/art: ashmem_create_region failed for 'indirect ref table': Too many open files
java.lang.OutOfMemoryError: Could not allocate JNI Env at java.lang.Thread.nativeCreate(Native Method) at java.lang.Thread.start(Thread.java:730)
 mmap  JNIEnv 
E/art: Failed anonymous mmap(0x0, 8192, 0x3, 0x2, 116, 0): Operation not permitted. See process maps in the log. java.lang.OutOfMemoryError: Could not allocate JNI Env
at java.lang.Thread.nativeCreate(Native Method)

at java.lang.Thread.start(Thread.java:1063)

<119



1.  mmap  mmap flag  MAP_ANONYMOUS  Linux   
2.  clone 

W/libc: pthread_create failed: couldn't allocate 1073152-bytes mapped space: Out of memory W/tch.crowdsourc: Throwing OutOfMemoryError with VmSize 4191668 kB "pthread_create (1040KB stack) failed: Try again" java.lang.OutOfMemoryError: pthread_create (1040KB stack) failed: Try again
at java.lang.Thread.nativeCreate(Native Method) at java.lang.Thread.start(Thread.java:753)
 clone 
W/libc: pthread_create failed: clone failed: Out of memory W/art: Throwing OutOfMemoryError "pthread_create (1040KB stack) failed: Out of memory" java.lang.OutOfMemoryError: pthread_create (1040KB stack) failed: Out of memory
at java.lang.Thread.nativeCreate(Native Method) at java.lang.Thread.start(Thread.java:1078)

OOM 
 OOM  OOM   OOM 

120> 2019 
OOM Probe  OOM 

Android      OOM   Java                OOM  Crash "" 
    Java  Debug.dumpHprofData(String fileName)  Java  HPROF  dump  OOM  dump 
  Probe                       1S              Runtime.getRuntime.totalMemory()-Runtime. getRuntime.freeMemory()   dump 
 HPROF    HPROF   



<121
 Dominator GC Roots   DominatorB  E  Dominator B  F  Dominator
 ShallowSize  RetainSize ShallowSize 
 ShallowSize  GC  D  RetainSize  DHI  ShallowSize  JVM  GC  GC Roots   GC Roots   Github  HAHA  Java   HPROF  HAHA 

122> 2019 
 App  HAHA  HPROF  
 OOM  OOM    256MB 
 200  MB OOMDump  250  MB 70MB    256MB  200 72   OOMDump  250  MB  OOM   HPROF  Instance   HPROF  Instance  OOM

<123
HPROF  Snapshot   Object  Class 
HPROF 
// 1.  HprofBuffer    ByteBuffer  HprofBuffer buffer = new MemoryMappedFileBuffer(heapDumpFile); // 2.  Hprof  HprofParser parser = new HprofParser(buffer); // 3.  Snapshot snapshot = parser.parse(); // 4.  gcRoots deduplicateGcRoots(snapshot);
 OOM  HprofParser   Instance   ClassInstance  ArrayInstance key   Instance  Snapshot  Instance  Intsance  Instance  Instance  OOM  Instance  RetainSize 

124> 2019   Instance  RetainSize
   HAHA  PC    RetainSize  RetainSize   Instance  OOM ""
 Reference   Instance  Instance  Instance  RetainSize
 Instance  RetainSize  TOP N  Instance

<125  N  RetainSize  Instance  Instance  
 5  Instance   Instance Instance  RetainSize  5%  Instance  RetainSize  


126> 2019      HAHA                        
Instance  ArrayInstance byte   RetainSize  Instance  Instance 
 HAHA   OOM 
Probe  RetainSize  Top N  GC Roots    Bitmap  2MB 

<127  Bitmap  Timer   4  Instance Instance  Retain Size  595634 4*595634=2.27MB
 HPROF 
 Probe   65%
 HPROF   HPROF  MB 
 HPROF    MB  HPROF  HPROF  
Debug.dumpHprofData()  VMDebug   /art/runtime/hprof/hprof.ccHPROF   HAHA  hrpof.cc 
HPROF 

128> 2019 
 HPROF      id    Class 
    
 TAG   TAG 
 TAG  hrpof.cc 

<129
  OOM  
 dump  Java   20s dump  Native  
 dump   

130> 2019 
 GOT  Hook  Hook  Hook  hrpof.cc   
 IO  open  write  HookHook   xHook 
 dump  Native  open   Hook  FD  write  
 dump  write   Hook  so   FD  FD  origin-write 
 mini-file  mini-file   0 
 HPROF  HPROF  Android Studio   HPROF   HPROF  

 Android 7.X  Probe  dump 

<131
 HPROF  
Probe  dump   Native Native   open  write  Hookhookopen  open  path  path   path  open FD open  hookwrite  FD  hookopen  FD   FD  write FD   write dump   HPROF  HPROF 

132> 2019 


 OOMProbe   
/proc/sys/kernel/threads-max   500  1W   /proc/pid/ status  VmPeak/VmSize 
 Thread.getAllStackTraces()  
    "crowdSource msg"  


thread name: Thread[nio_tunnel_handler,5,main] count: 1 thread name: Thread[OkHttp Dispatcher,5,main] count: 30 thread name: Thread[process_read_thread,5,main] count: 4 thread name: Thread[Jit thread pool worker thread 0,5,main] thread name: Thread[crowdSource msg,5,main] count: 202 thread name: Thread[Timer-4,5,main] count: 1 thread name: Thread[mqt_js,5,main] count: 1

count: 1

threadnames:Thread[Thread-5,5,main] count:1 trace: java.lang.Object.wait(NativeMethod) com.dianping.networklog.d.run(UnknownSource:28)

<133
FD 
 FD  JNIEnv     OOM     FD                  OOM  FD  Probe  FD  1s   FD  FD FD  95%  FD 
 /proc/pid/limits  Linux  Max open files  FD 
 FD  /proc/pid/fd  /proc/pid/fd FD 
 FD 
File fdFile=new File("/proc/" + Process.myPid() + "/fd"); File[] files = fdFile.listFiles(); int length = files.length; //  fd  for (int i = 0; i < length ; i++) {
if (Build.VERSION.SDK_INT >= 21) { Os.readlink(files[i].getAbsolutePath()); // 
} else { //6.0  readlink 
} }
 FD  FD FD   socket  handlerThread 
 anon_inode:[eventpoll]  anon_inode:[eventfd]  handlerThread

134> 2019 

 FD  FD 

anon_inode:[eventpoll] count: 381 anon_inode:[eventfd] count: 381 pipe count 26 socket count 32 /system/framework/framework-res.apk count: 1
.......... Thread  thread name: Thread[Jit thread pool worker thread 0,5,main] thread name: Thread[mtqq handler,5,main] count: 302 thread name: Thread[Timer-4,5,main] count: 1 thread name: Thread[mqt_js,5,main] count: 1

count: 1


Probe           Java     FD          OOM  Android  Probe  OOM   OOM Crash  2 0.02  Probe  HPROF  7.X  Native  

 App   App  

 App  App   App App App   tech#meituan.com

<135
 Web 

  Web  Web ""  Web 
 Web 
 Web  ""  API """" 
""  7  1   
 ""

 Web  

136> 2019 
 Web  Web  Web   H5  PC  
  IP    Web  SDK" " Web API 
   
 Web   Google 
Google 
Google  reCAPTCHACompletely Automated Public Turing Test To Tell Computers and Humans Apart  reCAPTCHA 

<137  reCAPTCHA  "" reCAPTCHA  AI 
reCAPTCHA  
  reCAPTCHA "" 
Google  Web  Web   Web  

 
 Web  
 1Web   
 2 Web 

138> 2019 
 3
123  

 ""  
 
1.  2.  3.  4. 
  
  
 JS   100  5   

<139
  
""  "" 


1. Web API  HTTPS   Token Token
2.  
Token 
Token  Web   Token Token   Token  Cookie   Token Token  Token  Token 
Token  Token  10  Redis   Token Token  KV   10 
 Cookie  Token Token 

140> 2019 
 Cookie  Token Cookie  Token Token Token  Redis  
Token 
     API 

    

 Web  "" ""
 Web    
""

 JS  

<141   Uglify.js    
  
  Source Map  
""

 

142> 2019 
 Step 0  Nginx   403  Web  Step 1 Token  Step 0  Web  Web   Token Step 2   Step 3 HTTPS  Token  Authorization  Header   CSRF  Step 4  Token  Token

<143

Step 5    

 Web   
 API  """" 
 Web   Web   Web  " "

 https://www.google.com/recaptcha/intro/v3.html  https://segmentfault.com/a/1190000006226236  https://www.freebuf.com/articles/web/102269.html

 Web 2015  SDK  

144> 2019 
 React Native  TSLint  


  Bug    Review      Static Program Analysis                                       Dynamic Program Analysis    Android  CheckStyleLintFindbugsPMD  iOS  Clang Static AnalyzerOCLint  React Native   JavaScript  ESLint TypeScript  TSLint  TSLint  TSLint  TSLint 
 TSLint 
 React Native 

<145
1.  React Native  2.  AndroidiOS
 3.  Android iOS 

 Review  React Native    
1.  2.  3.  4. "" IDE  5.  IDE  6. 
 TSLint 
TSLint 
TSLint  Palantir  TypeScript  Editors Build Systems Lint 
 TSLint  TSLint    TSLint  https://palantir.github. io/tslint/

146> 2019 
TSLint 
 TSLint 
TSLint 

<147
 TSLint 
 2.1  Palantir  TSLint  
TSLint 
 
 TSLint 
 package.json  TSLint 

148> 2019 
TSLint 
 tslint.json 
TSLint 
 extends  TSLint  airbnb  tslint-react  rules 
TSLint  true  false  ERROR WARNING 
 
/* tslint:disable */
 TSLint  
/* tslint:enable */

<149
 TSLint  
/* tslint:disable:rule1 rule2 rule3... */
 rule1 rule2 rule3...
/* tslint:enable:rule1 rule2 rule3... */
 rule1 rule2 rule3...
// tslint:disable-next-line
 TSLint 
someCode(); // tslint:disable-line
 TSLint 
// tslint:disable-next-line:rule1 rule2 rule3...
 rule1 rule2 rule3...  https://palantir.github.io/tslint/usage/ruleflags/

 npm install 
IDE  IDE   VSCode 

150> 2019 
TSLint 
 VSCode    React Native 
tslint --project tsconfig.json --config tslint.json
 ./node_modules/.bin/
./node_modules/.bin/tslint --project tsconfig.json --config tslint.json

src/Components/test.ts[1, 7]: Class name must be in pascal case
 CI 
  CI  CI   Jenkins  Castle CI 

<151
TSLint 

 commitpull request 
  commit  Hook  CI  CI 
  pull request  CI  
  CI  
 TSLint 
 TSLint 
 TSLint  
1. saga  try-catch catch  TSLint  
2.   TSLint  tslint-microsoft-contribtslint-eslint-rules 

152> 2019 

 TSLint   
TSLint 
 "class-name"
" class-name"                         "-

<153 class-name"https:// github.com/palantir/tslint/blob/master/src/rules/classNameRule.ts

TSLint 

154> 2019  
TSLint 
 2  1.  2.  'Rule'    Rule Lint.Rules.AbstractRule   TypedRule   Lint.Rules.AbstractRule 
TSLint 
 metadata  metadata 

<155
 ruleName   description   descriptionDetails   rationale   options  null  optionExamples    typescriptOnly true/false  TypeScript  hasFix true/false   requiresTypeInfo   optionsDescrition options   type 
"functionality""maintainability""style""typescript"
 functionality   maintainability  style  typescript TypeScript  
TSLint 
 
 apply  apply                   applyWithFunction       applyWithWalker          applyWithFunction   

156> 2019  applyWithWalker  applyWithWalker  IWalker   IWalker 
TSLint
 IWalker  AbstractWalker  WalkContext
TSLint
 WalkContext  applyWithFunction 

<157
TSLint
  applyWithFunction  applyWithWalker  IWalker   sourceFile  ts.forEachChild   AST   AST Explorerhttps://astexplorer.net/
https://github.com/fkling/astexplorer  TypeScript AST Viewerhttps://ts-ast-viewer.com/
https://github.com/dsherret/ts-ast-viewer AST Explorer   AST Explorer  AST   1. 
TSLint

158> 2019  2. 
TSLint
TypeScript AST Viewer  1. 
TSLint
2. 

<159
TSLint
 kind  
TSLint

160> 2019    AST     ts.forEachChild 
 ts.Node
TSLint
 kind  Node  StatementExpressionDeclaration        "class-name"    class  interface  ClassExpression ClassDeclarationInterfaceDeclaration AST  
TSLint

<161  isClassLikeDeclarationisInterfaceDeclaration  
TSLint
 isClassLikeDeclarationisInterfaceDeclaration  node.js 
TSLint
TSLint
 addFailureAtNode   addFailureAtaddFailure 

162> 2019 
TSLint
  kind 
  ts  ts  js   ts  js 
tsc ./src/*.ts --outDir dist
 ts  dist  tslint.json  
TSLint


<163
tslint --project tsconfig.json --config tslint.json
 

TSLint 
1.   CI  React Native  1  2 
2.  Bug 3. 
 TSLint 
1.  2.  3. TSLint 
 AST    TSLint     TSLint  React Native    TSLint  React Native  React

164> 2019 
Native  
    """"

 Android 2017 

<165
ESLint 


   JavaScript   JavaScript  JSLintJSHint ESLintFECS ----ESLint  JavaScript  AST Abstract Syntax Tree
ESLint  ESLint  /   ESLint    eslint-config-airbnb  eslint-config-standard
  ESLint  JavaScript   husky  lint-staged   JavaScript  

 ESLint  


166> 2019 
  Web  Node.jsReact Native Electron
                         React/Vue JavaScript/TypeScript 
  ESLint  

     
 
  ESLint     JavaScript       
    
   

<167   

 JavaScript   ESLint   

1.  JavaScript  ESLint   
2.  

168> 2019 
 3. "" / 
 4. 


  
 ESLint 
 ESLint  
ESLint 
 ESLint  ESLint  1.   ESLint 
ESLint    ESLint  

<169 babel-eslint  ES  JSX   @typescript-eslint/parser  TypeScript 
2.  ESLint    npm  3.   ESLint   ESLint    ESLint   ESLint 
  
   Node.
jsReactVueReact Native 

170> 2019 
  TypeScript  typescript-eslint TypeScript
   MRN React
Native  prettier 
  ESLint  TypeScript  React   React  TypeScript  React   ESLint  TypeScript   ts-react 
 ESLint 

<171
 npm   TypeScript  React  
//  typescripteslint-plugin-react@typescript-eslint  module.exports = {
root: true, extends: [
//  eslintrc. base.js
'eslint-config-xxx/eslintrc.react.js', 'eslint-config-xxx/eslintrc.typescript-react.js' ] }

     
TypeScript  TSLint  eslint + @typescript-eslint/parser + @typescript-eslint/eslint-plugin 
 ESLint   
  @typescript-eslint  TSLint 

TypeScript  2019   Lint   ESLint 

172> 2019 

 
 ESLint  ESLint  VSCode  ESLint    

  Commit  githook  ESLint   
  CI   ESLint  "" 


<173  husky  lint-staged  Commit  githook  git   lint-staged   precommit-eslint    Warn  Error      pre-commit hook    
 CI  Sonar   ESLint  Node  

 CLI   

174> 2019 
1.  Eslint 2.  ESLint  npm  3.  4.  .eslintrc  5.  6.  package.json 7. 
 typescript  @typescript-eslint/parser    ESLint   
 

<175
   ESLint  

176> 2019 

 

<177
  precommit-eslint  git commit    CAT   
    1  10    2  10 - 100    3  100 - 1000    4  1000 
201903  0.2  10 
1. 

178> 2019  2. 
     

  
  
  
changeLog    FAQ   2019  2 

<179 0.2
  2019  2  200    1000    20000-25000    75.562% 1  15.644%
 64.015%  2018  12  2019  3  2019  3 
 2019  1  2  3  2019  1  2   75% - 80%
 

180> 2019 
 

 
1.  HTML  CSS   HTML  CSS   HTML  CSS   HTML  CSS   JavaScript TypeScript
2.   
3.  
4.        
 ESLint  

<181



  Web   99%  30%  VoIP Push doze  Shark Push  98%   Node   10%   tech@meituan.com " "

182> 2019 
App
 iOS  zsource 

zsource 
 App  2015  CocoaPods   App   cocoapods-binary  CocoaPods   App  
   


<183  App    pod install  App   pod install   App                   CocoaPods       CocoaPods  pod  zsource   
zsource 
zsource 
 zsource  zsource   zsource  

184> 2019           

 Xcode  Xcode  Debug   Xcode  Xcode   UI 
 

 Xcode  A  B A  libA.a B  A  libA.a  A   A Xcode  A

<185
 


 Mach-O  DWARF     Mach-O  DWARF 
 
 osx-abi-macho-file-format-reference  Introduction to the DWARF Debugging Format  DWARF 1.1.0 Reference

 
   
                                ZSCViewController xxd  xxd  ZSCViewController 
xxd ./libZSource.a | grep -C 5 'ZSCViewControlle'


186> 2019 
xxd 
  MachOViewer   MachOViewer"__debug_ str"Section 
MacOViewer 


<187 "Apple LLVM version 10.0.0 "  Mach-O  Ruby  ruby-macho 

 Section   DWARF 
 CI   DWARF 
 dwarfdump 
dwarfdump ./libZSource.a | grep 'ZSCViewContro'

dwarfdump 
 AT_name  DWARF 1.1.0 Reference 
An AT_name attribute whose value is a null-terminated string containing the full or relative path name of the primary source file from which the compilation unit was derived.  ---- AT_comp_dir

188> 2019 
An AT_comp_dir attribute whose value is a null-terminated string containing the current working directory of the compilation command that produced this compilation unit in whatever form makes sense Forelax the host system. 

  AT_name App    

1.  
2.  dwarfdump   CocoaPods 
 App   
                    CocoaPods    zsource 

<189
pod zsource 

zsource      
  zsource 

 iOS 2016  App  App ReactNative  App  Flutter 

190> 2019 



 ""    Bug
 
  2  +1   20  
  "" 

<191

 " -  -  -  - "   5      

   5   5  5~10   
   RN Android  6  Git iOS  5 RN5  16  Git  

192> 2019 

   UI     

  Git  
Develop  Stage  10  8 


<193     Develop  Release  Release  Stage  Stage "" Release  Feature  Release  Stage   Stage  

 


194> 2019  1.  Stage  Release  2.  3.  Stage  Stage  4.  5.  6.  Stage  
Branch Merge    

Git + Atlassian Stash Jenkins Build   " "

<195
1.  Release   Release  Stage   10.1.1  Release  10.1.3  10.1.2   Release/x.x.x  Release   10.1.3   Stash   Jenkins  Jenkins Job,  Branch  Job   Release  Branch    2.  Stage    Stage     Jenkins  Job Stage    Stage      Jenkins   Jenkins 

196> 2019 
 
3.   Jenkins Job   Pull Request  4.      App  5.                                 Release  Stage    Stage  Stage   Stage    Stage  

 10  
  

<197
 

2016  

198> 2019 




  

<199

 

   

 

200> 2019   

  Mach 
  UI  30      




<201

1.  API  API 
2.  
3.  UI    

   
   API 
  

202> 2019    

1. 
 API   : 1.  2. 3.   Mach  MRNRN 
1.1  Android   Block  Block   Block  iOS  Element

 Block 

<203

Block            Block    BlockView   BlockViewModelBlockView BlockViewModel LogicBlock  Block
1.2

    Root Block   Root Block Context  Block  Block  Block 
1.3

204> 2019 
 Root Block  Block Tree            SubBlock   Root Block Root Block Context   Context  Context  Root Context 

<205  Key   Block   viewModel viewModel  Block   Root Block 
 Root Block   Block Block  BlockViewModel  Block  Block  View  
2. Block  2.1Android 
Android  APT  Block  Block   Map 

206> 2019 
@DynamicBinder(nativeId = "block_key_d", viewModel = blockDViewModel. class, modelType = blockDInfo.class)
NativeID  Block  KeyviewModel  View  modelType  API  Model
2.2iOS  iOS   Kylin  Kylin              Clang    section()  Kylin  {kylin_Key,kylin_Data}   Key    native_idElement viewModel

#define PGA_ELEMENT_REGISTER(NATIVE_ID, PGA_ELEMENT, PGA_VIEW_MODEL) \ KLN_STRING_EXPORT("AppKey_"#NATIVE_ID"", "{ \""#PGA_VIEW_MODEL"\"
: \""#PGA_ELEMENT"\"}");
3. API 
 API  data_key   Block
 API 
{ "data":{ "xxx_pay_by_friend": true, "xxx_by_friend_tip": "  TA  ", "xxx_by_friend_bubble_desc": "  ", "xxx_friend_order": false } "code":0, "msg":""
}

<207

 Model  Block 
 JSON  API 

{ "data":{ "pay_by_friend":{//key "xxx_pay_by_friend": true, "xxx_by_friend_tip": "  TA  ", "xxx_by_friend_bubble_desc": "  ", "xxx_friend_order": false } } "code":0, "msg":""
}
 API  Key 
 Block  Model
 layoutInfo 
 Block   native_id 
Block  Key data_key  API 
Key Block  ViewModel 

{ "layout_info":[ {"native_id":"order_pay_by_friend","data_key":"pay_by_friend"}, { "native_id":"block_container_default",//  "children":[ {"native_id":"order_flower_cake","data_key":"flower_cake"} ] } ]
}
native_id 

208> 2019  Children  Block 
4. 
                                 Android  Activity  Activity  
  BlockContext
 Block  BlockContext BlockContext  Root Block Context  Block  BlockContext 



<209
4.1Command  
//  private SupplierCommand<Object> mSupplierCommand = new SupplierCommand<>(); @Override public SupplierCommand<Object> getSupplierCommand() {
return mSupplierCommand; } //  context().getSupplierCommand().registerCommand(new Supplier() {
@Override public Object run() { }
}); //  Object  context().getSupplierCommand().execute();
4.2Event  

//  private SupplierEvent mSupplierEvent = new SupplierEvent(); @Override public SupplierEvent supplierResponseEvent() {
return mSupplierEvent; } //  context().supplierResponseEvent().subscribe(new Action() {
@Override public void action() { } }); //  context().supplierResponseEvent().trigger();
5. Block 
5.1 Root Block  Block  Block

210> 2019   Block
Block Tree 

<211
Block 
5.2Block  Block  API  layoutInfo layoutInfo  API  layoutInfo  native_id  Block   wm_confirm_order_logical  Block layoutInfo  UI  Block  5.3     Event 

212> 2019    Block  
6. Block 
 Block  View  BlockBlockView  ViewModel      Block     ViewModel    BlockView   ViewModel  BlockView  Block

<213
" " David Wheeler  ViewData ViewData  

 iOS  Android   NativeID  UI   33   

214> 2019 
 UI   UI 

<215
 


   RD  RD  RD   Bug
RD   

216> 2019 

MVC   MVC   
 PGA   Element  Block   VC iOS  2894  289  Block 



<217
""     PR

   

Android  iOS    
 Android  iOS   PGA  Element Android  iOS   native_id  API   ID   API  

  PGA   PGA 

218> 2019 
 
   Mach  RN  
 PGA   100%   

1. Mach (  )  2. MRN  React-native 0.54.3 
 3. Metrics  App 
 4. Hertz SDK
 



 AndroidiOSFE  / Base   tech@meituan.com

<219
Bifrost 

Bifrost [`bi:frst] Bifrost ----Asgard """"  Bifrost  SPASingle Page Application  / 

"" CRM  Vue   
  Bug    
  


   


220> 2019         Vue   Vue     Nginx    Iframe  ---- HR 
----SingleSPA 
Nginx  Iframe HR   SingleSPA   Product-Ready  Bifrost

<221

Bifrost  SingleSPA  
              Bifrost    /  

222> 2019 
 DOM   Vue  



 Bifrost 

<223



 Bifrost  Platform  
 AppName DOM  ID

224> 2019 
 Domain
Domain 
 ConfigPath URL

import { Platform } from '@sfe/bifrost'
new Platform({ layoutFrame: { render () { // render layout } }, appRegister: [ { appName: 'app1', configPath: '/path/to/app1/config.js' } ]
}).start()

                             

AppContainer  Mounted 
import { AppContainer } from '@sfe/bifrost' import Vue from 'vue' import VueRouter from 'vue-router'
Vue.use(VueRouter) const router = new VueRouter({}) new AppContainer({
appName: 'app1', router, mounted () {
return new Vue({ router, components: { App }, template: '<App/>'
}).$mount() } }).start()

<225
 
((callback) => callback({ scripts: [ '/js/chunk-vendors.dee65310.js','/js/home.b822227c.js' ], styles: [ '/css/chunk-vendors.e7f4dbac.css','/css/home.285dac42.css' ]
}))(configLoadedCb.crm)
  @sfe/bifrost-config-plugin  Webpack    JSONP 
 CDN  URL 

 Vue   LayoutContainer  AppContainer 
import { LayoutContainer } from '@sfe/bifrost' import Vue from 'vue'
import App from './app'
new LayoutContainer({ appName: 'layout', router, onInit ({ appSlot, callback }) { Vue.config.productionTip = false const app = new Vue({ el: '#app', router, store, render: (h) => ( <App appSlot={appSlot} />

226> 2019 
) }) callback() } }).start()
   Vue  NPM   NPM  NPM   
 Bifrost  MockPlatform   / 

 
       
 Bifrost 


   

<227
Bifrost  MockPlatformMockPlatform    API   NPM  
MockPlatform  API  Platform  API   URL 
// ...others...
new AppContainer({ // ...others... runDevPlatform: process.env.NODE_ENV === 'development', // 
 mock platform devPlatformConfig: { layoutFrame: { mode: 'remote', configPath: 'path/to/layout/config.js' }, appRegister: [{ appName: 'app2', mode: 'remote', configPath: 'path/to/app2/config.js' }] }, // ...others...
}).start()
 MockPlatform  RD  

  Vue  Vuex Bifrost   Vuex Store

228> 2019 
 StoreVue   Store  Bifrost  syncGlobalStore  Store  Store  Global   Vuex  mapState 
import { AppContainer, syncGlobalStore } from '@sfe/bifrost' import Vue from 'vue' import Vuex from 'vuex' // ...others... Vue.use(Vuex) const store = new Vuex.Store({}) new AppContainer({
mounted () { //  store  syncGlobalStore(store)
// ...others... return new Vue({
store, router, components: { App }, template: '<App/>' }).$mount() } }).start()
Bifrost  installGlobalModule   Store   Bifrost   

  lodashMoment  

<229
 Webpack External   DLL     
DLL ---- Vue  DLL  Vue  

  Git   Git Submodule  Lerna   Lerna Publish   package.json 

 JSONP  Talos   CDN 

 Nginx   

230> 2019   URL 
 /   URL  

<231

        Bifrost         Bifrost                                onAppLoading onAppLoadedonAppRoutingonError onAppRouting PVonAppLoading  onAppLoaded                  Loading  Loaded                                 onError 

 
  
  ""
   

232> 2019 
    
""
    Cookie 
 

   

 ""

  12 Bifrost  2  38 Bifrost   Vue   Bifrost Bifrost  /  

 Bifrost 

<233

Bifrost   Bifrost
    JS  CSS   

    

234> 2019 



    30  24   LBS tech@meituan.com 



<235

Litho 


1.  Litho 
Litho  Facebook  Android UI   RecyclerView  Litho  
Litho is a declarative framework for building efficient user interfaces (UI) on Android. It allows you to write highly-optimized Android views through a simple functional API based on Java annotations. It was primarily built to implement complex scrollable UIs based on RecyclerView. With Litho, you build your UI in terms of components instead of interacting directly with traditional Android views. A component is essentially a function that takes immutable inputs, called props, and returns a component hierarchy describing your user interface. Litho      Android UI           API      Android  Recyclerview Litho   Android   Props  
Litho  Android  UI  Facebook   Android  React  UI 

236> 2019 
 Android  API API Flexbox   Litho  Flexbox    Litho Litho  React   Litho  Yoga   Litho  View  Drawable Litho  Drawable   Litho   Litho 

 Litho 

<237

2. Litho 
Litho  Android  XML 
 Java 
Litho  Android 
Android  res/layout  xx.xml
 Activity  Fragment 
<?xml version="1.0" encoding="utf-8"?> <TextView xmlns:android="http://schemas.android.com/apk/res/android"
android:layout_width="wrap_content" android:layout_height="wrap_content" android:text="Hello World" android:textAlignment="center" android:textColor="#666666" android:textSize="40dp" />
public class MainActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.helloworld); }
}
Litho Litho  Android 

public class MainActivity { @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); ComponentContext context = new ComponentContext(this); final Text.Builder builder = Text.create(context); final Component = builder.text("Hello World") .textSizeDip(40) .textColor(Color.parseColor("#666666")) .textAlignment(Layout.Alignment.ALIGN_CENTER)

238> 2019 
.build(); LithoView view = LithoView.create(context, component); setContentView(view); } }
Litho 
Litho  Component""  React  View  Drawable
Litho  View Litho   Litho   Litho 
 Litho  Layout Spec  Mount Spec  Layout Spec  Android  ViewGroup @LayoutSpec  @OnCreateLayout  
@LayoutSpec class HelloComponentSpec {
@OnCreateLayout static Component onCreateLayout(ComponentContext c, @Prop String name) {
return Column.create(c) .child(Text.create(c) .text("Hello, " + name) .textSizeRes(R.dimen.my_text_size) .textColor(Color.BLACK) .paddingDip(ALL, 10) .build()) .child(Image.create(c) .drawableRes(R.drawable.welcome)

<239
.scaleType(ImageView.ScaleType.CENTER_CROP) .build()) .build(); } }
 Litho  HelloComponent 
public final class HelloComponent extends Component {
@Prop(resType = ResType.NONE,optional = false) String name;
private HelloComponent() { super();
}
@Override protected Component onCreateLayout(ComponentContext c) {
return (Component) HelloComponentSpec. onCreateLayout((ComponentContext) c, (String) name);
}
...
public static Builder create(ComponentContext context, int defStyleAttr, int defStyleRes) {
Builder builder = sBuilderPool.acquire(); if (builder == null) {
builder = new Builder(); } HelloComponent instance = new HelloComponent(); builder.init(context, defStyleAttr, defStyleRes, instance); return builder; }
public static class Builder extends Component.Builder<Builder> { private static final String[] REQUIRED_PROPS_NAMES = new String[]
{"name"}; private static final int REQUIRED_PROPS_COUNT = 1; HelloComponent mHelloComponent;
...
public Builder name(String name) { this.mHelloComponent.name = name; mRequired.set(0);

240> 2019 
return this; }
@Override public HelloComponent build() {
checkArgs(REQUIRED_PROPS_COUNT, mRequired, REQUIRED_PROPS_NAMES); HelloComponent helloComponentRef = mHelloComponent; release(); return helloComponentRef; } } }
Mount Spec  View  Drawable  @MountSpec   @onCreateMountContent Mount Spec  Layout Spec 
 @OnPrepare  @OnMeasure  @OnBoundsDefined  @OnCreateMountContent  @OnMount  @OnBind  @OnUnBind
  @OnUnmount


<241
Litho ----Layout Layout  Litho  Android  ViewGroup  Flexbox  Litho  Layout  Yoga  Litho   Yoga Yoga Litho  Layout  Row 

242> 2019 
Column Flexbox 
Litho   Litho  Props State  Props  @Prop   Props 
@MountSpec class MyComponentSpec {
@OnPrepare static void onPrepare(
ComponentContext c, @Prop(optional = true) String prop1) { ... }
@OnMount static void onMount(
ComponentContext c, SomeDrawable convertDrawable, @Prop(optional = true) String prop1, @Prop int prop2) {
if (prop1 != null) { ... } } }
 Prop  prop1  prop2   prop1  prop2 Litho  MyComponent  Builder  
MyComponent.create(c) .prop1("My prop 1") .prop2(256) .build();

<243
State ""State  Checkbox  State   State  Props   Props  State 
3. Litho 
Litho  4  Litho  4 

Litho  API  UI  UI React 
 Android  UI   Litho  React  UI Litho  Stetho  Chrome  

Android  View  (Measure)  (Layout)  (Draw)  UI  UI   Measure  Layout  Litho  CPU   Measure  Layout  UI  Litho  RecyclerView  
  RecyclerView 

244> 2019   
 Android   View 
 Litho      Android  View    View  Litho  Litho   View  Litho   Litho  

 Litho  
 Android  Litho 

<245  Litho  Android 
 Litho  Flexbox  Litho    Yoga Yoga  Flexbox 
 Yoga     Litho  View   DrawableLitho  Drawable   Drawable  ViewAndroid  Drawable        Litho                     Drawable 

246> 2019 
Litho  Drawable  View  Drawable  View   Drawable  View   Drawable  View 50  "Hello world" TextView  TextDrawable   8 Shallow Size 
  Drawable  Drawable  Litho  View         Tag    

<247
 Litho 
 Android  Android ConstraintLayout  Litho  Litho    Litho  Flexbox   Flexbox  Flexbox 

Litho  
 RecyclerView   RecyclerView    itemType1  itemType2  Recycler  

248> 2019    Litho item  LithoView 
 Litho  LithoView    itemType1 LithoView  Recycler   Litho 
 RecyclerView   LithoView  

 Litho  MTFlexbox   Litho  UI  Litho  Litho 
 Litho+  App  App   Litho+  

<249
 Litho+ 

 Litho  Drawable  View   30M  Vivo x20  

250> 2019 
FPS 
FPS  Litho   2   fps 
5. 
Litho  Android  React  API  UI Android  Litho  Litho   Litho  Litho   Flexbox 
6. 
1. Litho 

<251
2.  Facebook  Litho 3. React  4. Yoga 
7. 
 Android 2015   Android 2017 

252> 2019 
Android  Java 8 

 Lambda invokedynamic  Android  RetroLambda Android  dex   D8  Java 8  ----Lambda  Android  Java8 
Java 8 
Java 8  Java Oracle  2014  3  18   Java 8Java  JVM  ScalaGroovy Java 8  
Java 8 
 Lambda   @FunctionalInterface  Stream API mapfilter    ::    default   
 Lambda  Java   Stream  mapfilterreduce  IO  Java   Java 

<253
Android  Java 
 Java  Android  Java FrameworkAndroid-SDK  80%  JDK-API Eclipse or Android Studio Java   Oracle Google  Android  Java  
 Android  1.0  4.4 19  Android  4.4  Java 7
  Android 4.4  Android N(7.0)  4  Android  Jack/Jill  Java 8 Jack/Jill   Java   App 
  Android P(9.0) Google  Android Studio 3.x   D8 dex  Java 8 API 
"" Rx Rx  Java 8  Lambda  Android Developer 
 Android  Java 8 
Lambda 
 Android  Java 8 Lambda  "" Lambda  
Lambda  Java   Java  Lambda  

254> 2019 
Lambda 


  Lambda  ()  -run   Lambda  System.out.println("xixi") / System.out.println("ha-
ha") run 
Lambda 


javac J8Sample.java -> J8Sample.class javap -c -p J8Sample.class

<255


  Lambda  1  Line 11  0: invokedynamic #2, 0 // InvokeDynamic #0:run:()Ljava/lang/Runnable
  Lambda  2  Line 20  21: invokedynamic #6, 0 // InvokeDynamic #1:run:()Ljava/lang/Runnable
Lambda  invokedynamic   invokedynamic 
invokedynamic 
invokedynamic  Java 7  Java 

256> 2019 
 invokevirtualinvokestaticinvokeinterfaceinvokespecial  Java 
  class  (Constant Pool)  MethodRef   
  invokedynamic           class         (Constant Pool)  Invokedynamic_Info    BootstapMethod   BootstrapMethod  
invokedynamic   J8Sample 
javap -v J8Sample.class

<257  J8Sample.class  invokedynamic  
 invokedynamic   1 J8Sample.java  Lambda  1 Runnable runnable = () -> System.out.println("xixi"); // lambda  1  2 javac J8Sample.java  J8Sample.class  Lambda  1 0: invokedynamic #2, 0 // InvokeDynamic #0:run:() Ljava/lang/Runnable;  J8Sample.class 

258> 2019 
 3 1  #2  class  #2 = InvokeDynamic #0:#35 // #0:run:()Ljava/lang/Runnable;  InvokeDynamic  Constant_InvokeDynamic_ Info   4 #0  class  BootstrapMethod  
 5 java/lang/invoke/LambdaMetafactory.metafactory   invokedynamic 

<259
 Lambda   2 
  6   java -Djdk.internal.lambda.dumpProxyClasses J8Sample.class 

260> 2019   7 javap -p -c J8Sample\$\$Lambda\$1.class 

 run       invokestatic        J8Sample.lambda$main$0 
 7  Lambda  Java Android 
Android 
  Android   Java-BytecodeJVM           Android  Android-BytecodeDalvik/ART 


<261  Lambda  Java  invokedynamic   Dalvik/ART  invokedynamic   Android  dex  invokedynamic  Android   Java 8
Android 
 Java-Bytecode  Android-Bytecode  Desugar 

 RetroLambda Google  Jack & Jill  D8 dex 
  Desugar           Lambda  Java              
RetroLambda  JackD8  

262> 2019 
Android  RetroLambda
RetroLambda  Desugar  javac  dx  dex 
RetroLambda Desugar
      invokedynamic           5   java/lang/ invoke/LambdaMetafactory.metafactory    J8Sample\$\$Lambda\$1.class  javac      dx   dex   J8Sample\$\$Lambda\$1  J8Sample. class  J8Sample.class  invokedynamic  invokestatic 
 J8Sample.java  Retrolambda  Android 
AndroidStudio -> Build -> make project 

<263
app:transformClassesWithRetrolambdaForDebug      app:compileDebugJavaWithJavac  javac   app:transformDexArchiveWithDexMergerForDebug  dx     build/intermediates/ transforms/retrolambda  class 
J8Sample.class  J8Sample$$Lambda$1.class 

264> 2019 
 J8Sample.class  Lambda   1.7or1.6 
 J8Sample.lambda$main$0()  J8Sample.class 
Android  Jack&Jill 
Jack  Eclipse  ecj Jill  ASM4 Jack&Jill  Google  Android N(7.0)  javac&dx   jack  Desugar 
 Android P(9.0)  Jack&Jill  javac&D8   Desugar 

<265
D8  Android P(9.0)  dex  Android Studio 3.1   D8  dex 
D8 Desugar
Desugar  D8  Android Studio  IDE   RetroLambda 
       java/lang/invoke/LambdaMetafactory.metafactory     J8Sample\$\$Lambda\$1.class D8  dex  dex 
 J8Sample.java  D8  Android 
AndroidStudio -> Build -> make project 

266> 2019 
javac  J8Sample.class  invokedynamic   Desugar
 app:transformDexArchiveWithDexMergerForDebug  dx     build/intermediates/transforms/dexMerger  0  classex.dex
  $ANDROID_HOME/build-tools/28.0.3/dexdump -d classes.dex >> dexInfo.txt  dex 
 Lambda  1 Runnable runnable = () -> System.out. println("xixi"); 
 dexIno.txt  1.4M com.J8Smaple2.J8Sample  J8Sample  dex 

<267

J8Sample.main 
 Lambda  1 desugar     Java      new Lcom/j8sample2/-$$Lambda$J8Sample$jWmuYH0zEF070TKXrjBFgnnqOKc   Lcom/j8sample2/-$$Lambda$J8Sample$jWmuYH0zEF070TKXrjBFgnnqOKc  J8Sample$$Lambda$1  1  Hash 

268> 2019 
 Interface Ljava/lang/Runnable Lcom/j8sample2/-$$Lambda$J8Sa mple$jWmuYH0zEF070TKXrjBFgnnqOKc.run 
 RetroLambda 

 Lambda   invokedynamic  RetroLambda  D8    
 Lambda  Java8   BSM- 
 Android  Desugar  1. RetroLambda  javac 
dx  2. Jack&Jill  jack 
dex  3. D8  dex 
 dex 

 RetroLambda D8 Java8  Java8      API    DataAPI  D8         Android P(9.0)  9.0  Java 8D8 

<269
 API API  
                   Kotlin    Kotlin  Lambda Kotlin   Android  Java  Kotlin  Java 
 Java 8 D8   Kotlin 

270> 2019 



     

<271
  99.533% 98.818% 99.959%Crash  0.1 App 
   

272> 2019 

  
1. 
    App  UGC  

<273
  SDK  15M  
 App UGC    16:9 
  App UGC   Grafika                          App UGC Google CTS  

274> 2019 
2. 
  H.264 H.264  H.264  MPEG-2  2  MPEG-4  1.5  2 
  AAC AAC   ""

   SDK 

  CameraOpenGLMediaCodec  MediaMuxer              ijkplayer    mp4parser
  
  
 
 

<275  App  

 

 

276> 2019 
1.  2. 
Android  MediaPlayer  ijkplayer  FFmpeg MediaPlayer   Android  iOS MediaPlayer  API  FFmpeg   ijkplayer
 ijkplayer   3G  
 AndroidVideoCache   Socket  
 AndroidVideoCache   

<277
  


 MediaRecorder+Camera    16:9  
1. Camera+AudioRecord+MediaCodec+Surface 2. MediaRecorder+MediaCodec
 1  Camera  YUV  YUV  PCM  mp4 
 2   2  mp4  MediaCodec  OpenGL  
1     2  MediaRecorder Android 

278> 2019 

// VIVO Y66   MediaServer 
//  1080P  720P  //  720P  480 if(isVIVOY66() && mMediaServerDied) {
return getCamcorderProfile(CamcorderProfile.QUALITY_480P); }
//SM-C9000,  1280 x 720  GPU    GPU 
//  //  if (VideoAdapt.MODEL_SM_C9000.equals(Build.MODEL)) {
return getCamcorderProfile(CamcorderProfile.QUALITY_HIGH); }
//  1080 P  CamcorderProfile camcorderProfile = getCamcorderProfile(CamcorderProfile. QUALITY_1080P); if (camcorderProfile == null) {
camcorderProfile = getCamcorderProfile(CamcorderProfile. QUALITY_720P);
} //  QUALITY_HIGH  1080p  if (camcorderProfile == null) {
camcorderProfile = getCamcorderProfile(CamcorderProfile. QUALITY_HIGH);
} //  if (camcorderProfile == null) {
camcorderProfile = getCamcorderProfile(CamcorderProfile. QUALITY_480P);
}


 mp4 mp4 
Box  Box  Box  Box  Container

<279 Boxmp4  Track  Sample  Sample   Video Smaple  Audio SampleVideo Smaple  Audio Sample  mp4 
 mp4parser   FFmpeg mp4parser    mp4  Box mp4parser API  
 mp4  Box mp4parser  Box  OOM   mp4parser  Box

 mp4parser   3s   mp4parser  4-5s  3s  mp4parser  I  

280> 2019 

mp4parser 
public static double correctTimeToSyncSample(Track track, double cutHere, boolean next) {
double[] timeOfSyncSamples = new double[track.getSyncSamples(). length];
long currentSample = 0; double currentTime = 0; for (int i = 0; i < track.getSampleDurations().length; i++) {
long delta = track.getSampleDurations()[i]; int index = Arrays.binarySearch(track.getSyncSamples(), currentSample + 1); if (index >= 0) {
timeOfSyncSamples[index] = currentTime; } currentTime += ((double) delta / (double) track. getTrackMetaData().getTimescale()); currentSample++; } double previous = 0; for (double timeOfSyncSample : timeOfSyncSamples) { if (timeOfSyncSample > cutHere) {
if (next) { return timeOfSyncSample;
} else { return previous;
} } previous = timeOfSyncSample; } return timeOfSyncSamples[timeOfSyncSamples.length - 1]; }
 mp4parser MediaCodec 


 mp4 


 OpenMAX 

<281 OpenGL
  mp4 
  
1. 
 Colorformat 
  

282> 2019 

status_t ACodec::setupVideoEncoder(const char *mime, const sp<AMessage> &msg, sp<AMessage> &outputFormat, sp<AMessage> &inputFormat) {
if (!msg->findInt32("color-format", &tmp)) { return INVALID_OPERATION;
} OMX_COLOR_FORMATTYPE colorFormat =
static_cast<OMX_COLOR_FORMATTYPE>(tmp); status_t err = setVideoPortFormatType(
kPortIndexInput, OMX_VIDEO_CodingUnused, colorFormat); if (err != OK) {
ALOGE("[%s] does not support color format %d", mComponentName.c_str(), colorFormat);
return err; } ....... } status_t ACodec::setVideoPortFormatType(OMX_U32 portIndex,OMX_VIDEO_ CODINGTYPE compressionFormat,
OMX_COLOR_FORMATTYPE colorFormat,bool usingNativeBuffers) { ...... for (OMX_U32 index = 0; index <= kMaxIndicesToCheck; ++index) {
format.nIndex = index; status_t err = mOMX->getParameter(
mNode, OMX_IndexParamVideoPortFormat, &format, sizeof(format)); if (err != OK) { return err; } ...... }
2. 
 Camera  YUV
 YUV  YUV 
YUV UV Android 
 YUV 
3. 16 
 H264  16*16 

<283
 16  
4. 
4.1   Surface  OpenGL  OpenGL   SurfaceTexture  updateTexImage   GL  GL    OpenGL  TextureView    Surface  TextureView  OpenGL 
4.2  SurfaceTexture  OnFrameAvailableListener  GPU   SurfaceTexture  BufferQueue  Buffer  BufferQueue   Camera  GL  OnFrameAvailableListener  updateTexImage() SurfaceTexture  GL SurfaceTexture   BufferQueue BufferQueue    Buffer    FREEDEQUEUEDQUEUED ACQUIREDSHARED  Producer           Dequeue 

284> 2019   Free    Buffer   Buffer     DEQUEUED        Producer   Producer          Queue  Buffer      QUEUED  Owner   BufferQueue      BufferQueue    ConsumerListener  onFrameAvailable     Consumer    Consumer  Acquire  QUEUED   Buffer   Owner  Consumer  Consumer    Buffer     Release Buffer  BufferQueue BufferQueue  GraphicBuffer GraphicBuffer  
SurfaceTexture 
 Producer  Video  BufferQueue    Producer        dequeueBuffer       Buffer   QUEUED     dequeuedCount   mMaxDequeuedBufferCount         Free Buffer Producer    DequeueBuffer     onFrameAvailable 

<285
status_t BufferQueueProducer::dequeueBuffer(int *outSlot,sp<android::Fence> *outFence, uint32_t width, uint32_t height,
PixelFormat format, uint32_t usage,FrameEventHistoryDelta* outTimestamps) {
...... int found = BufferItem::INVALID_BUFFER_SLOT; while (found == BufferItem::INVALID_BUFFER_SLOT) {
status_t status = waitForFreeSlotThenRelock(FreeSlotCaller::Dequeue,
& found); if (status != NO_ERROR) {
return status; } } ...... } status_t BufferQueueProducer::waitForFreeSlotThenRelock(FreeSlotCaller caller,
int*found) const{ ...... while (tryAgain) {
int dequeuedCount = 0; int acquiredCount = 0; for (int s : mCore -> mActiveBuffers) {
if (mSlots[s].mBufferState.isDequeued()) { ++dequeuedCount;
} if (mSlots[s].mBufferState.isAcquired()) {
++acquiredCount; } } // Producers are not allowed to dequeue more than // mMaxDequeuedBufferCount buffers. // This check is only done if a buffer has already been queued if (mCore -> mBufferHasBeenQueued &&
dequeuedCount >= mCore -> mMaxDequeuedBufferCount) { BQ_LOGE("%s: attempting to exceed the max dequeued buffer count "
"(%d)", callerString, mCore -> mMaxDequeuedBufferCount);
return INVALID_OPERATION; } } ....... }

286> 2019 
5. 
Android 9.0 
 MediaCodec  Configure 
 CQ Android 9.0  9.0  CQ 

status_t ACodec::configureCodec( const char *mime, const sp<AMessage> &msg) {
....... if (encoder) {
if (mIsVideo || mIsImage) { if (!findVideoBitrateControlInfo(msg, &bitrateMode, &bitrate,
&quality)) { return INVALID_OPERATION;
} } else if (strcasecmp(mime, MEDIA_MIMETYPE_AUDIO_FLAC)
&& !msg->findInt32("bitrate", &bitrate)) { return INVALID_OPERATION; } } ....... } static bool findVideoBitrateControlInfo(const sp<AMessage> &msg, OMX_VIDEO_CONTROLRATETYPE *mode, int32_t *bitrate, int32_t *quality) { *mode = getVideoBitrateMode(msg); bool isCQ = (*mode == OMX_Video_ControlRateConstantQuality); return (!isCQ && msg->findInt32("bitrate", bitrate)) || (isCQ && msg->findInt32("quality", quality)); } 9.0  CQ  static OMX_VIDEO_CONTROLRATETYPE getBitrateMode(const sp<AMessage> &msg) { int32_t tmp; if (!msg->findInt32("bitrate-mode", &tmp)) { return OMX_Video_ControlRateVariable; } return static_cast<OMX_VIDEO_CONTROLRATETYPE>(tmp); }
 isBitrateModeSupportedint
mode framework 
 media_codecs.xml 

<287

6. 
 
  

     Andorid 9.0  98% 92%   


288> 2019 

   App  P0  

 sniffer  sniffer   Android 9.0  Android 9.0   5.2   98% 

CPU     
 
  

<289

1. Android  2. Google CTS 3. Grafika 4. BufferQueue  5. MediaCodec  6.  Android  7. mp4  8. AndroidVideoCache  9. ijkplayer 10. mp4parser 11. GPUImage



  APPPC  H5      PM   APP  AndroidiOSReactFlutter  React Native


 
2019  18   3 Java   Kubernetes   HULK  
   
 18  



<291

Java Unsafe 



Unsafe  sun.misc   Java  Java  Unsafe  Java  C   Unsafe   Java "" Unsafe 
 sun.misc.Unsafe  API 

 Unsafe Unsafe  getUnsafe  Unsafe  getUnsafe   SecurityException 
public final class Unsafe { //  private static final Unsafe theUnsafe;
private Unsafe() { } @CallerSensitive public static Unsafe getUnsafe() {
Class var0 = Reflection.getCallerClass();

292> 2019 
//  `BootstrapClassLoader`  if(!VM.isSystemDomainLoader(var0.getClassLoader())) {
throw new SecurityException("Unsafe"); } else {
return theUnsafe; } } }
    getUnsafe              Java      -Xbootclasspath/a  Unsafe  A  jar   bootstrap  A  Unsafe.getUnsafe  Unsafe 
java -Xbootclasspath/a: ${path} //  path  Unsafe  jar 
 theUnsafe
private static Unsafe reflectGetUnsafe() { try { Field field = Unsafe.class.getDeclaredField("theUnsafe"); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { log.error(e.getMessage(), e); return null; }
}



<293

Unsafe  API CASClass 




//  ,  C++  malloc  public native long allocateMemory(long bytes); //  public native long reallocateMemory(long address, long bytes); //  public native void freeMemory(long address); //  public native void setMemory(Object o, long offset, long bytes, byte value); //  public native void copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes); //  : getIntgetDouble getLonggetChar  public native Object getObject(Object o, long offset); //  : putInt,putDouble putLongputChar  public native void putObject(Object o, long offset, Object x);

294> 2019 
//  byte  allocateMemory    public native byte getByte(long address); //  byte  allocateMemory    public native void putByte(long address, byte x);
 Java heap JVM  Java  JVM JVM   JVM  Java  Unsafe  native 

  JVM  GC  
  I/O  I/O   

DirectByteBuffer  Java   NettyMINA  NIO DirectByteBuffer   Unsafe  API 
   DirectByteBuffer       DirectByteBuffer      Unsafe.allocateMemory Unsafe.setMemory   Cleaner  DirectByteBuffer  DirectByteBuffer 

<295
 Cleaner  Cleaner    Java             PhantomReference   GC  PhantomReference  ReferenceQueue   Cleaner  JVM  pending  Reference-Handler Reference-Handler   pending  Cleaner  clean 

296> 2019 

 DirectByteBuffer  Cleaner  GC  DirectByteBuffer  Reference-Handler  Cleaner  clean  Cleaner  Deallocator 

CAS 
 CAS 

/** * CAS
* @param o

 field 

<297

* @param offset  field  * @param expected 

* @param update 

* @return

true | false

*/

public final native boolean compareAndSwapObject(Object o, long

offset, Object expected,

Object update);

public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update);

public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update);

 CAS? CAS  ---- CAS                                   CAS  CPU  cmpxchg Unsafe  CAS  compareAndSwapXXX CPU  cmpxchg

CAS  java.util.concurrent.atomic Java AQSCurrentHashMap      AtomicInteger  valueOffset  value valueOffset  AtomicInteger              Unsafe  objectFieldOffset      AtomicInteger  valueOffset  AtomicInteger  value  CAS  value  

298> 2019 
 AtomicInteger  baseAddress="0x110000"   baseAddress+valueOffset   value    valueAddress="0x11000c" CAS  



<299
//  public native void unpark(Object thread); //  public native void park(boolean isAbsolute, long time); //  @Deprecated public native void monitorEnter(Object o); //  @Deprecated public native void monitorExit(Object o); //  @Deprecated public native boolean tryMonitorEnter(Object o);
 parkunpark 
 park  park 
unpark 

Java  AbstractQueuedSynchronizer
 LockSupport.park()  LockSupport.unpark()            
LockSupport  parkunpark  Unsafe  parkunpark 

Class 
 Class 
 & 
//  public native long staticFieldOffset(Field f); //  public native Object staticFieldBase(Field f); //     ensureClassInitialized   false public native boolean shouldBeInitialized(Class<?> c); //  

300> 2019 
public native void ensureClassInitialized(Class<?> c); //  JVM ClassLoader  ProtectionDomain public native Class<?> defineClass(String name, byte[] b, int off, int len, ClassLoader loader, ProtectionDomain protectionDomain); //  public native Class<?> defineAnonymousClass(Class<?> hostClass, byte[] data, Object[] cpPatches);

 Java 8 JDK  invokedynamic  VM Anonymous Class   Java  Lambda 
 invokedynamicinvokedynamic  Java 7  JVM   invokedynamic  
 VM Anonymous Class   Unsafe.defineAnonymousClass   ClassLoader   Class   GC  VM Anonymous Class  Java   ClassClassLoader 
 Lambda  invokedynamic   ASM  Unsafe  defineAnonymousClass    Lambda  Test



<301

Test  class   main invokedynamic          BootstrapMethods      lambda$main$0   Lambda  Unsafe. defineAnonymousClass  Consumer  accept  Test  lambda$main$0  Lambda  consumer.accept"lambda"  accept 

302> 2019 


//  public native long objectFieldOffset(Field f); // getIntgetDoublegetLong getChar  public native Object getObject(Object o, long offset); // putIntputDoubleputLong putChar  public native void putObject(Object o, long offset, Object x); //  volatile  public native Object getObjectVolatile(Object o, long offset); //  volatile  public native void putObjectVolatile(Object o, long offset, Object x); //  putObjectVolatile   field  volatile  public native void putOrderedObject(Object o, long offset, Object x); //  public native Object allocateInstance(Class<?> cls) throws InstantiationException;

   new new 


            Unsafe    allocateInstance     
Class 
JVM  private 

allocateInstance  java.lang.invokeObjenesis
Gson
 Gson 

<303                UnsafeAllocator           UnsafeAllocator  Unsafe  allocateInstance  

 arrayBaseOffset  arrayIndexScale  
//  public native int arrayBaseOffset(Class<?> arrayClass); //  public native int arrayIndexScale(Class<?> arrayClass);

 java.util.concurrent.atomic  AtomicIntegerArray Integer   AtomicIntegerArray  Unsafe  arrayBaseOffset arrayIndexScale  base  scale  getAndAdd  checkedByteOffset   CAS 

304> 2019 

 Java 8   CPU   
//  load  load  load  public native void loadFence(); //  store  store   store  public native void storeFence(); //  loadstore  public native void fullFence();

 Java 8 ----StampedLock StampedLock  ""  StampedLock   load  StampedLock  

<305
 Point  move   distanceFromOrigin distanceFromOrigin   tryOptimisticRead  (x,y)  StampedLock  validate  (x,y)   move  validate  true (x, y)   (x,y)   copy  
 StampedLock.validate   Unsafe  loadFence   load  StampedLock.validate  

306> 2019 


//  432  864  public native int addressSize(); //  2  public native int pageSize();

 java.nio  Bits   Unsafe  pageSize  

<307

 Java  sun.misc.Unsafe   Unsafe  API  Unsafe   

 OpenJDK Unsafe source  Java Magic. Part 4: sun.misc.Unsafe  JVM crashes at libjvm.so  Java  ­Unsafe  JVM     DirectByteBuffer    Java  2 

 Java 2017  

308> 2019 
Java 


 
"Debug " " Debug " "" "" "" "" " Review  " " " "" " Review "
 JSP 
 Java  JSPJava Server Pages  
 Java  JSP  

<309 "" " for " "                          Sesstion " "" ......    JVMJava   JSP  Java   class   JSP  JSP   JSP  :
JSP 
JSP  Web Tomcat  JSP  JSP 

310> 2019 
 Sevlet  JVM  Servet   Java  ClassLoader  Web  ClassLoader   Servlet  Servlet   JSP 
HTTP  JSP   ClassLoader ""class  Spring   ClassLoader 

Java 
 JSP    JVM 

public class Person{ private int age; private String name; public void speak(String str) { System.out.println(str);
} public Person(int age, String name) {
this.age = age; this.name = name; } }

<311
 Person  age  name speak    Person personA  personB
Person personA = new Person(43, "lixunhuan"); personA.speak("  "); Person personB = new Person(23, "afei"); personB.speak("  ");
personA  personB speak  Java 
" "
"" ""  
Method area is created on virtual machine startup, shared among all Java virtual machine threads and it is logically part of heap area. It stores per-class structures such as the run-time constant pool, field and method data, and the code for methods and constructors. Java  "" " class " "class " " Java  JVM " "" ""

312> 2019 
"......    JVM  ClassLoader"
" java.lang.instrument.Instrumentation"
java.lang.instrument.Instrumentation
redefineClasses  retransformClasses class class reDefineClasses 
This method is used to replace the definition of a class without reference to the existing class file bytes, as one might do when recompiling from source for fix-and-continue debugging. Where the existing class file bytes are to be transformed (for example in bytecode instrumentation) retransformClasses should be used.  class redefineClasses   class retransformClasses    class   field  instrument  The redefinition may change method bodies, the constant pool and attributes. The redefinition must not add, remove or rename fields or methods, change the signatures of methods, or change inheritance. These restrictions maybe be lifted in future versions. The class file bytes are not checked, verified and installed until after the transforma-

<313
tions have been applied, if the resultant bytes are in error this method will throw an exception.   reTransform    class  Java  class  redefineClasses   JVM   Java Scala  JVM   class JVM  class  "JVM "  class 

Java             class     JVM       class  JVM    Java   
 Java     ASM cglibSpring   ASM 
Spring  AOP Spring  

314> 2019 
Spring  Spring   Java  class Spring "" class  class  ASM 
 ASM  class   retransformClasses 
BTrace
 
1.  reTransform   
2.  JVM  3.  ASM ""
 BTrace  BTrace  BTrace 
A safe, dynamic tracing tool for the Java platform. BTrace  Java BTrace  ASMJava Attach ApiInstruments   BTrace  Java   ASM   BTrace  java.io  read   IO 

<315

package com.sun.btrace.samples;
import com.sun.btrace.annotations.*; import com.sun.btrace.AnyType; import static com.sun.btrace.BTraceUtils.*;
/** * This sample demonstrates regular expression * probe matching and getting input arguments * as an array - so that any overload variant * can be traced in "one place". This example * traces any "readXX" method on any class in * java.io package. Probed class, method and arg * array is printed in the action. */
@BTrace public class ArgArray { @OnMethod( clazz="/java\\.io\\..*/", method="/read.*/" ) public static void anyRead(@ProbeClassName String pcn, @
ProbeMethodName String pmn, AnyType[] args) {
println(pcn); println(pmn); printArray(args); } }
 2 
package com.sun.btrace.samples;
import com.sun.btrace.annotations.*; import static com.sun.btrace.BTraceUtils.*; import com.sun.btrace.annotations.Export;
/** * This sample creates a jvmstat counter and * increments it everytime Thread.start() is * called. This thread count may be accessed * from outside the process. The @Export annotated * fields are mapped to jvmstat counters. The counter * name is "btrace." + <className> + "." + <fieldName> */

316> 2019 
@BTrace public class ThreadCounter {
// create a jvmstat counter using @Export @Export private static long count;
@OnMethod( clazz="java.lang.Thread", method="start"
) public static void onnewThread(@Self Thread t) {
// updating counter is easy. Just assign to // the static field! count++; }
@OnTimer(2000) public static void ontimer() {
// we can access counter as "count" as well // as from jvmstat counter directly. println(count); // or equivalently ... println(Counters.perfLong("btrace.com.sun.btrace.samples. ThreadCounter.count")); } }
 HashMap  rehash
 BTrace BTrace   Git  BTrace  BTrace 
 
 BTrace  BTrace  
BTrace 
1. BTrace  BTrace  

<317 2. Compiler BTrace  BTrace class  3. Client class  Agent 4. Agent Java  Attach ApiAgent  JVM
 BTrace Server client  BTrace   Java Instrument  reTransform   BTrace 
BTrace 
BTrace  Instruments  class  Instruments BTrace BTrace  JVM  "" BTrace 

318> 2019 
1.  2.  3.  4.  catch  5.                       com.sun.btrace.
BTraceUtils  6.  7.  static public void  8.  9.  10.  11. java.lang.Object  12.  13.  assert 14.  Class 
BTrace  
Arthas
BTrace   2018  9  Java ---- ArthasArthas  Arthas 
 Java  ""

<319

"" Java  Instruments Attach API  ""ASM "" Java   Instruments  Attach API  JProfilerJvisualvm BTraceArthas  ASM  cglib  Spring AOP Java Java 5  InstrumentsJava 6  Attach API  ""     0  1   "" 

2017 

320> 2019 


1. 
1.1
Java "" JVM   .class  JVM  Java   JVM   Java  javac   .java  1 
 1Java 
 Java   Volatile   Spring AOP ORM   JVM 

<321  JVM  JVM  ScalaGroovyKotlin Java  "" ""
 JVM   Java  
1.2
.java     javac         .class            ByteCodeDemo  2 
 2
 ByteCodeDemo.class   2 JVM   JVM   3  

322> 2019 
 3JVM 
1Magic Number  .class 0xCAFEBABE        JVM                        .class   Java  James Gosling  CafeBabe  Java  2  4 Minor Version Major Version 2 "00 00 00 34"  0 52 Oracle  52  1.8 Java  1.8.0 3Constant Pool   Final    4 

<323
 4
 constant_pool_count  2  10   5  24  36"0"  35 
 5
 constant_pool_count-1 cp_info   cp_info  14  cp_info  6 

324> 2019 
 6 cp_info
 CONSTANT_utf8_info  7  "tag" 6  Tag utf8_info "01" Length Length   2  cp_info  7   utf8 "a"

<325
 7CONSTANT_utf8_info 
 cp_info  Tag   n  javap-verbose ByteCodeDemo  JVM  8  cp_info 
 8

326> 2019  4  Class  Public
AbstractFinal JVM  9 Access_ FlagJVM   Public Final ACC_ PUBLIC | ACC_FINAL 0x0001 | 0x0010=0x0011
 9
5   6  

<327 7   n  8    fields_info
 10
 2  11  9 0002  Private 8 "a" "I" int private int a
 11
9   

328> 2019 
 12
 9  ""  javap -verbose  13 
 13

<329
"  Code " JVM  "Code "
"  LineNumberTable" Code  Debug  JVM 
"  LocalVariableTable" This   This JVM  This   Static 
10 
1.3
 13 Code  0  17 .java   JVM    Oracle   iconst_2 2  0x05 int  2   0~17  add() 
1.4
JVM    FILO   CPU  
 JVM    add()  GIF 14   iconst_2  ireturn  13  Code 

330> 2019  0~17 
 14
1.5
 javap   Idea jclasslib 15 "View" "Show Bytecode With jclasslib" 
 15jclasslib 

<331
2. 
  
 16
2.1ASM
 ASM .class   JVM  17 ASM   AOPCglib  ASM jar   ASM  API ASM  AOP ASM    JVM 

332> 2019 
 17ASM 
2.1.1ASM API 2.1.1.1 API ASM Core API  XML  SAX    Core API Core API   ClassReader .class   ClassWriter
   Visitor CoreAPI 
 Visitor MethodVisitor FieldVisitor AnnotationVisitor   AOP MethodVisitor 2.1.1.2 API ASM Tree API  XML  DOM 

<333
TreeApi  CoreAPI TreeAPI  Node  DOM  
2.1.2 ASM  AOP  ASM  CoreAPI  AOP    Base  process() "process" "start""end"
public class Base { public void process(){ System.out.println("process"); }
}
    ASM   AOP           MyClassVisitor         visit         Generator         ClassReader  ClassWriterclassReader   MyClassVisitor  ClassWriter  Generator  MyClassVisitor 
import org.objectweb.asm.ClassReader; import org.objectweb.asm.ClassVisitor; import org.objectweb.asm.ClassWriter;
public class Generator { public static void main(String[] args) throws Exception { //  ClassReader classReader = new ClassReader("meituan/bytecode/asm/
Base"); ClassWriter classWriter = new ClassWriter(ClassWriter.COMPUTE_
MAXS); //  ClassVisitor classVisitor = new MyClassVisitor(classWriter); classReader.accept(classVisitor, ClassReader.SKIP_DEBUG);

334> 2019 
byte[] data = classWriter.toByteArray(); //  File f = new File("operation-server/target/classes/meituan/ bytecode/asm/Base.class"); FileOutputStream fout = new FileOutputStream(f); fout.write(data); fout.close(); System.out.println("now generator cc success!!!!!"); } }
MyClassVisitor  ClassVisitor
 MyMethodVisitor MethodVisitor 

import org.objectweb.asm.ClassVisitor; import org.objectweb.asm.MethodVisitor; import org.objectweb.asm.Opcodes;
public class MyClassVisitor extends ClassVisitor implements Opcodes { public MyClassVisitor(ClassVisitor cv) { super(ASM5, cv); } @Override public void visit(int version, int access, String name, String
signature, String superName, String[] interfaces) {
cv.visit(version, access, name, signature, superName, interfaces);
} @Override public MethodVisitor visitMethod(int access, String name, String desc, String signature, String[] exceptions) {
MethodVisitor mv = cv.visitMethod(access, name, desc, signature, exceptions);
//Base  process  if (!name.equals("<init>") && mv != null) {
mv = new MyMethodVisitor(mv); } return mv; } class MyMethodVisitor extends MethodVisitor implements Opcodes { public MyMethodVisitor(MethodVisitor mv) {
super(Opcodes.ASM5, mv); }

<335
@Override public void visitCode() {
super.visitCode(); mv.visitFieldInsn(GETSTATIC, "java/lang/System", "out", "Ljava/io/PrintStream;"); mv.visitLdcInsn("start"); mv.visitMethodInsn(INVOKEVIRTUAL, "java/io/PrintStream", "println", "(Ljava/lang/ String;)V", false); } @Override public void visitInsn(int opcode) { if ((opcode >= Opcodes.IRETURN && opcode <= Opcodes.RETURN)
|| opcode == Opcodes.ATHROW) { //  "end" mv.visitFieldInsn(GETSTATIC, "java/lang/System", "out", "Ljava/io/PrintStream;"); mv.visitLdcInsn("end"); mv.visitMethodInsn(INVOKEVIRTUAL, "java/io/PrintStream", "println", "(Ljava/lang/ String;)V", false); } mv.visitInsn(opcode); } } }


     MyClassVisitor    visitMethod           
 <init> 
MyMethodVisitor 
  MyMethodVisitor  visitCode  ASM
 Code  visitCode  AOP 

 MyMethodVisitor  ASM 
 MyMethodVisitor  visitInsn 
"return" AOP

336> 2019  
     MyMethodVisitor            AOP    ASM  methodVisitor  visitXXXXInsn() XXXX   mv.visitLdcInsn("end")   ldc"end""end"
 visitor  Generator  main  Base   target  Base.class   18  MyTest new Base() base.process()   AOP 
 18ASM  AOP 
2.1.3ASM   ASM  visitXXXXInsn()   ASM   visitXXXXInsn()    ASM ASM   ASM ByteCode Outline "Show Bytecode Outline""ASMi-

<337 fied" tab 19  ASM   AOP  visitor  visitMethod()  visitInsn() 
 19ASM Bytecode Outline
2.2Javassist
ASM    Javassist
 Javassist   java   ClassPoolCtClassCtMethod

338> 2019 
CtField 
 CtClasscompile-time class class   CtClass  
 ClassPool       ClassPool      CtClass    HashTablekey value  CtClass   pool.getCtClass("className")  pool  CtClass
 CtMethodCtField
 Demo  Javassist   Base  process()  "start""end" pool  CtClass  method.insertBefore  insertAfter   Java 
import com.meituan.mtrace.agent.javassist.*;
public class JavassistTest { public static void main(String[] args) throws NotFoundException,
CannotCompileException, IllegalAccessException, InstantiationException, IOException {
ClassPool cp = ClassPool.getDefault(); CtClass cc = cp.get("meituan.bytecode.javassist.Base"); CtMethod m = cc.getDeclaredMethod("process"); m.insertBefore("{ System.out.println(\"start\"); }"); m.insertAfter("{ System.out.println(\"end\"); }"); Class c = cc.toClass(); cc.writeFile("/Users/zen/projects"); Base h = (Base)c.newInstance(); h.process(); } }

<339
3. 
3.1
  AOP  ASM  AOP  main  MyClassVisitor  class  new  JVM  main  ASM   main  Javassist  Base 
 JVM   Javassist  Demo  main()         Base b=new Base()         JVM   Base  c.toClass()  20   c.toClass()  ClassLoader  native  defineClass() JVM 
 20 load 
  JVM    Base  main  process()  process() "process"
 JVM  process()  "start""end""process"

340> 2019 
"start process end" JVM   Java 
import java.lang.management.ManagementFactory;
public class Base { public static void main(String[] args) { String name = ManagementFactory.getRuntimeMXBean().getName(); String s = name.split("@")[0]; //  Pid System.out.println("pid:"+s); while (true) { try { Thread.sleep(5000L); } catch (Exception e) { break; } process(); } }
public static void process() { System.out.println("process");
} }
3.2Instrument
instrument  JVM  Java   JVMTI  Attach API JVMTI   JDK 1.6 instrument  JVM   JDK 1.6 instrument   instrument  ClassFileTransformer  transform()   transform  ASM  Javassist 
 ClassFileTransformer  TestTransformer  Javassist  Base  process() 

<341
"start""end"
import java.lang.instrument.ClassFileTransformer;
public class TestTransformer implements ClassFileTransformer { @Override public byte[] transform(ClassLoader loader, String className,
Class<?> classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) {
System.out.println("Transforming " + className); try {
ClassPool cp = ClassPool.getDefault(); CtClass cc = cp.get("meituan.bytecode.jvmti.Base"); CtMethod m = cc.getDeclaredMethod("process"); m.insertBefore("{ System.out.println(\"start\"); }"); m.insertAfter("{ System.out.println(\"end\"); }"); return cc.toBytecode(); } catch (Exception e) { e.printStackTrace(); } return null; } }
 Transformer JVM 
 Agent Agent  Instrument  JVM 
 Agent Agent  Instrumentation JDK
1.6 Instrumentation  InstrumentNative Code
 Instrument Classpath  Instrumentation 
 Transformer
Agent  Attach  JVM  JVM 
import java.lang.instrument.Instrumentation;
public class TestAgent { public static void agentmain(String args, Instrumentation inst) { //  Transformer Javassist  inst.addTransformer(new TestTransformer(), true); try { //  inst.retransformClasses(Base.class); System.out.println("Agent Load Done.");

342> 2019 
} catch (Exception e) { System.out.println("agent load failed!");
} } }
3.3JVMTI & Agent & Attach API
 Agent  JPDAJava Platform Debugger Architecture JVM  JPDA   JDPA  DebuggerJDPA  Java  JDK 
JPDA   Java JVMTIJava  JDWP Java JDI
 21JPDA

<343
 JVMTI  JVM TIJVM TOOL INTERFACEJVM  JVM  JVM  JVMTI JVM   JVM  JVM  GC  VM 
 Agent  JVMTI Agent  Java   java -agentlib  attach APIjar  Attach  id  Java 
Attach API  JVM   JVM  Dump  jstack  jmap   pid  Dump Attach API   Attach API  loadAgent()  Agent jar   Attach  JVM 
  Agent AgentMain  7  TestAgent 
  TestAgent  MANIFEST.MF  jar  MANIFEST.MF  Agent-Class  TestAgent  
 22Manifest.mf

344> 2019 
  Attach API jar  Attach  JVM pid  
import com.sun.tools.attach.VirtualMachine; public class Attacher {
public static void main(String[] args) throws AttachNotSupportedException, IOException, AgentLoadException, AgentInitializationException {
//  JVM pid VirtualMachine vm = VirtualMachine.attach("39333"); vm.loadAgent("/Users/zen/operation_server_jar/operationserver.jar"); } }
    MANIFEST.MF     Agent-Class    Attach    JVM  TestAgent  agentmain()   Instrumentation TestTransformer  Base  javassist " JVM  "
 Base  main()  JVM"process" Attacher  main()  JVM  pid  main()  "process""start""end" 
 23

<345
3.4
 JVM   JVM  
   Mock Mock   bTrace  Instrument
 JVM
4. 
 JVM   JVM  AOP    
5. 
 ASM4-Guide  Oracle:The class File Format  Oracle:The Java Virtual Machine Instruction Set  javassist tutorial  JVM Tool Interface - Version 1.2



" " 12  3 

346> 2019 
 280  
 /  tech@meituan.com

<347
JVM CPU Profiler 

 Profiling   JVM Profiler  CPUMemory ThreadClassesGC  CPU Profiling CPU Profiling " CPU ""  CPU " CPU Profiling   
 JVM  CPU Profiler  
CPU Profiler 
 JVM Profiler  JProfiler  JVM-Profiler Intellij IDEA   Profiler Blog
 IDEA  Java "Preferences -> Build, Execution, Deployment -> Java Profiler""CPU Profiler" "Run with Profiler" CPU Profiling   5min Profiler "Stop Profiling and Show Results"  Profiling 

348> 2019 
Intellij IDEA - 
Intellij IDEA - 


<349
 "" CPU  
"Run with Profiler"Profiler  JVM Agent  JVM Agent 
JVM Agent 
JVM Agent   JVM JVM  Agent   JVM Agent  C/C++/Rust  JVMTI Agent Java  Java Agent
 Java  Agent 
Plain Text -agentlib:<  >[=<  >]  <  >,  -agentlib:jdwp  -agentlib:jdwp=help -agentpath:<  >[=<  >]  -javaagent:<jar  >[=<  >]  Java  ,  java.lang.instrument
JVMTI Agent
JVMTIJVM Tool Interface JVM  C/C++   DebuggerProfilerMonitorThread Analyser   Java 
 JVMTI  Agent 
// $JAVA_HOME/include/jvmti.h
JNIEXPORT jint JNICALL Agent_OnLoad(JavaVM *vm, char *options, void *reserved);

350> 2019 
 C/C++ Linux  .so  -agentpath  Java JVM   JavaVM  JNI  JVMTI  JVM 
 JVMTI 
Java Agent
 C/C++  JVMTI Agent JVM  JVMTI  Java  Instrument API   Java  Java Agent jar  Agent   GreysArthasJVM-SandboxJVM-Profiler   Java  Java Agent 
 Java Agent  jar  MANIFEST.MF  Premain-Class 
public static void premain(String args, Instrumentation ins) { // implement
}
 jar  Java Agent -javaagent  jar   Java JVM 
 Instrumentation  Retransform Classes   Class  Trace Instrumentation  Class   JMX   Instrument API  JMX  JVMTI 
 Instrument API 

<351
CPU Profiler 
 Profiler  Agent   CPU Profiler CPU Profiling  
Sampling vs Instrumentation
 JProfiler JProfiler  CPU Profiling   : Sampling  Instrumentation CPU Profiler  
Sampling  StackTrace "" 
1.   Profiler        Agent       JVM      Profiler
2.   Dump
3.  Dump  
Instrumentation  Instrument API Class   
Instrumentation  AOP   
Sampling  "" JVM  Safe Point""

352> 2019 
 CPU Time Sampling Profiler  ""  CPU  Sampling Why (Most) Sampling Java Profilers Are Fucking Terrible
"" Sampling  CPU  Instrumentation  I/O   Profiler  Sampling  Sampling  
 Java Agent + JMX 
 Sampling CPU Profiler  Java Agent + JMX   Java Agent  JVM  ScheduledExecutorService JMX  threadMXBean.dumpAllThreads()  StackTrace
Uber  JVM-Profiler 
// com/uber/profiling/profilers/StacktraceCollectorProfiler.java
/* * StacktraceCollectorProfiler  CpuProfiler * jvm-profiler  CpuProfiler  CpuLoad  Profiler */
//  Profiler  ScheduledExecutorService  Profiler  @Override public void profile() {
ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false); // ... for (ThreadInfo threadInfo : threadInfos) {
String threadName = threadInfo.getThreadName(); // ...

<353
StackTraceElement[] stackTraceElements = threadInfo. getStackTrace();
// ... for (int i = stackTraceElements.length - 1; i >= 0; i--) {
StackTraceElement stackTraceElement = stackTraceElements[i]; // ... } // ... } }
Uber  Interval  100ms CPU Profiler   dumpAllThreads() Interval   CPU Profiling 
JVM-Profiler  ProfilingStackTraceCPUBusy MemoryI/OMethod Profiling  Kafka  Server 
 JVMTI + GetStackTrace 
 Java  Profiler  Java Agent  AppClassLoader JVM  agent.jar   Class JVM-Profiler   Kafka-Clienthttp-ClientJackson  Greys/Arthas/JVM-Sandbox   ClassLoader  
 C/C++  JVMTI  C API  JVM  CPU Profiler JVMTI 
1.  Agent_OnLoad() JNI  JavaVM*  GetEnv()   JVMTI  jvmtiEnv 

354> 2019 
// agent.c
JNIEXPORT jint JNICALL Agent_OnLoad(JavaVM *vm, char *options, void *reserved) {
jvmtiEnv *jvmti; (*vm)->GetEnv((void **)&jvmti, JVMTI_VERSION_1_0); // ... return JNI_OK; }
2.  jvmtiEnv  JVMTI 
//  jthread jvmtiError GetAllThreads(jvmtiEnv *env, jint *threads_count_ptr, jthread **threads_ptr);
//  jthread namedaemonpriority... jvmtiError GetThreadInfo(jvmtiEnv *env, jthread thread, jvmtiThreadInfo* info_ptr);
//  jthread  jvmtiError GetStackTrace(jvmtiEnv *env,
jthread thread, jint start_depth, jint max_frame_count, jvmtiFrameInfo *frame_buffer, jint *count_ptr);
 GetAllThreads() ""jthread  jthread  GetThreadInfo()   jthread  GetStackTrace() 
3.  Buffer   JVMTI  CPU Profiler  JVMTI  GetStackTrace()  JMX  ----Safe Point

<355
SafePoint Bias 
 Sampling  CPU Profiler   Sampling CPU Profiler  
1.  2.  Profiler 
   CPU "SafePoint Bias"
        JMX    JVMTI  Profiler      SafePoint BiasJVMTI  GetStackTrace()   Caller  GetStackTrace()  GetStackTrace()   UNIX  Handler  GetStackTrace()  JMX  SafePoint Bias Safepoints: Meaning, Side Effects and Overheads
 SafePoint Bias  Hack ----AsyncGetCallTrace
 JVMTI + AsyncGetCallTrace 
  UNIX  UNIX  Handler  UNIX   
OracleJDK/OpenJDK ----AsyncGetCallTrace

356> 2019 

//  typedef struct {
jint lineno; jmethodID method_id; } AGCT_CallFrame;
//  typedef struct {
JNIEnv *env; jint num_frames; AGCT_CallFrame *frames; } AGCT_CallTrace;
//  ucontext  trace  void AsyncGetCallTrace(AGCT_CallTrace *trace, jint depth, void *ucontext);
 ucontext   Java 
AsyncGetCallTrace "async"  Native GC   Java AGCT_CallTrace  num_frames    -2  GC
 AsyncGetCallTrace  JVMTI  jvmti.h   JVM   Trick Agent  JVM  Agent_ OnLoad  glibc  dlsym()  JVM  "AsyncGetCallTrace" 
 AsyncGetCallTrace  CPU Profiler 

<357
1.   Agent_OnLoad()      jvmtiEnv  AsyncGetCallTrace 
 AsyncGetCallTrace  :
typedef void (*AsyncGetCallTrace)(AGCT_CallTrace *traces, jint depth, void *ucontext); // ... AsyncGetCallTrace agct_ptr = (AsyncGetCallTrace)dlsym(RTLD_DEFAULT, "AsyncGetCallTrace"); if (agct_ptr == NULL) {
void *libjvm = dlopen("libjvm.so", RTLD_NOW); if (!libjvm) {
//  dlerror()... } agct_ptr = (AsyncGetCallTrace)dlsym(libjvm, "AsyncGetCallTrace"); }
2.  OnLoad               OnClassLoad 
OnClassPrepare    Hook    jmethodID        
AGCT  Traces  OnClassPrepare 
CallBack  Class  Methods JVMTI 
 jmethodID
void JNICALL OnClassLoad(jvmtiEnv *jvmti, JNIEnv* jni, jthread thread, jclass klass) {}
void JNICALL OnClassPrepare(jvmtiEnv *jvmti, JNIEnv *jni, jthread thread, jclass klass) {
jint method_count; jmethodID *methods; jvmti->GetClassMethods(klass, &method_count, &methods); delete [] methods; }
// ...
jvmtiEventCallbacks callbacks = {0}; callbacks.ClassLoad = OnClassLoad; callbacks.ClassPrepare = OnClassPrepare; jvmti->SetEventCallbacks(&callbacks, sizeof(callbacks)); jvmti->SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_CLASS_LOAD, NULL); jvmti->SetEventNotificationMode(JVMTI_ENABLE, JVMTI_EVENT_CLASS_PREPARE, NULL);

358> 2019 
3.  SIGPROF 
//  handler  ucontext  AsyncGetCallTrace  ucontext void signal_handler(int signo, siginfo_t *siginfo, void *ucontext) {
//  AsyncCallTrace  num_frames  }
// ...
//  SIGPROF  handler struct sigaction sa; sigemptyset(&sa.sa_mask); sa.sa_sigaction = signal_handler; sa.sa_flags = SA_RESTART | SA_SIGINFO; sigaction(SIGPROF, &sa, NULL);
//  SIGPROF  // interval  nanoseconds AsyncGetCallTrace   long sec = interval / 1000000000; long usec = (interval % 1000000000) / 1000; struct itimerval tv = {{sec, usec}, {sec, usec}}; setitimer(ITIMER_PROF, &tv, NULL);
4.  Buffer   AsyncGetCallTrace  CPU Profiler  CPU Profiler  Linux  perf_events  Java  Native  Native   Async-Profiler  Honest-Profiler Async-Profiler  IntelliJ IDEA  Java Profiler Async-Profiler  AsyncGetCallTrace The Pros and Cons of AsyncGetCallTrace Profilers

 
 svg 

<359 FlameGraph  Perl   svg  
base_func;func1;func2;func3 10 base_func;funca;funcb 15
 "" 
 flamegraph.pl 
$ flamegraph.pl stacktraces.txt > stacktraces.svg

 flamegraph.pl 

360> 2019 
HotSpot  Dynamic Attach 
            CPU Profiler             JProfiler/Arthas   Profling Java  Agent   Dynamic Attach
JDK  1.6  Attach API JVM  Agent  Profiler 
This is a Sun extension that allows a tool to`attach'to another process running Java code and launch a JVM TI agent or a java.lang.instrument agent in that process. Dynamic Attach  HotSpot   JVM  Agent  Dump Dump 
 sun.tools  Attach
Attach  HotSpot  JDK  Java   Java Agent PreMain  Agent   AgentMain  MANIFEST.MF  Agent-Class  Class
public static void agentmain(String args, Instrumentation ins) { // implement
}
 jar -javaagent  Attach   JVM JDK  API  Attach  Java Agent Arthas 
// com/taobao/arthas/core/Arthas.java

<361
import com.sun.tools.attach.VirtualMachine; import com.sun.tools.attach.VirtualMachineDescriptor;
// ...
private void attachAgent(Configure configure) throws Exception { VirtualMachineDescriptor virtualMachineDescriptor = null;
//  JVM  for (VirtualMachineDescriptor descriptor : VirtualMachine.list()) {
String pid = descriptor.id(); if (pid.equals(Integer.toString(configure.getJavaPid()))) {
virtualMachineDescriptor = descriptor; } } VirtualMachine virtualMachine = null; try { //  JVM  VirtualMachine.attach()  VirtualMachine  if (null == virtualMachineDescriptor) {
virtualMachine = VirtualMachine.attach("" + configure. getJavaPid());
} else { virtualMachine = VirtualMachine.
attach(virtualMachineDescriptor); }
// ...
//  VirtualMachine#loadAgent() arthasAgentPath  jar attach  JVM 
//  attach  agentmain  String  args virtualMachine.loadAgent(arthasAgentPath, configure. getArthasCore() + ";" + configure. toString()); } finally { if (null != virtualMachine) {
//  VirtualMachine#detach()  virtualMachine.detach(); } } }

362> 2019 
 HotSpot  Attach
sun.tools  API  Java  Java Agent  JVM  Attach JVMTI  Agent_OnLoad()  Agent_OnAttach()  JVMTI Agent Attach 
// $JAVA_HOME/include/jvmti.h
JNIEXPORT jint JNICALL Agent_OnAttach(JavaVM *vm, char *options, void *reserved);
     Async-Profiler   jattach              Attach  JVM jattach  Async-Profiler  Driver
Usage: jattach <pid> <cmd> [args ...]
Args: <pid>  JVM  ID <cmd>  <args> 

$ jattach 1234 load /absolute/path/to/agent/libagent.so true
libagent.so  ID  1234  JVM  Agent_OnAttach  Attach  euid  egid  Attach  JVM  jattach 
 Main  Attach 
// async-profiler/src/jattach/jattach.c
int main(int argc, char** argv) { //  //  euid  egid

<363
// ...
if (!check_socket(nspid) && !start_attach_mechanism(pid, nspid)) { perror("Could not start attach mechanism"); return 1;
}
int fd = connect_socket(nspid); if (fd == -1) {
perror("Could not connect to socket"); return 1; }
printf("Connected to remote JVM\n"); if (!write_command(fd, argc - 2, argv + 2)) {
perror("Error writing to socket"); close(fd); return 1; } printf("Response code = "); fflush(stdout);
int result = read_response(fd); close(fd); return result; }
             euid  egid   jattach     
check_socket "socket "check_socket 
// async-profiler/src/jattach/jattach.c
// Check if remote JVM has already opened socket for Dynamic Attach static int check_socket(int pid) {
char path[MAX_PATH]; snprintf(path, MAX_PATH, "%s/.java_pid%d", get_temp_directory(), pid); // get_temp_ directory()  Linux  "/tmp" struct stat stats; return stat(path, &stats) == 0 && S_ISSOCK(stats.st_mode); }
UNIX  Socket "UNIX
Socket" S_ISSOCK 

364> 2019 
 UNIX Socket"/tmp/.java_pid"  JVM 
 The attach listener thread then communicates with the source JVM in an OS dependent manner: - On Solaris, the Doors IPC mechanism is used. The door is attached to a file in the file system so that clients can access it. - On Linux, a Unix domain socket is used. This socket is bound to a file in the filesystem so that clients can access it. - On Windows, the created thread is given the name of a pipe which is served by the client. The result of the operations are written to this pipe by the target JVM.
 check_socket   JVM  UNIX Socket 
  Main      check_socket               start_attach_mechanism 
// async-profiler/src/jattach/jattach.c
// Force remote JVM to start Attach listener. // HotSpot will start Attach listener in response to SIGQUIT if it sees .attach_pid file static int start_attach_mechanism(int pid, int nspid) {
char path[MAX_PATH]; snprintf(path, MAX_PATH, "/proc/%d/cwd/.attach_pid%d", nspid, nspid);
int fd = creat(path, 0660); if (fd == -1 || (close(fd) == 0 && !check_file_owner(path))) {
// Failed to create attach trigger in current directory. Retry in /tmp
snprintf(path, MAX_PATH, "%s/.attach_pid%d", get_temp_ directory(), nspid);
fd = creat(path, 0660); if (fd == -1) {
return 0; } close(fd);

<365
}
// We have to still use the host namespace pid here for the kill() call kill(pid, SIGQUIT);
// Start with 20 ms sleep and increment delay each iteration struct timespec ts = {0, 20000000}; int result; do {
nanosleep(&ts, NULL); result = check_socket(nspid); } while (!result && (ts.tv_nsec += 20000000) < 300000000);
unlink(path); return result; }
start_attach_mechanism "/tmp/.attach_pid"  JVM  SIGQUIT  JVM        start_attach_mechanism             20ms  check_socket  300ms   Unlink  .attach_pid 
HotSpot  SIGQUIT         .attach_pid  HotSpot          "/tmp/. java_pid" UNIX Socket Connect 
 Dynamic attach has an attach listener thread in the target JVM. This is a thread that is started when the first attach request occurs. On Linux and Solaris, the client creates a file named .attach_pid(pid) and sends a SIGQUIT to the target JVM process. The existence of this file causes the SIGQUIT handler in HotSpot to start the attach listener thread. On Windows, the client uses the Win32 CreateRemoteThread function to create a new thread in the target process.
 Linux "/tmp/.attach_pid"

366> 2019 
 JVM  SIGQUIT HotSpot "/tmp/. java_pid" UNIX Socket Attach   .attach_pid  Attach Listener   JVM  Attach  SIGQUIT   Attach 
 jattach  connect_socket "/tmp/. java_pid"connect_socket 
// async-profiler/src/jattach/jattach.c
// Connect to UNIX domain socket created by JVM for Dynamic Attach static int connect_socket(int pid) {
int fd = socket(PF_UNIX, SOCK_STREAM, 0); if (fd == -1) {
return -1; }
struct sockaddr_un addr; addr.sun_family = AF_UNIX; snprintf(addr.sun_path, sizeof(addr.sun_path), "%s/.java_pid%d", get_temp_directory(), pid);
if (connect(fd, (struct sockaddr*)&addr, sizeof(addr)) == -1) { close(fd); return -1;
} return fd; }
 Socket  Socket   Main  write_command  Socket   read_response  JVM   Socket 
// async-profiler/src/jattach/jattach.c
// Send command with arguments to socket static int write_command(int fd, int argc, char** argv) {
// Protocol version

<367
if (write(fd, "1", 2) <= 0) { return 0;
}
int i; for (i = 0; i < 4; i++) {
const char* arg = i < argc ? argv[i] : ""; if (write(fd, arg, strlen(arg) + 1) <= 0) {
return 0; } } return 1; }
// Mirror response from remote JVM to stdout static int read_response(int fd) {
char buf[8192]; ssize_t bytes = read(fd, buf, sizeof(buf) - 1); if (bytes <= 0) {
perror("Error reading response"); return 1; }
// First line of response is the command result code buf[bytes] = 0; int result = atoi(buf);
do { fwrite(buf, 1, bytes, stdout); bytes = read(fd, buf, sizeof(buf));
} while (bytes > 0); return result; }
 write_command  JVM  
<PROTOCOL VERSION>\0<COMMAND>\0<ARG1>\0<ARG2>\0<ARG3>\0
 Load  HotSpot 
1\0load\0/absolute/path/to/agent/libagent.so\0true\0\0
 JVM  Attach

368> 2019 

Attach 
Load  HotSpot  JVMTI  Agent

static AttachOperationFunctionInfo funcs[] = {

{ "agentProperties", get_agent_properties },

{ "datadump",

data_dump },

{ "dumpheap",

dump_heap },

{ "load",

JvmtiExport::load_agent_library },

{ "properties",

get_system_properties },

{ "threaddump",

thread_dump },

{ "inspectheap",

heap_inspection },

{ "setflag",

set_flag },

{ "printflag",

print_flag },

{ "jcmd",

jcmd },

{ NULL,

NULL }

};

 threaddump  jstack 


 Profiler  Profiler  CPU Profiler  Attach JVMTIInstrumentationJMX  JVM   Memory ProfilerThread ProfilerGC Analyzer  


 JVM Tool Interface  The Pros and Cons of AsyncGetCallTrace Profilers  Why (Most) Sampling Java Profilers Are Fucking Terrible  Safepoints: Meaning, Side Effects and Overheads  Serviceability in HotSpot    IntelliJ IDEA 2018.3 EAP: Git Submodules, JVM Profiler (macOS and Linux) and
more

<369

 / 

Base    tech@meituan.com

370> 2019 
Java 

    Java  Java   Java   Java-debug-tool 
JVMTIJVM Tool Interface Java         Native      JVMTI JVM GC  Agent  JVM  JVM   Agent  JVM   JVM  JVM  Agent   Debug  Java Agent  
2.1Agent 
JVMTI  Native  Java SE 5  Agent   Native  Java SE 5  Java  Instrumentation java.lang.instrument Agent Native  Java Instrumentation  Agent JVMTI   Java Instrumentation  Agent 
2.1.1 Java Instrumentation API
  Agent 

<371
Java Agent  JVM  JVM   JVM  Agent
[1] public static void premain(String agentArgs, Instrumentation inst); [2] public static void premain(String agentArgs);
JVM  [1] [1] [2] JVM   Agent
[1] public static void agentmain(String agentArgs, Instrumentation inst); [2] public static void agentmain(String agentArgs);
           AgentArgs   "­ javaagent"      inst  Instrumentation  JVM  
  Main-Class Agent        jar   ManiFest     "Premain-Class" "Agent-Class"
Premain-Class: class Agent-Class: class
  JVM  Agent  jar  JVM   JVM  Agent"-javaagent:[=]" "Java -Help" Agent  JVM  com.sun.tools.attach.VirtualMachine  JVM   JVM Agent  JVM  com.sun. tools.attach.VirtualMachine  Agent 

372> 2019 
private void attachAgentToTargetJVM() throws Exception { List<VirtualMachineDescriptor> virtualMachineDescriptors =
VirtualMachine.list(); VirtualMachineDescriptor targetVM = null; for (VirtualMachineDescriptor descriptor :
virtualMachineDescriptors) { if (descriptor.id().equals(configure.getPid())) { targetVM = descriptor; break; }
} if (targetVM == null) {
throw new IllegalArgumentException("could not find the target jvm by process id:" + configure.getPid());
} VirtualMachine virtualMachine = null; try {
virtualMachine = VirtualMachine.attach(targetVM); virtualMachine.loadAgent("{agent}", "{params}"); } catch (Exception e) { if (virtualMachine != null) {
virtualMachine.detach(); } } }
 ID  JVM Attach  JVM 
 Agent VirtualMachine  Attach  Agent 
 JVM  Detach  Agent  JVM  Agent 
 JVM 
2.2 Agent
2.2.1
  JVM JVM                 JVM    
     GC              -agentlib-agentpath
-javaagent AgentJVM  Agent
 JVM 

<373
// -agentlib and -agentpath if (match_option(option, "-agentlib:", &tail) ||
(is_absolute_path = match_option(option, "-agentpath:", &tail))) {
if(tail != NULL) { const char* pos = strchr(tail, '='); size_t len = (pos == NULL) ? strlen(tail) : pos - tail; char* name = strncpy(NEW_C_HEAP_ARRAY(char, len + 1,
mtArguments), tail, len); name[len] = '\0'; char *options = NULL; if(pos != NULL) { options = os::strdup_check_oom(pos + 1, mtArguments); }
#if !INCLUDE_JVMTI if (valid_jdwp_agent(name, is_absolute_path)) { jio_fprintf(defaultStream::error_stream(), "Debugging agents are not supported in this VM\n"); return JNI_ERR; }
#endif // !INCLUDE_JVMTI add_init_agent(name, options, is_absolute_path);
} // -javaagent } else if (match_option(option, "-javaagent:", &tail)) { #if !INCLUDE_JVMTI
jio_fprintf(defaultStream::error_stream(), "Instrumentation agents are not supported in this VM\n");
return JNI_ERR; #else
if (tail != NULL) { size_t length = strlen(tail) + 1; char *options = NEW_C_HEAP_ARRAY(char, length, mtArguments); jio_snprintf(options, length, "%s", tail); add_init_agent("instrument", options, false); // java agents need module java.instrument if (!create_numbered_property("jdk.module.addmods", "java.
instrument", addmods_ count++)) {
return JNI_ENOMEM; } } #endif // !INCLUDE_JVMTI }
          hotspot/src/share/vm/runtime/arguments.cpp  
Arguments::parse_each_vm_init_arg(const JavaVMInitArgs* args, bool* patch_

374> 2019 
mod_javabase, Flag::Flags origin)  JVM   Agent  add_init_agent   add_init_agent 
// -agentlib and -agentpath arguments static AgentLibraryList _agentList; static void add_init_agent(const char* name, char* options, bool absolute_path)
{ _agentList.add(new AgentLibrary(name, options, absolute_path, NULL)); }
AgentLibraryList add_init_agent   Agent 
 -javaagent   Java Instrumentation API  AgentJava Instrumentation API  JVMTI -JavaAgent  add_init_ agent "instrument" Agent   JVM  Agent  JVM   JVM  Agent 
2.2.2
 JVM  JVM  Agent 
// Launch -agentlib/-agentpath and converted -Xrun agents if (Arguments::init_agents_at_startup()) {
create_vm_init_agents(); } static bool init_agents_at_startup() {
return !_agentList.is_empty(); }
 JVM  Agent  create_vm_init_agents  Agent create_vm_init_agents  Agent 

<375
void Threads::create_vm_init_agents() { AgentLibrary* agent; for (agent = Arguments::agents(); agent != NULL; agent = agent->next())
{ OnLoadEntry_t on_load_entry = lookup_agent_on_load(agent); if (on_load_entry != NULL) { // Invoke the Agent_OnLoad function jint err = (*on_load_entry)(&main_vm, agent->options(), NULL); }
} }
create_vm_init_agents  Agent  Agent  lookup_agent_on_load  Agent  Agent_OnLoad  Agent   Agent Agent  Java Instrumentation API  Agent               add_init_agent              "instrument" Linux   libinstrument.so  BSD      libinstrument.dylib        {JAVA_HOME}/jre/lib/ 
2.2.3instrument 
libinstrument       Java Instrumentation API    Agent  libinstrument            JPLISAgentJava Programming Language Instrumentation Services Agent             Java Instrumentation API    Agent         JVMTI   Java Instrumentation  API 
       JVM     JVM    -javaagent     Agent libinstrument  JVMTI Agent_OnLoad libinstrument  Agent_OnLoad 

376> 2019 
JNIEXPORT jint JNICALL DEF_Agent_OnLoad(JavaVM *vm, char *tail, void * reserved) {
initerror = createNewJPLISAgent(vm, &agent); if ( initerror == JPLIS_INIT_ERROR_NONE ) {
if (parseArgumentTail(tail, &jarfile, &options) != 0) { fprintf(stderr, "-javaagent: memory allocation failure.\n"); return JNI_ERR;
} attributes = readAttributes(jarfile); premainClass = getAttribute(attributes, "Premain-Class"); /* Save the jarfile name */ agent->mJarfile = jarfile; /*
* Convert JAR attributes into agent capabilities */ convertCapabilityAttributes(attributes, agent); /* * Track (record) the agent class name and options data */ initerror = recordCommandLineData(agent, premainClass, options); } return result; }
 libinstrument  Agent_OnLoad 
 JPLISAgent ManiFest 
Premain-Class JPLISAgent  initializeJPLISAgent
 Agent  initializeJPLISAgent 
JPLISInitializationError initializeJPLISAgent(JPLISAgent *agent, JavaVM *vm, jvmtiEnv *jvmtienv) {
/* check what capabilities are available */ checkCapabilities(agent); /* check phase - if live phase then we don't need the VMInit event */ jvmtierror = (*jvmtienv)->GetPhase(jvmtienv, &phase); /* now turn on the VMInit event */ if ( jvmtierror == JVMTI_ERROR_NONE ) {
jvmtiEventCallbacks callbacks; memset(&callbacks, 0, sizeof(callbacks)); callbacks.VMInit = &eventHandlerVMInit; jvmtierror = (*jvmtienv)->SetEventCallbacks(jvmtienv,&callbacks, sizeof(callbacks)); } if ( jvmtierror == JVMTI_ERROR_NONE ) {

<377
jvmtierror = (*jvmtienv)->SetEventNotificationMode(jvmtienv,JVM TI_ENABLE,JVMTI_EVENT_ VM_INIT,NULL);
} return (jvmtierror == JVMTI_ERROR_NONE)? JPLIS_INIT_ERROR_NONE : JPLIS_INIT_ERROR_ FAILURE; }
      callbacks.VMInit = &eventHandlerVMInit;     
 VMInit  JVM  even-
tHandlerVMInit 
Premain 
void JNICALL eventHandlerVMInit( jvmtiEnv *jvmtienv,JNIEnv *jnienv,jthread thread) {
// ... success = processJavaStart( environment->mAgent, jnienv); // ... } jboolean processJavaStart(JPLISAgent *agent,JNIEnv *jnienv) {
result = createInstrumentationImpl(jnienv, agent); /*
* Load the Java agent, and call the premain. */ if ( result ) {
result = startJavaAgent(agent, jnienv, agent->mAgentClassName, agent->mOptionsString, agent->mPremainCaller);
} return result; } jboolean startJavaAgent( JPLISAgent *agent,JNIEnv *jnienv,const char *classname,const char *optionsString,jmethodID agentMainMethod) { // ... invokeJavaAgentMainMethod(jnienv,agent->mInstrumentationImpl,agentMainM ethod, classNameObject,optionsStringObject); // ... }
Instrument invokeJavaAgentMainMethod 
 premain  Instrument 

378> 2019 

2.3 Agent
 JVM  Agent Agent   Agent  Agent  Agent  Agent 
2.3.1AttachListener
Attach  Attach Listener  Attach Listener 
// Starts the Attach Listener thread void AttachListener::init() {
//  const char thread_name[] = "Attach Listener"; Handle string = java_lang_String::create_from_str(thread_name, THREAD); { MutexLocker mu(Threads_lock);
JavaThread* listener_thread = new JavaThread(&attach_listener_thread_ entry);
// ... } }
Attach Listener  attach_listener_thread_entry
static void attach_listener_thread_entry(JavaThread* thread, TRAPS) { AttachListener::set_initialized(); for (;;) { AttachOperation* op = AttachListener::dequeue(); // find the function to dispatch too AttachOperationFunctionInfo* info = NULL; for (int i=0; funcs[i].name != NULL; i++) { const char* name = funcs[i].name; if (strcmp(op->name(), name) == 0) { info = &(funcs[i]); break; }} // dispatch to the function that implements this operation

<379

res = (info->func)(op, &st); //... } }

  AttachListener::dequeue     

static AttachOperationFunctionInfo funcs[] = {

{ "agentProperties", get_agent_properties },

{ "datadump",

data_dump },

{ "dumpheap",

dump_heap },

{ "load",

load_agent },

{ "properties",

get_system_properties },

{ "threaddump",

thread_dump },

{ "inspectheap",

heap_inspection },

{ "setflag",

set_flag },

{ "printflag",

print_flag },

{ "jcmd",

jcmd },

{ NULL,

NULL }

};

 Agent "load" Attach Listener
 AttachListen-
er::dequeue  dequeue 
LinuxAttachOperation* LinuxAttachListener::dequeue() { for (;;) { // wait for client to connect struct sockaddr addr; socklen_t len = sizeof(addr); RESTARTABLE(::accept(listener(), &addr, &len), s); // get the credentials of the peer and check the effective uid/guid // - check with jeff on this. struct ucred cred_info; socklen_t optlen = sizeof(cred_info); if (::getsockopt(s, SOL_SOCKET, SO_PEERCRED, (void*)&cred_info,
&optlen) == -1) {

380> 2019 
::close(s); continue; } // peer credential look okay so we read the request LinuxAttachOperation* op = read_request(s); return op; } }
 Linux 
Attach Listener  accept 
 AttachOperation 

Attach Listener ""
JVM  Attach Listener 
""
// Start Attach Listener if +StartAttachListener or it can't be started lazily
if (!DisableAttachMechanism) { AttachListener::vm_start(); if (StartAttachListener || AttachListener::init_at_startup()) { AttachListener::init(); }
} // Attach Listener is started lazily except in the case when // +ReduseSignalUsage is used bool AttachListener::init_at_startup() {
if (ReduceSignalUsage) { return true;
} else { return false;
} }
        create_vm  DisableAttachMechanismStar-
tAttachListener  ReduceSignalUsage          false  
AttachListener::init();  create_vm  vm_start 


<381
void AttachListener::vm_start() { char fn[UNIX_PATH_MAX]; struct stat64 st; int ret; int n = snprintf(fn, UNIX_PATH_MAX, "%s/.java_pid%d", os::get_temp_directory(), os::current_process_id()); assert(n < (int)UNIX_PATH_MAX, "java_pid file name buffer overflow"); RESTARTABLE(::stat64(fn, &st), ret); if (ret == 0) { ret = ::unlink(fn); if (ret == -1) { log_debug(attach)("Failed to remove stale attach pid file at %s", fn); } }
}
 Linux  /tmp/  .java_pid{pid} 
 Attach Listener AttachListen-
er::init()  create_vm 
 Attach Listener  JVM 
""
// Signal Dispatcher needs to be started before VMInit event is posted os::signal_init();
 create_vm  Attach 
""
void os::signal_init() { if (!ReduceSignalUsage) { // Setup JavaThread for processing signals EXCEPTION_MARK; Klass* k = SystemDictionary::resolve_or_fail(vmSymbols::java_lang_
Thread(), true, CHECK); instanceKlassHandle klass (THREAD, k); instanceHandle thread_oop = klass->allocate_instance_handle(CHECK); const char thread_name[] = "Signal Dispatcher"; Handle string = java_lang_String::create_from_str(thread_name,
CHECK); // Initialize thread_oop to put it into the system threadGroup Handle thread_group (THREAD, Universe::system_thread_group()); JavaValue result(T_VOID);

382> 2019 
JavaCalls::call_special(&result, thread_oop,klass,vmSymbols::object_ initializer_name(),vmSymbols::threadgroup_string_void_signature(),
thread_group,string,CHECK); KlassHandle group(THREAD, SystemDictionary::ThreadGroup_klass()); JavaCalls::call_special(&result,thread_group,group,vmSymbols::add_ method_name(),vmSymbols::thread_void_signature(),thread_oop,CHECK); os::signal_init_pd(); { MutexLocker mu(Threads_lock);
JavaThread* signal_thread = new JavaThread(&signal_thread_entry); // ... } // Handle ^BREAK os::signal(SIGBREAK, os::user_handler()); } }
JVM "Signal Dispatcher"
              "Signal Dispatcher"     signal_
thread_entry

 signal_thread_entry  Attach  "SIGBREAK"  Attach  JVM  AttachListener::is_init_trigger()

bool AttachListener::is_init_trigger() {

if (init_at_startup() || is_initialized()) {

return false;

// initialized at startup or already

initialized

}

char fn[PATH_MAX+1];

sprintf(fn, ".attach_pid%d", os::current_process_id());

int ret;

struct stat64 st;

RESTARTABLE(::stat64(fn, &st), ret);

if (ret == -1) {

<383
log_trace(attach)("Failed to find attach file: %s, trying alternate", fn);
snprintf(fn, sizeof(fn), "%s/.attach_pid%d", os::get_temp_ directory(), os::current_process_ id());
RESTARTABLE(::stat64(fn, &st), ret); } if (ret == 0) {
// simple check to avoid starting the attach mechanism when // a bogus user creates the file if (st.st_uid == geteuid()) {
init(); return true; } } return false; }
 JVM  Attach Listener
 /tmp  .attach_pid%d 
 AttachListener  init  Attach Listener 
 Attach 
 Attach Listener  Signal Dispatcher Signal Dis-
patcher  Signal Dispatcher "SIGBREAK"
 Attach Listener 
2.3.2 Agent 
 Agent  JVM 
 Agent"atta-
chAgentToTargetJvm" VirtualMachine
 attach  Agent  VirtualMachine 
attach 
public static VirtualMachine attach(String var0) throws AttachNotSupportedException, IOException {
if (var0 == null) { throw new NullPointerException("id cannot be null");

384> 2019 
} else { List var1 = AttachProvider.providers(); if (var1.size() == 0) { throw new AttachNotSupportedException("no providers
installed"); } else { AttachNotSupportedException var2 = null; Iterator var3 = var1.iterator(); while(var3.hasNext()) { AttachProvider var4 = (AttachProvider)var3.next(); try { return var4.attachVirtualMachine(var0); } catch (AttachNotSupportedException var6) { var2 = var6; } } throw var2; }
} }
 attachVirtualMachine  attach  MacOS 
AttachProvider  BsdAttachProvider BsdAttach-
Provider  attachVirtualMachine 
public VirtualMachine attachVirtualMachine(String var1) throws AttachNotSupportedException, IOException {
this.checkAttachPermission(); this.testAttachable(var1); return new BsdVirtualMachine(this, var1); } BsdVirtualMachine(AttachProvider var1, String var2) throws AttachNotSupportedException, IOException { int var3 = Integer.parseInt(var2); this.path = this.findSocketFile(var3); if (this.path == null) {
File var4 = new File(tmpdir, ".attach_pid" + var3); createAttachFile(var4.getPath()); try {
sendQuitTo(var3); int var5 = 0; long var6 = 200L; int var8 = (int)(this.attachTimeout() / var6); do {
try {

<385
Thread.sleep(var6); } catch (InterruptedException var21) {
; } this.path = this.findSocketFile(var3); ++var5; } while(var5 <= var8 && this.path == null); } finally { var4.delete(); } } int var24 = socket(); connect(var24, this.path); } private String findSocketFile(int var1) { String var2 = ".java_pid" + var1; File var3 = new File(tmpdir, var2); return var3.exists() ? var3.getPath() : null; }
findSocketFile  JVM  Attach Listener
"tmp/" java_pid{pid} 
 Attach 
 connect  JVM "load"
 Agent java_pid{pid}  sendQuitTo 
JVM "SIGBREAK" Attach Listener 
 java_pid{pid} 
 connect  JVM 
2.3.3load 
"load" JVM 
static jint load_agent(AttachOperation* op, outputStream* out) { // get agent name and options const char* agent = op->arg(0); const char* absParam = op->arg(1); const char* options = op->arg(2); // If loading a java agent then need to ensure that the java.
instrument module is loaded if (strcmp(agent, "instrument") == 0) {

386> 2019 
Thread* THREAD = Thread::current(); ResourceMark rm(THREAD); HandleMark hm(THREAD); JavaValue result(T_OBJECT); Handle h_module_name = java_lang_String::create_from_str("java. instrument", THREAD); JavaCalls::call_static(&result,SystemDictionary::module_Modules_ klass(),vmSymbols::loadModule_name(),
vmSymbols::loadModule_signature(),h_module_ name,THREAD);
} return JvmtiExport::load_agent_library(agent, absParam, options, out); }
 java.instrument  Agent   load_agent_library ,  Agent   Java instrument API  Agent libinstrument   libinstrument  agentmain  libinstrument  premain   Java Agent 
3.1
 Agent  Java Instrumentation API  Instrumentation  addTransformer                  ClassFileTransformer     ClassFileTransformer         transform            transform  Instrumentation  redefineClasses  
* The redefinition may change method bodies, the constant pool and attributes.
* The redefinition must not add, remove or rename fields or methods, change the
* signatures of methods, or change inheritance. These restrictions maybe be

<387
* lifted in future versions. The class file bytes are not checked, verified and installed
* until after the transformations have been applied, if the resultant bytes are in
* error this method will throw an exception.

 ASM 
 redefineClasses 
 redefineClasses 
 JVM  redefineClasses 

3.2
      libinstrument      JPLISAgent     
Agent  Java  JVMTI 
 Java  Java Instrumentation API  redefineClasses
libinstrument 
public void redefineClasses(ClassDefinition... var1) throws ClassNotFoundException {
if (!this.isRedefineClassesSupported()) { throw new UnsupportedOperationException("redefineClasses is
not supported in this environment"); } else if (var1 == null) { throw new NullPointerException("null passed as
'definitions' in redefineClasses"); } else { for(int var2 = 0; var2 < var1.length; ++var2) { if (var1[var2] == null) { throw new NullPointerException("element of
'definitions' is null in redefineClasses"); }
} if (var1.length != 0) {
this.redefineClasses0(this.mNativeAgent, var1); } } }

388> 2019 
private native void redefineClasses0(long var1, ClassDefinition[] var3) throws ClassNotFoundException;
  InstrumentationImpl   redefineClasses            Native  redefineClasses() libinstrument  Native 
JNIEXPORT void JNICALL Java_sun_instrument_InstrumentationImpl_ redefineClasses0
(JNIEnv * jnienv, jobject implThis, jlong agent, jobjectArray classDefinitions) {
redefineClasses(jnienv, (JPLISAgent*)(intptr_t)agent, classDefinitions); }
redefineClasses  
 JVMTI  RetransformClasses  
// class_count - pre-checked to be greater than or equal to 0 // class_definitions - pre-checked for NULL jvmtiError JvmtiEnv::RedefineClasses(jint class_count, const jvmtiClassDefinition* class_ definitions) { //TODO: add locking
VM_RedefineClasses op(class_count, class_definitions, jvmti_class_ load_kind_redefine);
VMThread::execute(&op);

<389
return (op.check_error()); } /* end RedefineClasses */
         JVM      VM_RedefineClasses    VM_ OperationVM_Operation  JVM             GC    VM_Operation  VMThread      VM_Operation        VMThread VMThread  VM_Operation   doit VM_RedefineClasses   VM_RedefineClasses 
 
// Load the caller's new class definition(s) into _scratch_classes. // Constant pool merging work is done here as needed. Also calls // compare_and_normalize_class_versions() to verify the class // definition(s). jvmtiError load_new_class_versions(TRAPS);
 
// Remove all breakpoints in methods of this class JvmtiBreakpoints& jvmti_breakpoints = JvmtiCurrentBreakpoints::get_ jvmti_breakpoints(); jvmti_breakpoints.clearall_in_class_at_safepoint(the_class());
 JIT 
// Deoptimize all compiled code that depends on this class flush_dependent_code(the_class, THREAD);
  itable/vtable   
SystemDictionary::notice_modification();
VM_RedefineClasses  RedefineClasses 

390> 2019  Java-debug-tool  Java Instrument API 
 JVM  TcpServer   TcpServerTcpServer  handler handler   Java-debug-tool 
4.1Java-debug-tool 
Java-debug-tool  Java Agent  API  API  JVM   Java-debug-tool  Client-Server  Java-debug-tool  
1

<391

   
    Java-debug-tool  Java Instrumenta-
tion  API Java Instrumentation   JVMTI 
 Agent  JVM Java-debug-tool  Spy   JVM  Spy  JVM     Java-debug-tool  
4.2Java-debug-tool 
Java-debug-tool  """" Java-debug-tool   Java-debug-tool ""
   Fields  1    Fields  2    

392> 2019  Java-debug-tool 

4.2.1
Java-debug-tool  ASM    Java-debug-tool     Java-debug-tool 
2
 2 Java-debug-tool   +     

<393
 
  Java-debug-tool 
   Advice   Advice  Java-debug-tool   Advice Advice  Java-debug-tool  AdviceAdvice   Java-debug-tool 
4.2.2Advice 
Advice  AdviceAdvice   JVM  Advice  "match"  Java-debug-tool  " Advice"->" Advice"
 Advice  JVM  JVM  Advice  Advice   Advice  Java-debug-tool 

394> 2019 
3
Advice  Java-debug-tool   Advice  Advice   Advice   Advice  Advice 
4.3Java-debug-tool 
4.3.1
 Java-debug-tool   Java-debug-tool  

<395
4
 4  Java-debug-tool         Advice  
Java-debug-tool   Java-debug-tool   Java-debug-tool   

396> 2019 
Java-debug-tool    
4.3.2 
Java-debug-tool 
      JVM  STW               
 5  Java-debug-tool 

<397
5
4.4Java-debug-tool 
Java-debug-tool  greys greys  greys  Java-debug-tool 

398> 2019 
 Java  Java    

 ASM 4 guide  Java Virtual Machine Specification  JVM Tool Interface  alibaba arthas  openjdk



 -      Java   tech@meituan.com

<399
 ReentrantLock  AQS 


Java LockSemaphoreReentrantLock  AbstractQueuedSynchronizer AQSAQS    ReentrantLock  ReentrantLock  AQS   AQS   AQS AQS  Sync Queue  Condition Queue  AQS   ReentrantLock ReentrantLock 


400> 2019 
1. ReentrantLock
1.1ReentrantLock 
ReentrantLock   ReentrantLock  ReentrantLock   Synchronized 

// **************************Synchronized  ************************** // 1.  synchronized (this) {} // 2.  synchronized (object) {} // 3.  public synchronized void test () {} // 4.  for (int i = 0; i < 100; i++) {
synchronized (this) {} } // **************************ReentrantLock  **************************

<401
public void test () throw Exception { // 1.  ReentrantLock lock = new ReentrantLock(true); // 2.  lock.lock(); try { try { // 3.  ;  if(lock.tryLock(100, TimeUnit.MILLISECONDS)){ } } finally { // 4.  lock.unlock() } } finally { lock.unlock(); }
}
1.2ReentrantLock  AQS 
ReentrantLock 
 Java"" ReentrantLock
 AQS  ReentrantLock 
 AQS   AQS 
 AQS 

// java.util.concurrent.locks.ReentrantLock#NonfairSync
//  static final class NonfairSync extends Sync {
... final void lock() {
if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread());
else acquire(1);
} ... }


402> 2019 
  CAS  State 
  CAS  State Acquire 
 
  1
 AQS  2    1       

// java.util.concurrent.locks.ReentrantLock#FairSync
static final class FairSync extends Sync { ... final void lock() { acquire(1); } ...
}
Lock  Acquire 

<403
 
Acquire  Acquire  FairSync  UnfairSync  AQS  
 ReentrantLock   Acquire  AbstractQueuedSynchronizer  ----AQS AQS  ReentrantLock  AQS   2.3.5 
2. AQS
 AQS 

404> 2019 
  Method Attribution  AQS  AQS 
API   
  API  AQS     AQS  
2.1
AQS    CLH  
CLHCraigLandin and Hagersten AQS  CLH FIFOAQS  


<405

AQS  Volatile  int  FIFO  CAS  State 
2.1.1AQS   AQS ----NodeNode  CLH  







waitStatus 

thread



prev



predecessor  npe

nextWaiter

 CONDITION  Condition Queue 

next





 SHARED

 

EXCLUSIVE 

waitStatus 

406> 2019 





0

 Node 

CANCELLED  1

CONDITION  -2

PROPAGATE  -3 SHARED 

SIGNAL

 -1

2.1.2 State  AQS ----StateAQS   state  Volatile  
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private volatile int state;



 protected final int getState() protected final void setState(int newState) protected final boolean compareAndSetState(int expect, int update)

  State   State   CAS  State

 Final  State 

<407

408> 2019 

  AQS API 

2.2AQS  ReentrantLock 
AQS  Protected   State  ReentrantLock  

 protected boolean isHeldExclusively()

  Condition  

protected boolean tryAcquire(int arg 

arg)

 True False

protected boolean tryRelease(int arg 

arg)

 True False

protected int tryAcquireShared(int arg)

arg  0  

protected boolean tryRelease- arg 

Shared(int arg)

 True False

                               tryAcquire-tryReleasetryAcquireShared-tryReleaseShared      AQS  ReentrantReadWriteLockReentrantLock  tryAcquire-tryRelease
 AQS  

<409
 ReentrantLock  AQS  

410> 2019 
   ReentrantLock  Lock          Sync  Lock     Sync#lock       
ReentrantLock  Lock   AQS  Acquire   AQS  Acquire  tryAcquire  tryAcquire   ReentrantLock  tryAcquire  ReentrantLock  tryAcquire 

<411  tryAcquire  tryAcquire  AQS  ReentrantLock      ReentrantLock  Unlock   Unlock  Sync  Release  AQS  Release     tryRelease  tryRelease           tryRelease  ReentrantLock  Sync     AQS   ReentrantLock  API  
2.3 ReentrantLock  AQS
ReentrantLock  

// java.util.concurrent.locks.ReentrantLock static final class NonfairSync extends Sync {
... final void lock() {

412> 2019 

} ... }

if (compareAndSetState(0, 1)) setExclusiveOwnerThread(Thread.currentThread());
else acquire(1);

 Acquire 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();
}

 tryAcquire 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
protected boolean tryAcquire(int arg) { throw new UnsupportedOperationException();
}

 AQS   ReentrantLock  True  
2.3.1 2.3.1.1  Acquire(1)  tryAcquire   addWaiter  2.3.1.2  addWaiter(Node.EXCLUSIVE)  

<413
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node;
} private final boolean compareAndSetTail(Node expect, Node update) {
return unsafe.compareAndSwapObject(this, tailOffset, expect, update); }

 
 Pred  Tail
  New  Node  Prev  Pred
   compareAndSetTail                  
tailOffset  Expect  tailOffset  Node  Expect  Node
 Tail  Update 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
static { try { stateOffset = unsafe.objectFieldOffset(AbstractQueuedSynchronizer.
class.getDeclaredField("state")); headOffset = unsafe.
objectFieldOffset(AbstractQueuedSynchronizer.class. getDeclaredField("head"));
tailOffset = unsafe. objectFieldOffset(AbstractQueuedSynchronizer.class. getDeclaredField("tail"));
waitStatusOffset = unsafe.objectFieldOffset(Node.class. getDeclaredField("waitStatus"));

414> 2019 
nextOffset = unsafe.objectFieldOffset(Node.class. getDeclaredField("next"));
} catch (Exception ex) { throw new Error(ex); } }
 AQS   tailOffset  tail  new  Node  
  Pred  Null Pred  Tail  Enq 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private Node enq(final Node node) { for (;;) { Node t = tail; if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; } } }
}
  addWaiter   

 1.  1  2.  2  1 

<415

3.  hasQueuedPredecessors   False  True
// java.util.concurrent.locks.ReentrantLock
public final boolean hasQueuedPredecessors() { // The correctness of this depends on head being initialized // before tail and on head.next being accurate if the current // thread is first in queue. Node t = tail; // Read fields in reverse initialization order Node h = head; Node s; return h != t && ((s = h.next) == null || s.thread != Thread.
currentThread()); }
 h != t && ((s = h.next) == null || s.thread != Thread. currentThread());  
  h != t  (s =

416> 2019 
h.next) == null Tail   Head Head  Tail True   (s = h.next) != null  s.thread == Thread.currentThread()   s.thread != Thread.currentThread() 
// java.util.concurrent.locks.AbstractQueuedSynchronizer#enq
if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head;
} else { node.prev = t; if (compareAndSetTail(t, node)) { t.next = node; return t; }
}
 head != tail Tail        Tail   Head   Head     Tail  567   
2.3.1.3 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
public final void acquire(int arg) { if (!tryAcquire(arg) && acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();
}
 addWaiter  Node   Node Node 

<417
 acquireQueued acquireQueued 
""
acquireQueued 

"""" acquire-
Queued 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
final boolean acquireQueued(final Node node, int arg) { //  boolean failed = true; try { //  boolean interrupted = false; //  for (;;) { //  final Node p = node.predecessor(); //  p 
 if (p == head && tryAcquire(arg)) { //  node setHead(node); p.next = null; // help GC failed = false; return interrupted; } //  p 
 p  node  waitStatus  -1
if (shouldParkAfterFailedAcquire(p, node) && parkAndCheckInterrupt())
interrupted = true; } } finally { if (failed)
cancelAcquire(node); } }
setHead  waitStatus


418> 2019 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private void setHead(Node node) { head = node; node.thread = null; node.prev = null;
}
// java.util.concurrent.locks.AbstractQueuedSynchronizer
//  private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {
//  int ws = pred.waitStatus; //  if (ws == Node.SIGNAL)
return true; //  waitStatus>0  if (ws > 0) {
do { //  node.prev = pred = pred.prev;
} while (pred.waitStatus > 0); pred.next = node; } else { //  SIGNAL compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; }
parkAndCheckInterrupt 

// java.util.concurrent.locks.AbstractQueuedSynchronizer
private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted();
}


<419
" " CPU  shouldParkAfterFailedAcquire 

420> 2019 
  shouldParkAfterFailedAcquire 
 waitStatus  -1    2.3.2CANCELLED  acquireQueued  Finally 
// java.util.concurrent.locks.AbstractQueuedSynchronizer final boolean acquireQueued(final Node node, int arg) {
boolean failed = true; try {

<421
... for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { ... failed = false;
... } ...
} finally { if (failed) cancelAcquire(node); }
}
  cancelAcquire    Node       CANCELLED   

// java.util.concurrent.locks.AbstractQueuedSynchronizer
private void cancelAcquire(Node node) { //  if (node == null) return; //  node.thread = null; Node pred = node.prev; //  node while (pred.waitStatus > 0) node.prev = pred = pred.prev; //  Node predNext = pred.next; //  node  CANCELLED node.waitStatus = Node.CANCELLED; //  //  else tail  null if (node == tail && compareAndSetTail(node, pred)) { compareAndSetNext(pred, predNext, null); } else { int ws; //  head 1:  SIGNAL
2:  SINGAL  //  1  2  true null //  if (pred != head && ((ws = pred.waitStatus) == Node.SIGNAL
|| (ws <= 0 && compareAndSetWaitStatus(pred, ws, Node.SIGNAL))) && pred.thread != null)

422> 2019 
{ Node next = node.next; if (next != null && next.waitStatus <= 0) compareAndSetNext(pred, predNext, next);
} else { //  head  
unparkSuccessor(node); } node.next = node; // help GC } }

  CANCELLED  waitStatus <= 0  Pred  Node  Node  CANCELLED

1 2 Head  3 Head 
 

<423  Head   Head 

424> 2019 
 CANCELLED   Next  Prev   Prev 
  cancelAcquire   Try  shouldParkAfterFailedAcquire   Prev  Prev  Node  Prev  shouldParkAfterFailedAcquire                 Prev  shouldParkAfterFaile-
dAcquire   Prev  
do { node.prev = pred = pred.prev;
} while (pred.waitStatus > 0);
2.3.3 

<425
 ReentrantLock 

// java.util.concurrent.locks.ReentrantLock
public void unlock() { sync.release(1);
}

// java.util.concurrent.locks.AbstractQueuedSynchronizer
public final boolean release(int arg) { if (tryRelease(arg)) { Node h = head; if (h != null && h.waitStatus != 0) unparkSuccessor(h); return true; } return false;
}
 ReentrantLock  Sync 

// java.util.concurrent.locks.ReentrantLock.Sync
//  protected final boolean tryRelease(int releases) {
//  int c = getState() - releases; //  if (Thread.currentThread() != getExclusiveOwnerThread())
throw new IllegalMonitorStateException(); boolean free = false; //  null state if (c == 0) {
free = true; setExclusiveOwnerThread(null); } setState(c); return free; }

426> 2019 



// java.util.concurrent.locks.AbstractQueuedSynchronizer

public final boolean release(int arg) { //  tryRelease  true

if (tryRelease(arg)) { // 



Node h = head; //  waitStatus 

if (h != null && h.waitStatus != 0)

unparkSuccessor(h);

return true;

}

return false;

}

 h != null && h.waitStatus != 0 
h == null Head head == null
Head 
 head == null 
h != null && waitStatus == 0 

h != null && waitStatus < 0 
 unparkSuccessor 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private void unparkSuccessor(Node node) { //  waitStatus int ws = node.waitStatus; if (ws < 0) compareAndSetWaitStatus(node, ws, 0); //  Node s = node.next; //  null  cancelled
cancelled  if (s == null || s.waitStatus > 0) { s = null; //  waitStatus<0 

<427
for (Node t = tail; t != null && t != node; t = t.prev) if (t.waitStatus <= 0) s = t;
} //  <=0 unpark if (s != null)
LockSupport.unpark(s.thread); }
 Cancelled 
 addWaiter 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; if (pred != null) { node.prev = pred; if (compareAndSetTail(pred, node)) { pred.next = node; return node; } } enq(node); return node;
}
                       node.prev =
pred; compareAndSetTail(pred, node)  Tail 
 pred.next = node;  unparkSuccessor
                             
CANCELLED  Next Prev 
 Node
                           
CANCELLED          Next               
                              

428> 2019 
acquireQueued 
2.3.4  return Thread.interrupted();

// java.util.concurrent.locks.AbstractQueuedSynchronizer
private final boolean parkAndCheckInterrupt() { LockSupport.park(this); return Thread.interrupted();
}
   acquireQueued    parkAndCheckInterrupt   True  
False interrupted 
 interrupted 
// java.util.concurrent.locks.AbstractQueuedSynchronizer
final boolean acquireQueued(final Node node, int arg) { boolean failed = true; try { boolean interrupted = false; for (;;) { final Node p = node.predecessor(); if (p == head && tryAcquire(arg)) { setHead(node); p.next = null; // help GC failed = false; return interrupted; } if (shouldParkAfterFailedAcquire(p, node) &&
parkAndCheckInterrupt()) interrupted = true;
} } finally {
if (failed) cancelAcquire(node);
} }
 acquireQueued  True selfInterrupt 

<429
// java.util.concurrent.locks.AbstractQueuedSynchronizer static void selfInterrupt() {
Thread.currentThread().interrupt(); }
  Java 
1.   Thread.interrupted()   False
2.   
 Worder  runWorker  Thread.interrupted()  ThreadPoolExecutor 
2.3.5  1.3 
Q A  Q  A CLH  FIFO  Q A 2.3.1.3 

430> 2019 
Q  A  2.3.2  QLock  Acquire  AAQS  Acquire  tryAcquire tryAcquire   tryAcquire 
3. AQS 
3.1ReentrantLock 
ReentrantLock  AQS   ReentrantLock  ReentrantLock  

// java.util.concurrent.locks.ReentrantLock.FairSync#tryAcquire
if (c == 0) { if (!hasQueuedPredecessors() && compareAndSetState(0, acquires)) { setExclusiveOwnerThread(current); return true; }
} else if (current == getExclusiveOwnerThread()) {
int nextc = c + acquires; if (nextc < 0)
throw new Error("Maximum lock count exceeded"); setState(nextc); return true; }

// java.util.concurrent.locks.ReentrantLock.Sync#nonfairTryAcquire
if (c == 0) { if (compareAndSetState(0, acquires)){

<431

setExclusiveOwnerThread(current); return true; } } else if (current == getExclusiveOwnerThread()) { int nextc = c + acquires; if (nextc < 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; }

 State  State  Volatile 
// java.util.concurrent.locks.AbstractQueuedSynchronizer private volatile int state;

 State  1. State  0 2.  +1
 +1 3.  -1 0

3.2JUC 
 ReentrantLock AQS   JUC   AQS 



AQS

ReentrantLock

 AQS ReentrantLock  

Semaphore

 AQS tryRelease  acquireShared 

 AQS  0  Acquire  CountDownLatch
CountDownLatch  await 

432> 2019 

 ReentrantReadWriteLock ThreadPoolExecutor

AQS  AQS  16  16   Worker  AQS tryAcquire  tryRelease

3.3
 AQS  AQS 

public class LeeLock {
private static class Sync extends AbstractQueuedSynchronizer { @Override protected boolean tryAcquire (int arg) { return compareAndSetState(0, 1); }
@Override protected boolean tryRelease (int arg) {
setState(0); return true; }
@Override protected boolean isHeldExclusively () {
return getState() == 1; } }
private Sync sync = new Sync();
public void lock () { sync.acquire(1);
}
public void unlock () { sync.release(1);
} }
 Lock 

<433
public class LeeMain {
static int count = 0; static LeeLock leeLock = new LeeLock();
public static void main (String[] args) throws InterruptedException {
Runnable runnable = new Runnable() { @Override public void run () { try { leeLock.lock(); for (int i = 0; i < 10000; i++) { count++; } } catch (Exception e) { e.printStackTrace(); } finally { leeLock.unlock(); }
} }; Thread thread1 = new Thread(runnable); Thread thread2 = new Thread(runnable); thread1.start(); thread2.start(); thread1.join(); thread2.join(); System.out.println(count); } }
 20000
 AQS 


 ReentrantLock  AQS 
 AQS  ReentrantLock ""

434> 2019 

 Lea D. The java. util. concurrent synchronizer framework[J]. Science of Computer Programming, 2005, 58(3): 293-309.
 Java    Java""

 Java 2018 

  12  3   300  1   2018    tech@meituan. com



<435

 Kubernetes 



" """   
 Kubernetes  Kubernetes Kubernetes  

""2013  2015  7  ----HULK2016  Docker    IT 2018  Kubernetes  

436> 2019 

 Docker     Docker  ""----HULK HULK1.0
   2  HULK 1.0   HULK   Kubernetes  Kubernetes  Kubernetes ----HULK2.0

HULK2.0  Kubernetes    Kubernetes   Kubernetes  Kubernetes API  Kubernetes

<437  

HULK2.0 
  OPS  Portal HULK   ,  HULK API  HULK  HULK HULK API   HULK 

438> 2019 
 Kubernetes HULK  IP  Hostname  Kubernetes  Kubernetes API   Kubernetes Kubernetes API  HULK   Kubernetes API 
HULK API  Kubernetes API  
Kubernetes 
 Kubernetes  Kubernetes   Docker Swarm  Mesos  Kubernetes     

<439
HULK-Kubernetes 
Kubernetes  HULK  

 10  +   container-init 
   NodePod  Container 
 Node Container   
D  AppKey  Pod  

440> 2019   Pod    Pod    
Kubernetes 
kube-scheduler 
 1.6  Kubernetes 1.10   3000  Pod   5s Kubernetes   Pod  Pod    400% 
Kubernetes 
kube-scheduler 

<441 kubernetes 

 Node    Predicates
 Predicate  Node   Priorities  Priority    Select Node  Pod 
kube-scheduler 
 Node  Node  
"" Node 

442> 2019  
kube-scheduler  Predicates 
           Kubernetes   (   PR)    alwaysCheckAllPredicates  Kubernetes1.10   alwaysCheckAllPredicates=true  
               40%             Kube-scheduler  1.10

  

<443  
kube-scheduler 
 Node  BestFit  Node  Node  Node 
 1000  Node PodA 700  Node   Predicates Node  700  Node Node  NodeX  N  Node N  Node  Node 

444> 2019 
 100  Predicates Node   100  Node  NodeX  100  Node  100  Node  NodeY   100  Node  100  Node 200  300  Node  
  (  PR1PR2) Kubernetes 1.12   
kubelet 

 Kubelet  Kubelet   Kubelet    Kubelet  
 Kernel  Kernel    Kubelet  ReuseRebuild 

<445
IP   CNI  Pod   IP IP  Pod  
  Kubelet    Kubelet   
 
 Kubelet  Numa  CPUShare  CPUSet 
  ulimit io limitpid limitswap 
     Kubernetes     Pod               Pod  IP  HostName   
 

446> 2019  
  Site   Site 
    
 P2P   P2P  



<447

  CPU I/O   
   
     CPU  CPU Set   
  
   Numa CPU Set 

 
   CPU Set   CPU  CPU  

448> 2019 
  
     
  TorZone 
  
  GPUSSD 

   
 Numa  Numa       CPU  Memory        Numa Node     Node 
 CPU Set  CPU   CPU 
   SLA
  SLA

<449
  
SLA


  -    SLA  



  IT  Kubernetes    IT  Kubernetes  Linux   tech@meituan.com 

450> 2019 
 HULK 

 /  2019 QCon  Kubernetes   Kubernetes 

HULK  HULK   VM 
    5 
   HULK  
HULK   HULK  """ ""HULK"
 HULK  1.0  2.0 

<451
HULK 1.0  OpenStack   CMDB   2018  OpenStack  Kubernetes  HULK 2.0 1.0  
       1  10 

452> 2019 
HULK2.0 
 CMDB   VM 
    CPU/IO 
    HULK Agent  Node    

3.1
 

<453
 a.  b.  c.  API  K8s  K8s  Master  Work d.  Node  IP  Hostname  IP e. Container-init  Agent     traceId TaskId  TaskId

454> 2019 
 HULK Portal  
  1 
 
3.2
 

                    swap   memlock ulimit 
 ZooKeeper 
 

<455   Set   
 N+1  IDC     Manifest  Kubernetes  YAML  
3.3
 Kubernetes  Scheduler   ApiServer ApiServer  Bind  Pod   Node Bind  Predicates  Priorities 
Predicates PodFitsResources   CPU Pod   Node
Priorities 

456> 2019   LeastRequestedCPU    BalancedResourcesAllocationCPU  0-10
  3000  Pod  5s K8s
1.6  Node   Node  
 Node   Node 
 40%  1.10   GitHub  PR
 SLA 

<457  
 Pod  Pod   N    SLA
3.4
 1 /      IP 

458> 2019  2Kubelet   1 /    Reuse Rebuild   CNI  Pod  IP 2


Raptor  CATFalcon   5 

4.1

<459

 2  1 
  QPSCPU 
 
 5  2  10  10  5  2 
 
4.2

460> 2019  
 12   8  TaskId  TaskId   8  28 
 20  
4.3
 30  3 ABC     
  

4.4

<461

 2  20  15   30  50   
 1 80% 2  
4.5

462> 2019 
 1:00:00~1:01:00   1:01:10  70s+ 30s+  100s+
   1~2 

  "" Kubernetes 
  
   SLA 
 SLA 
   VM 
   HULK HULK 




Base     tech@meituan.com

<463
 IDC  HIDS 


 IDC   IDC   Host-based Intrusion Detection System HIDS HIDS   IDC  

 HIDS 
1.  50W-100W  IDC  2.  Agent  3.  4.  5.  Agent 
 6. Agent  Server  7.  8.  9. Agent 

HIDS Agent 

464> 2019 
  Agent 
 Agent   1-2  30 20  
 Agent  HIDS  Agent   Agent 
Agent  Agent 
 Agent    

1.  2.  3.  4.  5.  Linux  6.  CPU 7.  8. 

 

<465   CPU   Agent       Agent   
  

 Agent     CAP 
CAP-theorem.png

466> 2019 
CAP 
 CAP 
 Consistency  Availability  Partition Tolerance
 
                               CAP   Consistency  Availability  Consistency  Availability Partition Tolerance
CAP Theorem
CAP 
                         Partition tolerance AP  CP
 HIDS  Agent    Agent   CP 
 CP 
 CP  etcdZooKeeper Consul  

<467  Consul  AP  Consul Partition tolerance    consul             Consul uses a CP architecture, favoring consistency over availability.
Consul is opinionated in its usage while Serf is a more flexible and general purpose tool. In CAP terms, Consul uses a CP architecture, favoring consistency over availability. Serf is an AP system and sacrifices consistency for availability. This means Consul cannot operate if the central servers cannot form a quorum while Serf will continue to function under almost all circumstances.
etcdZooKeeperConsul 
 etcd  etcd  ZooKeeper  Consul 
etcd-ZooKeeper-Consul
 HIDS Agent       Key  etcd   KubernetesAWSOpenStackAzure Google CloudHuawei Cloud  etcd 

468> 2019 
 etcd  HIDS 
 etcd
 etcd  API  
 Watch    etcd etcd  N/2+1 
 Leader Member     Golang Client SDK   Key  Key   UserRole  Key   Key   TLS   Txn  API  Compare API  Key   Lease  Key   etcd  Key  BTree  O n  Key 
etcd Key 

 Server  /hids/server/config/{hostname}/master  Agent  /hids/agent/master/{hostname}  Plugin  /hids/agent/config/{hostname}/plugin/ID/conf_name
Server Watch /hids/server/config/{hostname}/master Agent  Agent Watch /hids/server/config/{hostname}/ 

<469
Agent  Key  Lease Id keepalive  1/3  keepalive 
 Key  Role  User  Key 
etcd 
 etcd  DNS    N  etcd   N/2-1    
 IP IPIP  DNS  Member  Discover  DNS  
 etcd  DNS   IP
1. IP  DNS  2. 
 TLS   IP DNS 
etcd Cluster 
etcd  Client   IP  IP etcd Node

470> 2019  List etcd Cluster  TLS 
 HIDS 
hids-cluster-architecture
 AgentServer   Server  Agent 
 Server Agent etcd 

                             

<471
Linux  Windows  Golang  
1.  syscall  libc  Linux 
2.  3.  GC  4.  Golang  KubernetesDocker  5. etcd  Golang SDK  6.  CSP 

HIDS  
      Agent     Hook   

 

472> 2019 

hids-framework
 Interface etcd Client Logger App  App 

  App Loggeretcd Client   App  sandbox   App  
IConfig
 App  IConfig   Check  
 Reflect  JSON  Config   Struct  JSON 

<473
 Config  Struct 
type IConfig interface { Check() error // 
}
func ConfigLoad(confByte []byte, config IConfig) (IConfig, error) { ... //  IConfig
var confTmp IConfig confTmp = reflect.New(reflect.ValueOf(config).Elem().Type()). Interface().(IConfig) ...
//  confTmp  confTmpReflect := reflect.TypeOf(confTmp).Elem() confTmpReflectV := reflect.ValueOf(confTmp).Elem()
//  config IConfig configReflect := reflect.TypeOf(config).Elem() configReflectV := reflect.ValueOf(config).Elem() ... for i = 0; i < num; i++ { //  Field
envStructTmp := configReflect.Field(i) //  if envStructTmp.Type == confStructTmp.Type {
configReflectV.FieldByName(envStructTmp.Name). Set(confTmpReflectV.Field(i))
TimerClock 


clock_gettime  CPU  

 100ms200ms 1s 

 Ticker Ticker 
 Ticker CPU 

474> 2019 
Catcher
 panic   

         Sandbox  InitRunShutdown      App  App App   PID  App  Context   Sync.Cond  App   App 

 IO
         
 IO
 /usr/include/sys/syslog.h
 LOG_EMERG  LOG_ALERT  LOG_CRIT  LOG_ERR  LOG_WARNING

<475
 LOG_NOTICE  LOG_INFO  LOG_DEBUG
 
 2  20M50M 
IRetry
 Agent  RPC  Agent  etcd Cluster  TCP HTTP2 Agent   Agent      Agent  Agent  
//  Interface type INetRetry interface {
//  Connect() error String() string //  GetMaxRetry() uint ... } //  func (this *Context) Retry(netRetry INetRetry) error { ... maxRetries = netRetry.GetMaxRetry() //  hashMod = netRetry.GetHashMod() for { if c.shutting {
return errors.New("c.shutting is true...")

476> 2019 
} if maxRetries > 0 && retries >= maxRetries {
c.logger.Debug("Abandoning %s after %d retries.", netRetry.String(), retries)
return errors.New("  ") } ... if e := netRetry.Connect(); e != nil {
delay = 1 << retries if delay == 0 {
delay = 1 } delay = delay * hashInterval ... c.logger.Emerg("Trying %s after %d seconds , retries:%d,error:%v", netRetry.String(), delay, retries, e) time.Sleep(time.Second * time.Duration(delay)) } ... }

 IDC  Agent 





 etcd  Metrics 
Prometheus  Grafana  Alert
 IM
Agent  Watch 
Active Streams etcd
Watch  Key  Agent 

<477 Agent  Agent  Watch  Key 
etcd-Grafana-Watcher-Monitor
etcd  Members 
etcd-Grafana-GC-Heap-Objects
 etcd  Member Leader  GC  

478> 2019 

agent-mem-es
agent-cpu-es

<479
 Agent  CPU\  Agent  
  
  ID   ID   


     


  Agent CPU  N   5% 
 Master  Ticker    App 

 etcd Key  Agent  Key  Agent 

480> 2019  


 log_agent  VM Docker  log_agent  Kafka  Hive 


 etcd  Agent   CPU 
 BUG  Agent 



 linux  

 Docker 





cn_proc  Docker 

 PID /proc/  


Audit  Docker 

 cn_proc

 Auditd

Hook 







  Agent  
  Netlink              :kernel-proc-connector-and-containers

<481
process-connector

 cn_proc  Autid cn_proc   CPU
 Hook  HOOK syscall   Bug IDC 

 cn_proc  Docker  Hook  Linux 


 PID  Docker  PID  PID  /proc/ Bash 
 Linux Kernel Netlink  cn_proc 

482> 2019 
 Bash  
 Docker  Docker  
  Netlink 
 ParseNetlinkMessage   Buff    
""  Docker  50  Docker  Docker   10M   200M   Debug  ParseNetlinkMessage  PID  Golang GC  CPU  linux/ connector.h  struct cb_msglinux/cn_proc.h  struct proc_event   Golang  GC   GC  debug.FreeOSMemory
FreeOSMemory forces a garbage collection followed by an attempt to return as much memory to the operating system as possible. (Even if this is not called, the runtime gradually returns memory to the operating system in a background task.)  PID

<483
 FreeOSMemory 
  CPU 
 sync.Pool    15M 
 Golang GC   CPU 


1. etcd Client Lease Keepalive  Bug 2. Agent  Cgroup  Bug 3. Docker  4.  Nginx  TCP 
 5.  10W  fd
   
  
 

484> 2019 

  
 syscall hook  Hook  Hook  
    SRE  
 
  


1. https://en.wikipedia.org/wiki/CAP_theorem 2. https://www.consul.io/intro/vs/serf.html 3. https://golang.org/src/runtime/debug/garbage.go?h=FreeOSMemory#L99 4. https://www.ibm.com/developerworks/cn/linux/l-connector/ 5. https://www.kernel.org/doc/ 6. https://coreos.com/etcd/docs/latest/

<485

2017  

  IDC   CVE"" Black Hat   Web    IDC   / Server  /  JVM/JS V8Web " +  "    

 Web&  &  &   zhaoyan17@meituan.com https://mp.weixin.qq.com/s/ynEq5LqQ2uBcEaHCu7Tsiw
 MTSRC security.meituan.com

486> 2019 
Leaf ID 

Leaf  ID              "There are no two identical leaves in the world."Leaf   Leaf  ID Leaf  Github  https://github.com/Meituan-Dianping/Leaf 
Leaf 
Leaf 
1.  ID ID  2.  MySQL 
 3.  CentOS 4C8G  QPS  5W+
TP99  1ms  4.  RPC  HTTP 
Leaf 
Leaf                 ID     DB    N  Server Server  DB  ID List  ID  Leaf  DB  ID List ID   ID  ID 

<487
 ID  DB 


 Leaf Server 1 DB  [11000]  Leaf Server 2 DB  [10012000]  Leaf Server 3 DB  [20013000]     Round-robin      Leaf Server           Client  ID 110012001210022002...... 12100120012002200334...... Leaf Server   DB  Leaf 

+-------------+--------------+------+-----+-------------------+------

-----------------------+

| Field

| Type

| Null | Key | Default

| Extra

|

488> 2019 

+-------------+--------------+------+-----+-------------------+------

-----------------------+

| biz_tag

| varchar(128) | NO | PRI |

|

|

| max_id

| bigint(20) | NO |

| 1

|

|

| step

| int(11)

| NO |

| NULL

|

|

| desc

| varchar(256) | YES |

| NULL

|

|

| update_time | timestamp | NO |

| CURRENT_TIMESTAMP | on update

CURRENT_

TIMESTAMP |

+-------------+--------------+------+-----+-------------------+------

-----------------------+

Leaf Server  SQL 
Begin UPDATE table SET max_id=max_id+step WHERE biz_tag=xxx SELECT tag, max_id, step FROM table WHERE biz_tag=xxx Commit

V1  DB  
1.  DB  DB  
2.  DB  DB  

Leaf  Buffer 
Leaf  Buffer   DB  Buffer   DB  Buffer  Leaf 

<489
Leaf  1.  Leaf  DB  10
 10  1  2.  DB
 ID 
Leaf  Step
 QPS  Q L T Q * T = L  L  Q T  Leaf   T  L  Q T   Leaf  T  step  nextStep
 T < 15minnextStep = step * 2  15min < T < 30minnextStep = step  T > 30minnextStep = step / 2 

490> 2019   Leaf  DB   ID  DB
MySQL 
 MySQL Leaf  DB   Zebra  MHA  MySQL Group Replication
Leaf  
   
Leaf 
Leaf  Web   buffer  ID   Web 
Leaf Snowflake
SnowflakeTwitter  ID  64   Snowflake  ID 

<491
  1  0   2-42    43-52  workerID Server  ID    53-64  ID  +  +  ID  ID    Leaf    Java         Zookeeper               Zookeeper          Leaf      Zookeeper  workerID  workerID  ZooKeeper   SLA

 Leaf  MySQL  MySQL  Leaf Key  Leaf  Leaf   Leaf Key  Leaf  Shutdown  Leaf Key   MySQL  Key List 
  Leaf Key  Leaf  ID Leaf   Leaf   Leaf Key 

492> 2019 

 ID Leaf  ID 
  ID  MySQL  
 Snowflake ID 
 ID  
Leaf  Github https://github.com/Meituan-Dianping/Leaf  Github issues

<493
  OCTO 

 OCTO   OCTO  /  
 OCTO  OCTO-RPCOCTO-NSOCTO-Portal  OCTO-RPCOCTO-NSOCTO-Portal 

OCTO  2014  
 "" 
  
  
  ----OCTOOCTO Octopus  OCTO   

494> 2019 
OCTO   ""OCTO    OCTO 

OCTO     SET   
OCTO 
  /         
  
  
  

<495       QoS   
OCTO 
OCTO 
 OCTO-RPC Java/C++ RPC  JavaC++ Node.js 
 SGAgent /  
 OceanusHTTP  Oceanus HTTP 
 OCTO-NS SDKJava/C++ SGAgent NSC Scanner 

496> 2019 
  Watt  MCC  OCTO-Portal

OCTO 
OCTO OCTO-RPC OCTO-NSOCTO-Portal 
OCTO-RPC
 OCTO   90% Java/C++  Java  C++ OCTO-RPC
OCTO-NS
 /    SDKJava/C++ SGAgent  NSC Scanner : OCTO-NS
OCTO-Portal
  : OCTO-Portal

<497
OCTO-Portal 

OCTO   OCTO   99.999% SOA  OCTO  

 OCTO 
1.  AP  2. RPC 
 3. Service Mesh / 
 OCTO 

2015 

498> 2019 
2017 

 OCTO  C++/Java  Service Mesh   tech@meituan.com

<499
 OCTO2.0 

 2019 QCon  Service Mesh 
OCTO 
OCTO   OCTO  
  90%  
  /    SET 
  

  Java 80%   Java   10   
  

500> 2019   Bug   
   

 Service Mesh Service Mesh   Sidecar  Sidecar  Sidecar  Sidecar  
 Service Mesh   Sidecar  
  Sidecar  SDK  
  SDK 

<501
 Service Mesh   Sidecar   
 Mesh   Mesh  OCTO  OCTO2.0 OCTO Mesh

2.1OCTO Mesh 
 Service Mesh  2018    Mesh   Service Mesh  
 OCTO       5                   Service Mesh  
  
  N  
 

502> 2019 
 Envoy  
 Envoy  Filter  xDS 
 
  Istio  Kubernetes  Istio API 
 Istio   Kubernetes  
 Istio  OCTO  
 

2.2OCTO Mesh 

<503

 OCTO Mesh   SDK  
  SDK   OCTO Proxy Sidecar  OCTO Proxy 1
 1   OCTO Proxy  UNIX Domain Socket 
 Istio  iptables   Kubernetes  iptables  iptables  OCTO Proxy  TCP     OCTO Proxy   LEGO Agent  OCTO Proxy   Istio  Pilot Agent

504> 2019    xDS  xDS  
       Adcore       Adcore PilotAdcore Dispatcher  Mesh  Meta Server  
 Adcore Pilot  
 Adcore Dispatcher   Mesh 
  Envoy  P2P OCTO Mesh  
  
  Mesh  

<505   Istio  Kubernetes OCTO Mesh 
 Meta Server  Mesh 

 Mesh    /      Mesh    Mesh   
3.1 Mesh 
 Istio   Istio  

506> 2019         P2P 
 Adcore Pilot  Meta Server  Pilot        SessionMgrSnapshotDiplomat       SessionMgr  Snapshot  SessionMgr  Diplomat    Pilot   Sidecar  SessionMgr   OCTO Proxy  Pilot   Meta Server 

<507
Meta Server  Pilot  OCTO Proxy  Pilot  Meta Server  Meta Server   Pilot  Pilot  OCTO Proxy   OCTO Proxy   Pilot  OCTO Proxy  Pilot Meta Server 

508> 2019 
Mesh  Pilot   Zookeeper  OCTO Proxy  Pilot 
 100   100  Watcher  1000   100*1000 = 100000  Watcher 1000    
 OCTO Proxy   Snapshot  Snapshot  3 Data Cache  Node Snapshot  Ability Manager   OCTO Proxy
 1000  100  Watcher  Watcher  Data Cache  1000  OCTO Proxy  Pilot 

<509 Snapshot    Envoy-Control-Plane Envoy-Control-Plane  xDS    Meta Server   Pilot Proxy   Snapshot 
Istio  Envoy  Envoy  P2P   N  N  
 P2P  Scanner  Scanner   Pilot  Scanner Pilot  eDS  Proxy N 

510> 2019 
Google  Traffic Director    GC  Double Check 
 
3.2
OCTO Mesh  Mesh  Istio  Kubernetes   Etcd  10     Pilot  Pilot  Pilot   Pilot 
  Meta Server 

<511   Pilot  Fetch   Mafka  Adcore Dispatcher   Pilot  Pilot  Proxy  Meta ServerMeta Server  Dispatcher Pilot  Dispatcher  
3.3
Service Mesh """"   

512> 2019 
xDS   Envoy    Mock-Sidecar  Mock-Sidecar   Step  Step YAML  Step   YAML  Sidecar  YAML 
3.4

<513
 Proxy  OCTO Proxy   LEGO Proxy  LEGO   DB LEGO Agent  Poll   LEGO Agent  OCTO Proxy

4.1
    Mesh 
  &  
   OCTO Mesh  4 Meta Server  Mesh 
 Mesh  
4.2
 OCTO Mesh 
  OCTO Mesh  
  OCTO Mesh  
 

514> 2019 



Base     tech@meituan.com



<515

XGBoost 


1. 
XGBoost "" XGBoost  Spark  XGBoost on Spark XGBoost on Spark   XGBoost  Spark 
 XGBoost Java  Spark Python   Java  XGBoost  
 XGBoost  XGBoost  Java Spark   Dmlc JNI  

// 41  double[] input = new double[]{1, 2, 5, 0, 0, 6.666666666666667, 31.14, 29.28, 0, 1.303333, 2.8555, 2.37, 701, 463, 3.989, 3.85, 14400.5, 15.79, 11.45, 0.915, 7.05, 5.5, 0.023333, 0.0365, 0.0275, 0.123333, 0.4645, 0.12, 15.082, 14.48, 0, 31.8425, 29.1, 7.7325, 3, 5.88, 1.08, 0, 0, 0, 32]; //  float[]

516> 2019 
float[] testInput = new float[input.length]; for(int i = 0, total = input.length; i < total; i++){
testInput[i] = new Double(input[i]).floatValue(); } //  Booster booster = XGBoost.loadModel("${model}"); //  DMatrix41  DMatrix testMat = new DMatrix(testInput, 1, 41); //  float[][] predicts = booster.predict(testMat);
             333.67892            328.1694030761719

2. 
   6.666666666666667 
 Debug  

<517
XGBoost on Spark  XGBoostClassifier  XGBoostRegressor   API   API  JNI    
 Python   XGBoostClassifier  XGBoostRegressor  
 XGBoost on Spark   NaN-10  0   
XGBoost4j 
XGBoost4j  DMatrix  0.0f  
/** * create DMatrix from dense matrix * * @param data data values * @param nrow number of rows * @param ncol number of columns * @throws XGBoostError native error */
public DMatrix(float[] data, int nrow, int ncol) throws XGBoostError { long[] out = new long[1];
//0.0f  missing  XGBoostJNI.checkCall(XGBoostJNI.XGDMatrixCreateFromMat(data, nrow, ncol, 0.0f, out));
handle = out[0]; }
XGBoost on Spark 
 xgboost on Spark  NaN 

518> 2019 
/** * @return A tuple of the booster and the metrics used to build
training summary */
@throws(classOf[XGBoostError]) def trainDistributed(
trainingDataIn: RDD[XGBLabeledPoint], params: Map[String, Any], round: Int, nWorkers: Int, obj: ObjectiveTrait = null, eval: EvalTrait = null, useExternalMemory: Boolean = false,
//NaN  missing  missing: Float = Float.NaN,
hasGroup: Boolean = false): (Booster, Map[String, Array[Float]]) = {
//... }
 Java  DMatrix  0 
 XGBoost on Spark  NaN 
 Java  XGBoost on Spark 


       Java            NaN     
328.1694
// 41  double[] input = new double[]{1, 2, 5, 0, 0, 6.666666666666667, 31.14, 29.28, 0, 1.303333, 2.8555, 2.37, 701, 463, 3.989, 3.85, 14400.5, 15.79, 11.45, 0.915, 7.05, 5.5, 0.023333, 0.0365, 0.0275, 0.123333, 0.4645, 0.12, 15.082, 14.48, 0, 31.8425, 29.1, 7.7325, 3, 5.88, 1.08, 0, 0, 0, 32]; float[] testInput = new float[input.length]; for(int i = 0, total = input.length; i < total; i++){
testInput[i] = new Double(input[i]).floatValue(); }

<519
Booster booster = XGBoost.loadModel("${model}"); // 41  DMatrix testMat = new DMatrix(testInput, 1, 41, Float.NaN); float[][] predicts = booster.predict(testMat);
3. XGBoost on Spark 
 Spark ML SparseVector
 SparseVector  DenseVector  
 DenseVector  Vector  Vector  
  SparseVector  0   SparseVector  0  0 
 0      0 SparseVector 
SparseVector 

520> 2019 
SparseVector  0  0 
 0  Spark ML  VectorAs-
sembler  0 SparseVector 

private[feature] def assemble(vv: Any*): Vector = { val indices = ArrayBuilder.make[Int] val values = ArrayBuilder.make[Double] var cur = 0 vv.foreach {
case v: Double =>
//0  if (v != 0.0) {
indices += cur values += v } cur += 1 case vec: Vector => vec.foreachActive { case (i, v) =>
//0  if (v != 0.0) {
indices += cur + i values += v } } cur += vec.size case null => throw new SparkException("Values to assemble cannot be null.") case o => throw new SparkException(s"$o of type ${o.getClass.getName} is not supported.") } Vectors.sparse(cur, indices.result(), values.result()).compressed }
                     SparseVector 
 Spark ML  XGBoost on
Spark XGBoost on Spark  Sparse Vector  0 


<521
val instances: RDD[XGBLabeledPoint] = dataset.select( col($(featuresCol)), col($(labelCol)).cast(FloatType), baseMargin.cast(FloatType), weight.cast(FloatType)
).rdd.map { case Row(features: Vector, label: Float, baseMargin: Float, weight: Float) =>
val (indices, values) = features match {
//SparseVector  0  XGBoost  case v: SparseVector => (v.indices, v.values.map(_.toFloat))
case v: DenseVector => (null, v.values.map(_.toFloat)) } XGBLabeledPoint(label, indices, values, baseMargin = baseMargin, weight = weight) }
XGBoost on Spark  SparseVector  0 

   Spark ML   Vector                
Vector             SparseVector   DenseVector  
    Vector       Spark              
SparseVector  DenseVector
 Sparse  Dense 

/** * Returns a vector in either dense or sparse format, whichever
uses less storage. */
@Since("2.0.0") def compressed: Vector = {
val nnz = numNonzeros // A dense vector needs 8 * size + 8 bytes, while a sparse vector needs 12 * nnz + 20 bytes. if (1.5 * (nnz + 1.0) < size) {
toSparse } else {
toDense } }

522> 2019 
 XGBoost on Spark  Float.NaN   DenseVector Float.NaN  SparseVector XGBoost on Spark   SparseVector  0  Float.NaN  0
 DenseVector XGBoost  Float.NaN SparseVector  XGBoost  Float.NaN  0
 Float.NaN  0  Float.NaN   XGBoost on Spark 0  
 Serving  SparseVector   Serving 
4. 
 XGBoost on Spark   XGBoost on Spark XGBoost on Spark 
val instances: RDD[XGBLabeledPoint] = dataset.select( col($(featuresCol)), col($(labelCol)).cast(FloatType), baseMargin.cast(FloatType), weight.cast(FloatType)
).rdd.map { case Row(features: Vector, label: Float, baseMargin: Float, weight: Float) =>
//  val values = features match {
//SparseVector  Dense case v: SparseVector => v.toArray.map(_.toFloat)
case v: DenseVector => v.values.map(_.toFloat) } XGBLabeledPoint(label, null, values, baseMargin = baseMargin,

<523
weight = weight) } /** * Converts a [[Vector]] to a data point with a dummy label. * * This is needed for constructing a [[ml.dmlc.xgboost4j.scala.
DMatrix]] * for prediction. */
def asXGB: XGBLabeledPoint = v match { case v: DenseVector => XGBLabeledPoint(0.0f, null, v.values.map(_.toFloat)) case v: SparseVector =>
//SparseVector  Dense XGBLabeledPoint(0.0f, null, v.toArray.map(_.toFloat))
}


 XGBoost 




   AB     /  /   / tech@ meituan.com

524> 2019 
 Spring Boot "" 


 MDP   Spring Boot               Swap           4G       7G     JVM     "-XX:MetaspaceSize=256M -XX:MaxMetaspaceSize=256M -XX:+AlwaysPreTouch -XX:ReservedCodeCacheSize=128m -XX:InitialCodeCacheSize=128m, -Xss512k -Xmx4g -Xms4g,-XX:+UseG1GC -XX:G1HeapRegionSize=4M" 
top 

<525

1.  Java Code   unsafe.allocateMemory  DirectByteBuffer  
 -XX:NativeMemoryTracking=detailJVM   jcmd pid VM.native_memory detail 
jcmd 

526> 2019   committed  jcmd 
Code  unsafe.allocateMemory  DirectByteBuffer   Native CodeC   Native Code 
 pmap  64M   jcmd  64M 
pmap 

<527
2. 
 Native Code  Java  
 gperftools  gperftools  gperftoolsgperftools 
gperftools 
 malloc  3G   700M-800M Native Code  malloc   mmap/brk gperftools  glibc
 strace      gperftools                 "strace -f -e"brk,mmap,munmap"-p pid" OS  strace  :
strace 

528> 2019   GDB  dump   strace 
 gdp -pid pid  GDB  dump memory mem.bin startAddress endAddressdump  startAddress  endAddress  / proc/pid/smaps  strings mem.bin  dump 
gperftools 
 JAR  JAR   strace   strace
 strace   strace  64M  
strace 
 mmap  pmap 

<529
strace  pmap 
 jstack   strace  ID jstack pid   10  16 
strace 
MCC Reflec-

530> 2019  tions  Spring Boot  JAR JAR  Inflater  Btrace 
btrace 
 MCC  
3. 
       64MJAR    gperftools  700M 
 malloc 

<531
            Spring Boot Loader         Spring Boot  Java JDK  InflaterInputStream  Inflater  Inflater  JAR  ZipInflaterInputStream  Inflater  Spring Boot  bug Inflater   finalize  Spring Boot  GC 
 jmap  Inflater   GC  finalize Inflater   Spring Boot Loader  Inflater finalize   finalize  Inflater  C   malloc end  free 
 free  Spring Boot  InflaterInputStream  Java JDK  
 gperftools  Spring Boot   3G  700M  GC  
glibc 2.12  gperftools  2.5G  smaps  Native Stack 

532> 2019 
gperftools 
 glibc 64M glibc  2.11 64  64M 
glib 
 MALLOC_ARENA_MAX   tcmallocgperftools 
  gcc zjbmalloc.c -fPIC -shared -o zjbmalloc.so  export LD_PRELOAD=zjbmalloc.so  glibc  Demo 

<533

#include<sys/mman.h>

#include<stdlib.h>

#include<string.h>

#include<stdio.h>

//  64 sizeof(size_t)  sizeof(long)

void* malloc ( size_t size )

{

long* ptr = mmap( 0, size + sizeof(long), PROT_READ | PROT_WRITE,

MAP_PRIVATE | MAP_

ANONYMOUS, 0, 0 );

if (ptr == MAP_FAILED) {

return NULL;

}

*ptr = size;

// First 8 bytes contain length.

return (void*)(&ptr[1]);

// Memory that is after length

variable

}

void *calloc(size_t n, size_t size) {

void* ptr = malloc(n * size);

if (ptr == NULL) {

return NULL;

}

memset(ptr, 0, n * size);

return ptr;

}

void *realloc(void *ptr, size_t size)

{

if (size == 0) {

free(ptr);

return NULL;

}

if (ptr == NULL) {

return malloc(size);

}

long *plen = (long*)ptr;

plen--;

// Reach top of memory

long len = *plen;

if (size <= len) {

return ptr;

}

void* rptr = malloc(size);

if (rptr == NULL) {

free(ptr);

return NULL;

}

rptr = memcpy(rptr, ptr, len);

free(ptr);

return rptr;

534> 2019 

}

void free (void* ptr )

{

if (ptr == NULL) {

return;

}

long *plen = (long*)ptr;

plen--;

// Reach top of memory

long len = *plen;

// Read length

munmap((void*)plen, len + sizeof(long));

}

  700M-800M gperftools  700M-800M 



 malloc  800M 1.7G   mmap mmap   536k  512k * 4kpagesize= 2G   1.7G   mmap    Page



<535


MCC  JAR Spring Boot   GC Spring Boot  finalize  glibc  ""  MCC  JAR  Spring Boot     2.0.5.RELEASE       ZipInflaterInputStream  GC Spring Boot  

1. GNU C Library (glibc) 2. Native Memory Tracking 3. Spring Boot 4. gperftools 5. Btrace

2015  C 

536> 2019 



   AB  1 AB  A  B   Google  2000  AB   2011 Google  7,000  AB ""  AB 
 1 AB 

<537
  AB   AB    
Wedge 


Wedge 
         

Overlapping Experiment Infrastructure: More, Better, Faster ExperimentationGoogle   LBS  

 Google  

538> 2019 
  Overlapping Layer "" "" "" H1  H2  H1  H2
  Non-overlapping Layer "" "" CTR""  V1  V2 V1  V2
 "" 
       
  

 2 
 Web  UI 
      SDK 

<539
 2

1.  
 3   App App 
  Scene
  Layer matching 
 Matching Layer  Layer 

540> 2019   Layer 
  Layer  Scene   A   3  Layer_3  Layer_4   Layer Layer  
 Exp
 3
  4    Layer
 4  Layer_1 10   334    Layer 

<541  4  Layer_2  Layer_3 4   Exp_6  Exp_7
 4
2. 
  Layer 


542> 2019   Hash  Hash   Hash   Hash  uuid dpid   Hash Hash  +scene_id   Hash Hash  +scene_id+layer_id+layer_name   Hash  Bucket 100  Bucket  Bucket   Bucket Bucket  1%   5 50%  Exp_1Exp_2  20%  10% 

 5
 6    Hash    Hash  Hash  hashNum
 50  99 V_Exp_1V_Exp_2     50  99 Hash  Hash  hashNum Exp_1Exp_2  

<543
 6
  7     
 7

544> 2019 
  >  > 
3. 
                               table 

1.  
2.  



 8

<545
AB 
 CTR CTR PV RPS 
PVSPVServer PVCPVClient PV CPV  requestId   requestId  join  request  Durid 
 join  OLAP  Druid Druid   OLAP Druid    

 Wedge 

546> 2019 
     
 . 

 . 
  Java  C++ 
tech@meituan.com 

<547
  


      Bug  ""  
   
 
1.  
2.  

548> 2019 

 Generalized Alarms  1 
1
 

 2  
2

<549

"Clustering Intrusion Detection Alarms to Support Root Cause Analysis [KLAUS JULISCH, 2002]"  
server_room_a-biz_tag-online02 Thrift get deal ProductType deal error. server_room_b-biz_tag-offline01 Pigeon query deal info error. server_room_a-biz_tag-offline01 Http query deal info error. server_room_a-biz_tag-online01 Thrift query deal info error. server_room_b-biz_tag-offline02 Thrift get deal ProductType deal error.
"  " "server_room_a   " "server_room_b  RPC " 
 
DAG 3 
 3
  Attribute
 Ai 

550> 2019 
 Domain Ai  Dom(Ai)   Generalization Hierarchy Ai 
 Gi   Dissimilarity d(a1, a2) a1a2 
 d(a1, a2)  a1  a2  
 d(a1, a2) x1x2   Ai  x1x2  Gi   p  x1x2  d(x1, x2) := min{d(x1, p) + d(x2, p) | p  Gi, x1 p, x2 p} 3 d("Thrift", "Pigeon") = d("RPC","Thrift") + d("RPC","Pigeon") = 1 + 1 = 2
 a1a2
 1
a1 = ("server_room_b-biz_tag-offline02","Thrift"), a2 = ("server_room_a-biz_tag-online01","Pigeon"),  d(a1, a2) = d("server_room_ b-biz_tag-offline02","server_room_a-biz_tag-online01") + d(("Thrift", "Pigeon") = d("server_room_b-biz_tag-offline02","") + d("server_ room_a-biz_tag-online01","   ") + d("RPC","Thrift") + d("RPC", "Pigeon") = 2 + 2 + 1 + 1 = 6
 C g  C   a  C, a g      {"dx-trip-package-api02 Thrift get deal list error.","dx-trippackage-api01 Thrift get deal list error."} "dx  thrift   ""  "

<551
 
 2
H©  g g  C  ""(Cover)
 L min_ size Gi(i = 1, 2, 3......n)  Ai   L  C |C| >= min_size H© min_size   min_size  L   min_size = 1  L 
 NP  |C| >= min_size H© 

1.  Gi  
2.  L  Ai L   Ai  Gi  Ai 
3.  2  min_size  
4.  3 

552> 2019 



 Lmin_size G1,......,Gn



T := L;

//  T

for all alarms a in T do

a[count] := 1; // "count"  a 

while a  T : a[count] < min_size do {

 Ai;

for all alarms a in T do

a[Ai] := parent of a[Ai] in Gi;

while identical alarms a, a' exist do

Set a[count] := a[count] + a'[count];

delete a' from T;

}

 7  :

 Ai  Fi
fi(v) := SELECT sum(count) FROM T WHERE Ai = v  Fi := max{fi(v) | v  Dom(Ai)}  Fi  Ai

//  Ai  v 

 a  a[count]>= min_size  Ai ,  Fi >= fi(a[Ai]) >= min_size Ai  Fi  min_size a[count]  min_size Fi   Ai 
 min_size  min_size  
 ms0 (0 <  < 1) min_size  ms0ms0 * (1 - )ms0 * (1 + )   -  ms1 = ms0 * (1 - ) 
- 

<553


1. 
 Case  ID ID 
 Case  ID  
  5 
2. 
1              "code=......,message=......"   Case 

554> 2019   ""
2  4 
 4

<555 3min_size 
min_size = 1/5 *   min_size = 1/5 *   0.05
4  min_size   20   count  min_size  3.    
 5

556> 2019 
 6
 7
 8
"" 

 C  API 
1. 


     939 

<557

 9 

 9

558> 2019 

 1 

ID

Server Room

Error EnvironSource ment

Position  


1   Prod

com.*.*.*.CommonProductQueryClient

Summary   Count
 com.netflix.hystrix. exception.HystrixTimeoutException: commonQueryClient. 249 getProductType execution timeout after waiting for 150ms.

2   Prod

com.*.*.*.PluginRegistry.lambda

java.lang.IllegalArgumentException: 
240  :  

3   Prod

com.*.*.*.TrProductQueryClient

com.netflix.hystrix. exception.HystrixTimeoutException: TrQueryClient.listTr- 145 ByDids2C execution timeout after waiting for 1000ms.



(  /

4 

Prod

 / 

 )

com.*.*.*.RemoteDealServiceImpl

com.netflix.hystrix.

exception.Hystrix-

TimeoutException:

ScenicDealList.

89

listDealsByScenic

execution timeout af-

ter waiting for 300ms.

5   Prod

com.*.*.*.CommonProductQueryClient

com.netflix.hystrix. exception.HystrixTimeoutException: commonQueryClient. 29 listTrByDids2C execution timeout after waiting for 1000ms.

6   Prod

com.*.*.*.ActivityQueryClientImpl

com.netflix.hystrix. exception.HystrixTimeoutException: commonQueryClient. 21 getBusinessLicense execution timeout after waiting for 100ms.

<559

Position 

Server Error Environ-

ID



Room Source ment



7   prod

com.*.*.*.CommonProductQueryClient



(  /

8 

Prod

 / 

 )

com.*.*.*.RemoteDealServiceImpl

9   Prod 10   Prod

com.*.*.*.TrProductQueryClient com.*.*.*.TrProductQueryClient

Summary   Count
 com.netflix.hystrix. exception.HystrixTimeoutException: commonQueryClient. 21 getBusinessLicense execution timeout after waiting for 100ms.
com.netflix.hystrix. exception.HystrixTimeoutException: HotelDealList. 17 hotelShelf execution timeout after waiting for 500ms.
Caused by: java.lang. 16
InterruptedException
Caused by: java.lang. 13
InterruptedException

 Count  

2. 
 Staging  A/B  

 Staging   A/B   527 

 10 

560> 2019 
 10

<561

 2 

Position 

Server Error Envi-

Summary 

ID



Count

Room Source ronment





com.*.*.*.Activi- [hystrix]  , circuit

1   Staging

291

tyQueryClientImpl short is open

com.*.*.*.AbEx- [hystrix] tripExperiment

2  A/B  Staging

105

perimentClient error, circuit short is open

3  

com.netflix.hystrix.ex-

ception.HystrixTimeout-

com.*.*.*.Cache- Exception: c-cache-rpc.

Staging

15

ClientFacade

common_deal_base.rpc

execution timeout after

waiting for 1000ms.

com.*.*.*.que4   Staging
ryDealModel

Caused by: com.meituan. service.mobile.mtthrift. netty.exception.Request- 14 TimeoutException: request timeout

com.netflix.hystrix.

exception.HystrixTimeout-

com.*.*.*.Com-

Exception: commonQuery-

5   Staging monProductQue-

9

Client.getBusinessLicense

ryClient

execution timeout after

waiting for 100ms.

com.*.*.*.getOr- java.lang.IllegalArgument-

6   Staging

7

derForm

Exception: 

com.*.*.*.Pre7   Staging
SaleChatClient

com.netflix.hystrix.exception.HystrixTimeoutException: CustomerService.Pre- 7 SaleChat execution timeout after waiting for 50ms.

8  

Staging

Caused by: java.net.Sockcom.*.*.*.Spring-
etTimeoutException: Read 7 CacheManager
timed out

com.*.*.*.que9   Staging
ryDetailUrlVO

java.lang.IllegalArgument2
Exception: 

com.*.*.*.que10   Staging
ryDetailUrlVO

java.lang.IllegalArgument1
Exception: 

 

562> 2019 
3. 
 Staging  
 Staging    2165  11 
 11

<563

 3 

ID

Server Room

Error Source

Environment

Position  

Summary   


Count

1  Squirrel Staging com.*.*.*.cache

Timeout

491

2  Cellar Staging com.*.*.*.cache

Timeout

285

3  Squirrel Staging com.*.*.*.TdcServiceImpl

Other Exception 149

4  

Staging com.*.*.*.cache

Timeout

147

5  Cellar Staging com.*.*.*.TdcServiceImpl

Other Exception 143

6  Squirrel Staging com.*.*.*.PoiManagerImpl 

112

com.*.*.*.CommonPro7   Staging
ductQueryClient

Other Exception 89

8  

Staging com.*.*.*.TrDealProcessor Other Exception 83

9  

Staging com.*.*.*.poi.PoiInfoImpl

Other Exception 82

10   Staging com.*.*.*.client

Timeout

74

Squirrel  Cellar    
   Other Exception
 
 


 

564> 2019 
1.   
2.   K-Means
3.  

1. Julisch, Klaus."Clustering intrusion detection alarms to support root cause analysis."ACM transactions on information and system security (TISSEC) 6.4 (2003): 443-471.
2. https://en.wikipedia.org/wiki/Cluster_analysis

2017  2017 




<565


  2018   
    
  1 

 1

566> 2019  
 2  
     
 2
Quake   
  
   Mock 
  
  /   

<567       


 3
 3  /  /    
 /   Mock   Mock   Mafka/Cellar/Squirrel/Zebra 

568> 2019 
 
  ""
  / 
 
 
     
    SSO    
    /  /  
   Quake  Quake




<569

 4
 
 Mock   4    
 5 Mock 

570> 2019   Mock
 Mock  Mock  4  C
SDK  Mock  5  Mock  JVM-Sandbox  AOP  Mock  Mock                                Mock  Mock  0  Mock  Mockserver   SDK  Mock  Mock   Mock  Mock 

 6
   Java   Java  Java 

<571  RPC  
                              Quake   6   Quake   Quake 
 Quake 

 7
     

572> 2019      Zebra   Cellar/Squirrel   Mafka    Mock  
 MTrace  4 Mock   Mock 
 MTrace  
 8MTrace 
  Mock 

 Mock 

<573

 9 Mock 
  
 10
  
 /  

574> 2019  
 QPS  

  11  QPS    QPS   
 11
 11    /   



 / 
 

<575


    Mock ""

576> 2019 

 "" 7  

Quake 
  Quake   Quake """"

<577

  
 

578> 2019 

 
 
 

<579

  ""   

[1] Quake [2]  JVM-Sandbox [3] Dubbo  [4] Java 

2013 

580> 2019 



    Tcl      John Ousterhout   A Philosophy of Software Design[1] IT  """" "" """ """ A Philosophy of Software Design 
       "     "                  John Ousterhout "" 

  (  ) [2]  
John Ousterhout  

<581
 cp  tp  C  (  tp 
 cp 
     Unknown Unknowns
 (Obscurity)  

    
     
 

582> 2019   
   Unknown Unknowns

4.1
 Bug   
 
  bug "" 

<583
 10-15%   
4.2
        "" ""  

5.1
   TCP/IP 

584> 2019 
TCP/IP   IP  
5.2
  Thrift  Thrift  (   (  XML   (  )  
 

<585
5.3
  
   

  
6.1
 (Deep Module)  

586> 2019 
Unix  I/O  Open   
int open(const char* path, int flags, mode_t permissions);
 (Shallow Module)   Java I/O  I/O  FileInputStreamBufferedInputStreamObjectInputStream  BufferedInputStream I/O   I/O  I/O  
FileInputStream fileStream = new FileInputStream(fileName); BufferedInputStream bufferedStream = new BufferedInputStream(fileStream); ObjectInputStream objectStream = new ObjectInputStream(bufferedStream);
   
6.2
   
 

<587

void backspace(Cursor cursor); void delete(Cursor cursor); void deleteSelection(Selection selection); //  void delete(Position start, Position end);
 
  
  
  
6.3
  "" "" 
   B+    SQL  

588> 2019 
  [3]  
6.4
"" 
       
 

  (Unkown Unkowns)   
  
7.1
 """"" """

<589
   "" ""
7.2
 What  Why  (How)
     
   
   
   
 

590> 2019 
 
  
7.3
     

John Ousterhout  25  3    
   

 John Ousterhout. A Philosophy of Software Design. Yaknyam Press, 2018.  · .  .  , 2016.  Martin Fowler. Refactoring: Improving the Design of Existing Code (2nd Edition) .
Addison-Wesley Signature Series, 2018.

<591



 /   ,   tech@meituan.com


   AI   AI   /  App    ......  

 BERT 


<593

2018 Natural Language ProcessingNLP  RNN  ELMo[1]  ULMFiT[2] Transformer[3]  OpenAI GPT[4]  Google BERT[5]  1    NLP   NLP  [6] Pre-training Fine-tuning NLP  

 1NLP Pre-training and Fine-tuning 
"""Pre-training and Fine-tuning"2009  CVPR 2009   ImageNet  [7] 120  1000  ImageNet  ResNetVCGInception 

594> 2019 
    ImageNet  PSACAL VOC  20%[8]
 NLP  Word Embedding NLP   NLP    NLP    NLP   Word2Vec[9]  GloVe[10]  
                               "bank""" "" Context2Vec[11] Long Short Term MemoryLSTM[12]            Left-to-Right   Right-to-LeftELMo   LSTM ELMo   Embedding  NLP  ELMo  Feature-based 
Fine-tuningGPTBERT   Transformer   Task-specific 

<595
NLP  [13] Google AI  BERTBidirectional Encoder Rep-
resentations from Transformers 11   NLP BERT  NLP   NAACL 2019 BERT   NLP  NLP  NLP  ImageNet    [14]
   40  UGC UGC   NLP    MT-BERT  MT-BERT  
BERT    Transformer                  2         Transformer             Encoder   Transformer  Google  2017 Self-attention  NLP  RNN  Transformer  RNN  State-Of-The-ArtSOTA  Transformer  Transformer   Google Attention is all you need[3]

596> 2019 

 2BERT  Transformer 


 1 Google  Base  Large  BERT 

1BERT BaseLarge

 Base Large

Layers 12 24

Hidden Size 768 1024

Attention Head 12 16

 110M 340M


BERT   TokenToken EmbeddingSegment EmbeddingPosition Embedding 3 

<597
 3BERT 
  Wordpiece  Subword  
  Token [CLS] Hidden State  Transformer    [SEP]   Segment Embeddings  Token  Embedding    Segment Embedding Segment Embedding

BERT  Masked Language Model  Next Sentence Prediction 
Masked Language ModelMLM
 [MASK] 
1 [MASK] 2 Batch  15%  

598> 2019 
 80%  [MASK] 10%  10%  Transformer Encoder   
Next Sentence PredictionNSP
  A  B  50%   B  A 50%  B NSP  B  A NSP  
Google   [15]  NSP   NSP  1-2  Epoch   98%-99% NSP 
 & 
Google  BERT  BooksCorpus800M  Wikipedia2500M BERT  Google AI  Cloud TPU  BERT BERT Base  Large  4  Cloud TPU16  TPU 16  Cloud TPU 64  TPU 4 100 40  Epoch  Nvidia  GPU  BERT  GPU 
 BERTMT-BERT1 2 34 MT-BERT  4 

<599
 4MT-BERT 
 AFO 
BERT   AFO[16]AI Framework On Yarn MT-BERT AFO   YARN  GPU  Horovod  Horovod  Uber    [17]  Facebook ImageNet [18]  Ring Allreduce[19]  Uber   TensorFlow  Horovod  Inception V3  ResNet-101 TensorFlow  GPU Horovod   TensorFlow TensorFlow   Tensorflow Horovod  
Horovod  Open

600> 2019 
MPI  Nvidia NCCL Rank  Step  Horovod      Rank 0  Master  Rank1-n  Worker     Worker  Master  Map AllreduceHorovod   MPI Woker   Tensor  Master MPI  MPI Master  Worker  Map   Tensor  n   Tensor  Tensor Master   Tensor  MPI  Tensor   Map  Tensor Master  Tensor  Allreduce 

Float 32Double  Batch Size Batch Size  Baidu Research  Nvidia  ICLR 2018    [20]        Float32FP32 Float16 FP16 Nvidia  Pascal  Volta   Tesla V100  FP16  P4  P40  INT8  

<601  MT-BERT   FP32  FP16   FP32  FP16   FP16  FP32 FP32 Master-weights FP32  FP32 Master-weights  FP32   FP16  FP16   Loss Scaling  Loss   FP16   MT-BERT 

 5MT-BERT  Nvidia V100  Tensorflow 1.12Cuda 10.0Horovod 0.15.2

602> 2019 

 5    Benchmark  2  3

2MT-BERTBenchmark



Metric



Macro-F1

Query 

F1

Query NER F1

MT-BERT FP32
72.04% 93.27% 91.46%

MT-BERT  Google BERT


72.25%

71.63%

93.13%

92.68%

91.05%

90.66%

3MT-BERTBenchmark

 MSRA-NER LCQMC ChnSentiCorp NLPCC-DBQA XNLI

Metric F1 Accuracy Accuracy MRR Accuracy

MT-BERT FP32 MT-BERT 

95.89%

95.75%

86.74%

85.87%

95.00%

94.92%

94.07%

93.24%

78.10%

76.57%

Google BERT 95.76% 86.06% 92.25% 93.55% 77.47%

 2  3  MT-BERT   2 


Google  BERT   UGC   Google  BERT Domain Adaptation  Domain-aware Continual Training  BERT  Google   BERT Large  MT-BERT Large 

<603

 5  Benchmark  3  Benchmark   8  4 MT-BERT  Benchmark  Benchmark 

4MT-BERTGoogle BERT8Benchmark

Benchmark MSRA-NER LCQMC ChnSentiCorp NLPCC-DBQA XNLI  Query  Query NER

Metric F1 Accuracy Accuracy MRR Accuracy Macro-F1 F1 F1

Google BERT 95.76% 86.06% 92.25% 93.55% 77.47% 71.63% 92.68% 90.66%

MT-BERT 95.89% 86.74% 95.00% 94.07% 78.10% 72.04% 93.27% 91.46%


BERT  Common Sense BERT     Query  """" Query BERT   BERT  
 MT-BERT    BERT   [21]  NLP 

604> 2019  ---- Knowledge-aware Masking ""  MT-BERT 
BERT  BERT  Masked LMMLM "" ""  ""
 6  BERT  MLM " """""""3   3 
 6MT-BERT  Masking  Whole Word Masking 
BERT " X """ ""  Knowledge-aware Masking  MT-
BERT"" ""  6  Knowledge-aware Masking " """MT-BERT """" ""MT-BERT ""

<605

  MT-BERT  Knowledge-aware Masking 

5MT-BERT

 BERT (Vanilla masking) MT-BERT (Knowledge-aware Masking)

Macro-F1 72.04% 72.48%


BERT   Query   MT-BERT  1000QPS  30  GPU  TP999  50ms 

 FP16  INT8 FP32
    [22]  BERT 

 Query   Query  16  Sequence   Transformer   MTBERT  4  Transfomer MT-BERT-MINIMBM  7 Query  MBM 

606> 2019  
 7 MT-BERT  Query  F1 
MBM  TP999  12-14ms   6  MT-BERT  BERT  BERT  Google  ALBERT A Lite BERT[23]  GLUE  SOTA
 8  BERT  
1.  NSP  BERT "[CLS]" 
2.  MLM 

<607
Token  Token   BERT  Token  
 8BERT 
 MT-BERT  


     NLP  6  20   AI Challenger 2018  
 MT-BERT   9  Share LayersTask-specific Layers  MT-BERT MTBERT 

608> 2019    Attention+Softmax   MT-BERT   Macro-F1 
 9 MT-BERT 
 10   App   UGC    POI  

<609
 10
Query 
Deep Query UnderstandingDQU  Query  MT-BERT  Query   Inference  4  MT-BERT  MT-BERT-MINIMBM  Query   11 
 11MBM 

610> 2019 
 Query  QPS MBM  17   Query   95% MBM  Query  Bad Case 

 UGC  POI    ""     46  
   
  MT-BERT  8(b) 

<611
 12

 Natural Language Inference, NLISemantic Textual SimilaritySTS
Query  Query   """"" """""""Query  

612> 2019 
 Query  Query   Query Query  STS   MT-BERT  Query   8(a)  Query  Query "[CLS] text_a [SEP] text_b [SEP]" MT-BERT "[CLS]" Query   MT-BERT  Benchmark   XGBoost 

 NLP   Named Entity RecognitionNER  
NER  Query UGC   / NLP   MT-BERT  Query   Query  Query  
 Query "BME"  B E  M 13  Query  Query" """"" POI MT-BERT  Query  

<613
 13 Query 
 MT-BERT 
 MT-BERT  MT-BERT   
 MT-BERT   AFO  GPU  
 MT-BERT 
 BERT    MT-BERT  
MT-BERT 
MT-BERT  NLU    

614> 2019 


[1] Peters, Matthew E., et al."Deep contextualized word representations."arXiv preprint arXiv:1802.05365 (2018).
[2] Howard, Jeremy, and Sebastian Ruder."Universal language model fine-tuning for text classification."arXiv preprint arXiv:1801.06146 (2018).
[3] Vaswani, Ashish, et al."Attention is all you need."Advances in neural information processing systems. 2017.
[4] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving Language Understanding by Generative Pre-Training. Technical report, OpenAI.
[5] Devlin, Jacob, et al."Bert: Pre-training of deep bidirectional transformers for language understanding."arXiv preprint arXiv:1810.04805 (2018).
[6] Ming Zhou."The Bright Future of ACL/NLP."Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. (2019).
[7] Deng, Jia, et al."Imagenet: A large-scale hierarchical image database."2009 IEEE conference on computer vision and pattern recognition. Ieee, (2009).
[8] Girshick, Ross, et al."Rich feature hierarchies for accurate object detection and semantic segmentation."Proceedings of the IEEE conference on computer vision and pattern recognition. 2014.
[9] Mikolov, Tomas, et al."Distributed representations of words and phrases and their compositionality."Advances in neural information processing systems. 2013.
[10] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In EMNLP.
[11] Oren Melamud, Jacob Goldberger, and Ido Dagan.2016. context2vec: Learning generic context embedding with bidirectional lstm. In CoNLL.
[12] Hochreiter, Sepp, and Jürgen Schmidhuber."Long short-term memory."Neural computation 9.8 (1997): 1735-1780.
[13]  .  Word Embedding  BERT -- . https://zhuanlan.zhihu.com/p/49271699
[14] Sebastion Ruder."NLP's ImageNet moment has arrived."http://ruder.io/nlpimagenet/. (2019)
[15] Liu, Yinhan, et al."Roberta: A robustly optimized BERT pretraining approach." arXiv preprint arXiv:1907.11692 (2019).
[16]   .   TensorFlow   WDL            . https://tech.meituan. com/2018/04/08/tensorflow-performance-bottleneck-analysis-on-hadoop.html
[17] Uber." Meet Horovod: Uber's Open Source Distributed Deep Learning Framework for TensorFlow". https://eng.uber.com/horovod/
[18] Goyal, Priya, et al."Accurate, large minibatch sgd: Training imagenet in 1 hour." arXiv preprint arXiv:1706.02677 (2017).

<615
[19] Baidu. https://github.com/baidu-research/baidu-allreduce [20] Micikevicius, Paulius, et al."Mixed precision training."arXiv preprint
arXiv:1710.03740 (2017). [21]      .           ----       . https://tech.meituan.
com/2018/11/22/meituan-brain-nlp-01.html [22] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean."Distilling the knowledge in a neural
network."arXiv preprint arXiv:1503.02531 (2015). [23] Google. ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations. https://openreview.net/pdf?id=H1eA7AEtvS. (2019)

  NLP 

 NLP  NLPDeep LearningKnowledge Graph    NLP Service  AI NLP     NLP NLP   tech@meituan.com  +  NLP 

616> 2019 

  2018 QCon  

2018  12  31  200    O2O   NLP   

"Eat BetterLive Better" 

<617 "" App   "" 
   

 Google  O2OOnline To Offline  

618> 2019 
 5      
    "" 
 O2O Online Offline   

<619
  DCGNDCGMAP    GMV
 O2O  """" "" "" "" "" 
   

620> 2019 
   "" "" 
 """" "" 
""    
 



<621


  API   
            API

622> 2019  
 
 Case  
 """ "
  7 ""   Term 
 "" 
 """" 

<623
  
  
      

624> 2019 
 O2O  ""   "" 
  """"   

<625
  
  NERNamed Entity Recognition
""  
     NER  NER   NER 

626> 2019 
NER " """""   
 CRFConditional Random Field   
" +  + " " + " Term  CRF    CRF  NER  

<627
          Word Embedding              NLP  LSTMLong ShortTerm Memory+CRF  NER  LSTM  Embedding  
 LSTM+CRF CRF   LSTM+CRF  NER 
 LSTM+CRF  NER   CNN+LSTM+CRF CNN  CNN  Embedding  Embedding  LSTM 

628> 2019 
 4  
  
     
   NER   TermWeight  
 "" 




<629

  Google   App  LR/FTRLFM/ FFMGBDTDNN 
                               LBS     LR/FTRLFM/ FFMGBDTDNN 

 4 
1.  
2. 

630> 2019   App
3.      
4.   
   

<631
  App App   App 
  Cover    
  
  
   App      App 

632> 2019    

   LR/FTRL  FM/FFM GBDT+LR   Wide&Deep
    

<633
 XGB(XGBoost, eXtreme Gradient Boosting)  GBDT XGB    
XGB      
  MLP(Multiple-Layer Perception)  """"  """"  

634> 2019  
 MLP  MLP  XGB XGB  MLP  MLP  XGB ID   MLP  ID  IDID   ID ID 
 MLP 

<635  "1024512-256" 3-6    3  6 
MLP   
                             Embedding Wide&Deep Wide   Embedding  FNN  Embedding  FM  K  
 FM   Embedding  Embedding   FM  FM

636> 2019   Embedding  V3 
FNN  Embedding   DeepFMDeepFM  Wide&Deep  FM  LR  LR  FM  DeepFM  FM  Wide&Deep   LR Embedding "" FM Embedding Embedding  FM Layer  DeepFM 
 DeepFM  Embedding   PNNPNN  Product   And"" Add"" 

<637
PNN  Product Layer  PNN   EmbeddingEmbedding  Product Product    

638> 2019 
PNN       Embedding               DCN Deep&Cross NetworkDCN  Cross Network   Deep&Cross Deep   Cross  Stack 
Deep  Embedding Cross  DCN  Cross   Cross  x  Feature Crossing  x0 x   w x1  x2  Cross  
DCN  
 DeepFMPNNDCN   ?  
XGB   600  400  ;  Embedding  
 Wide&DeepWide&Deep  Wide   Deep Wide  Deep  Item  Deep  LR  Deep  V3  ID    V4 

<639
 MTL    ? " ->   ->  ->  -> " 5  
 1 =  ×   ×  ×  4 

640> 2019    4  2 "End to End"    AB 
"End to End" CTR   CVR  Embedding   Wide&Deep 



<641

 MLP   Embedding  FNN DeepFMPNNDCN   Wide  Wide&Deep MTL  Wide&Deep 
 :

642> 2019                   FNNDeepFMPNNDCN
Wide&Deep  

 BP Basis Point1BP=0.01% XGB  BaselineMLP             XGBMLP  XGB                        FNN        Wide&Deep   Embedding  DeepFMDeep&Cross  

<643
  
 Embedding  FNN  Wide&Deep  
                          Sigmoid

644> 2019  ReLULeaky_ReLUELU         AdagradRmspropAdam            ReLU+Adam           Batch Normalization  Dropout   3  6  
 Serving   KerasTensorFlow  
 TensorFlow TF-Serving  MLX  MLX   Serving  

<645
 
   ""
    2017 
   
  ""  XGB

646> 2019 
  
   ""   

 O2O  ""  NER     

[1] John Lafferty et al. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.ICML2001.
[2] Guillaume Lample et al Neural architectures for named entity recognition. NAACL2016.
[3] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. [4] Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint
arXiv:1508.01991. [5] Xuezhe Ma et al.End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-
CRF.ACL2016.

<647
[6] T Chen, C Guestrin. XGBoost: A scalable tree boosting system. KDD2016. [7] Weinan Zhang et al. Deep Learning over Multi-Field Categorical Data: A Case
Study on User Response Prediction. ECIR 2016. [8] Huifeng Guo et al. DeepFM: A Factorization-Machine based Neural Network for
CTR Prediction. IJCAI2017. [9] Yanru Qu et al. Product-based neural networks for user response prediction.
ICDM2016. [10] Heng-Tze Cheng et al. 2016. Wide & deep learning for recommender systems.
2016.In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. [11] Ruoxi Wang et al. Deep & Cross Network for Ad Click Predictions. ADKDD2017.

2016 2010   "Kaggle "QCon 

648> 2019 


1. 

 App    
1.  POI UGC   
2.   
3.  "" 
4. LBS LBS   
                            

<649
 NLPNatural Language Processing  NLP    KPI  

 NLP ----   NLP "" " "[1]  1  5 " "---- ) 5  3 
1.  2.  3.  Listwise ----LambdaDNN

650> 2019 
 1 5 
2. 
Learning to RankL2R 
1.          Loss Function   L2R     Pointwise PairwiseListwise
2.  GBDT+LRDeep&Wide 
                                LR FM  FFM GBDT  GBDT+LR LRFMGBDT 



<651

 2
1. LR    LR  
2. FM  LR  FM 
3. GBDT  Boosting   GBDT  

   2018  L2  
1. 

652> 2019   
2.   ID   Embedding  
3.  DeepFM Google  DeepCrossNetwork 
       Google    Wide&Deep          [2]   Wide  LRGBDT  Deep  Low-Order Categorical    End-to-End  
 3Deep&Wide 

<653
3. 
   CV CV  ----  NLP  Transformer BERT  Transformer  NLP Task  State-of-The-Art 
 CTR    
3.1
   
    Embedding        Label  
  

654> 2019 
  ""
  
3.2 Embedding
  UGC   Embedding  Embedding  Embedding  
3.2.1 Embedding  ""  
 Pooling Embedding  Sum/Average Pooling  
 RNNLSTM/GRU  
 Attention   Embedding    Attention         Sum Pooling LSTM/GRU  [4]

<655
 4
 Session 
3.2.2 ID  Embedding  ID  Embedding    ID  ID  Embedding   Airbnb  KDD 2018  [9]----  ID Airbnb   1-2  Airbnb  

656> 2019 
 5
  ID US_lt1_pn3_pg3_r3_5s4_c2_b1_bd2_bt2_nu3
 Airbnb     
3.2.3 Embedding  Embedding  ID   UGC  """"""

<657
 6
  Embedding 
  Pooling   Top 
  /  /  Top N  Pooling  

658> 2019 
          DSSM                 Embedding  
3.2.4 Embedding   Embedding  Query Shop  Embedding   Embedding  
  
  Embedding     Word2vecFasttext  -   Embedding   DSSM  Query-  Query   Embedding
 Multi-Task Embedding   Embedding   Embedding 

<659
 7Multi-Task  Embedding 
3.3
   
  
  ResNet50  [3] 
  Embedding 
 Logo  
 8

660> 2019 
4.  Listwise LambdaDNN
4.1 Gap
 Gap    Pointwise  Log Loss   Gap
1.  QV_CTR  SSR(Session Success Rate)  Pointwise  Log Loss  Item 
2.  Pointwise  
 9Pointwise  Listwise 

<661

4.2 Log Loss  NDCG
 Query   NDCG(Normalized Discounted Cumulative Gain)    Log Loss             NDCG 
 DCG(Discounted Cumulative Gain)   Query  l G  Doc   G(lj)=2lj-1lj  {012}    (j)=1/log(j+1)Doc  Query  DCG   k Zk  DCG@k   NDCG@k
 NDCG  LambdaRank   NDCG  [6]  NDCG   Lambda  LambdaDNN
   Lambda       LambdaRankLambdaRank      Pairwise  Query  Pair Pij  Query  Doci  Docj   si  sj  Doci  Docj 

662> 2019 
 Sij  Pair  Doci  Docj   Doci  Docj  Sij=1 -1 
 Pair  i  Sij  1 
  Doci  Docj  NDCG  Lambda   NDCG 
Lambda   Lambda  Doc   Lambda  Query  Doc  Doc  Lambda   NDCG 

<663
 10Lambda 
4.3LambdaDNN 
    TensorFlow        LambdaDNN        Lambda  Query   Shuffle  Worker 
1.  QueryId  Shuffle Query  Query  TFRecord
2.  Query  Doc  Size  Query  TF  Mini-Batch     MR  Key  Query    

664> 2019 
 11Lambda 
 
1.  ID  Training  
2.     TfRecord   RecordDataSet            Worker  10 
3. Concat   Categorical      Multi-Hot  Tensor     Embedding_Lookup  Map  
4.  Tensor  
5.  PS  Tensor  Worker  
 30  

<665
4.4
NDCG   NDCG 
   NDCG 
1.   NDCG  12 
2.  Position Bias Position Bias   [7][8] a.  b.   a  Position Bias 
 12

666> 2019   NDCG  LambdaDNN  Base 
 Pointwise DNN 
 13LambdaDNN  NDCG  PvCtr 
4.5Lambda 
Lambda  DNN   LambdaDNN   LambdaDeepFM  LambdaDCN  DCN  Cross  
 14DCN 
Lambda  DCN  DCN  NDCG 



<667

 15Lambda Loss  DCN 
5. 
 ""
1.  Bad Case  """"" " 
2.  Bad Case    Bad Case 
3.    
 
                        Lime(Local

668> 2019  Interpretable Model-Agnostic Explanations)    [5]""  """"
 16Lime 
 Lime ---- Pairwise  Listwise 
1. Pairwise    """" "" 1.3km  "" 0.2km  10 ""

<669 2. Listwise  Lime 

 17
6. 
2018   
 Graph Embedding  BERT  Query  
                 DNN        DNN                DeepFM  DCN   LambdaDeepFM 

670> 2019 
LambdaDCN 
Lambda Loss  Query 
 Query 
 Query 
 Log Loss  Lambda Loss  Multi-Task 
Shuffle 
 Google  TF Ranking  Groupwise 
 Listwise 
 Pointwise 


1.  2. Wide & Deep Learning for Recommender Systems 3. Deep Residual Learning for Image Recognition 4. Attention Is All You Need 5. Local Interpretable Model-Agnostic Explanations: LIME 6. From RankNet to LambdaRank to LambdaMART: An Overview 7. A Novel Algorithm for Unbiased Learning to Rank 8. Unbiased Learning-to-Rank with Biased Feedback 9. Real-time Personalization using Embeddings for Search Ranking at Airbnb

2016  2016  2013   2012    AI  NLP   30  ICDE 2015  ACL 2016 Tutorial"Understanding Short Texts" 3  5   Facebook  Research Scientist  Facebook  NLP Service

<671


1. 
      

    
1.   
2.  
3. 


672> 2019  "" 
 1

     NLPNatural Language Processing   
   
 

<673  95%       
 2
   

 
  
  Gap

674> 2019 
"" 
 
    
   
   
2. 
 NLP  
2.1
 NLG  NLP  NLUNature Language Understanding NLU  NLU  NLG   NLU  
   NMT 2019   GPT2  Text2Text 

<675  Data2Text  Image2Text  
 3
2.2
 
   2014  Seq2Seq Model  Token  Embedding   Encoder  Token Decoder  Encoder  Decoder  Attention   Decoder  Encoder   Image2Text  CNN 

676> 2019   Decoder  
 4Seq2Seq 
 Encoder-Decoder   
  """ "  
    
   

<677
  N-Gram  BLUE  ROUGE  Edit Distance Coverage  Jarcard    
                     GANGenerative Adversarial Networks  GAN  NLP   Seq2Seq  
  Encoder  Decoder    2018 
 Contextual Embedding                Elmo(Embeddings from Language Models)OpenAI  GPT(Generative Pre-Training)           BERT(Bidirectional Encoder Representations from Transformers)   NLP    Embedding   ELMo LSTM  Embedding Transformer 

678> 2019  Encoder  Decoder  Attention  12    RNN  Attention 
 5GPT ELMo BERT 
 Tree-Based Embedding  Tree Base  RNN  Embedding Tree   Task  "" 
3. 
 2017  
3.1
 
 Push  


<679  
 E&EExplore and Exploit         UGC     99%      
 7
3.2
 NLP    Context 

680> 2019  NLP   
 8
3.3
  95%  
 
 1.    Case
 2.  Gap 
 

<681        Feeds 
 OOV   /  E&E     Seq  ""    Topic Feature Context   + 
 9
1.  
2. 

682> 2019  
3.  +TF-IDF   Bad Case
4.  /   
     Bi-LSTM  Attention                PreTrain  Word Embedding     LSTM   Attention Dropout   Sigmod    Base  ELMo  Loss LSTM  ELMo Loss  Pre Train  
 10Bi-LSTM  Attention
 ""

<683  CNN+Bi-LSTM+Attention  
 11CNN+Bi-LSTM  Attention
 Topic  Context "" 
 RNN-LSTM    Context  Self-Attention  
   Gap  Bad Case  

684> 2019 
  10% 
 13
       
   NLU    Context  
    Gap
  Target  Context  

<685 "" 
             RNN-Base  Seq2Seq        Encoder   ContextDecoder   
 14LSTM Attention Based Seq2Seq 
 RNN     O(1)
Encoder  Source   Context NMT   Transformer  Context Encoder  Encoder  Decoder  Context  Attention  Context 

686> 2019 
 15Transformer Based Seq2Seq Model
                               10% 
 Combine  Combine   Copy      Copy    Copy  OOV " - "  Copy  Copy  Copy   Copy  Generate  "Where To Point"

<687
 Gap  Language Model Word  Loss   Context   Label Decoder  Beam Search  Decoder  NMT  Coverage Loss   Combine 
 E&E   E&EExplore and Exploit  Epsilon Greedy  Epsilon   Epsilon     7   

688> 2019 
 17 E&E 
3.4
 O2O   
 Data2Text  ""   D2T   Seq   
  Context   Topic Topic  LDA 

<689  Key  Value  Field  Value "" Key"" Value" " Value  NLP  
 Context   Context Loss 
       
 18
  Hard Constrained 

690> 2019 
  Soft Constrained  NMT  
  Decoder  Beam Search  
  Input Context  Output Decoder  Context  Hard Constrained  Output  Model  Soft Constrained   Context Model 
 Decoder  Beam Search  Word   N  N  Beam Search  KK  2 
 Beam Search   Fuction 

<691
 19Decoder Beam_Search 
                          Hard Con-
strained   Context   Soft Constrained                 Context   Decoder fuction  Hard&Soft Constrained     PGC  UGC   PGC  UGC   Context 
3.5 


692> 2019   N   Beam Search   Decoder  Random Search 
 Context   batch batch_size  n-gram  
 20
4. 
   Wide&DeepDNNFNN  CTR   

   Cover  
 

<693
     
  CTR  /  
 User/Context  Item/POI 

   
5. 
 2018   
2018  2019 NLP    2019  GPT2  

694> 2019 




[1] Context-aware Natural Language Generation with Recurrent Neural Networks. arXiv preprint arXiv:1611.09900.
[2] Attention Is All You Need. arXiv preprint arXiv:1706.03762. [3] Universal Transformers. arXiv preprint arXiv:1807.03819. [4] A Convolutional Encoder Model for Neural Machine Translation. arXiv preprint
arXiv:1611.02344. [5] Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural
Networks for Extreme Summarization. arXiv preprint arXiv:1808.08745. [6] Bert: Pre-training of deep bidirectional transformers for language understanding.
arXiv preprint arXiv:1810.04805. [7] ELMODeep contextualized word representations. arXiv preprint arXiv:1802.05365. [8] openAI GPTImproving Language Understanding by Generative Pre-Training. [9] Neural machine translation by jointly learning to align and translate. arXiv preprint
arXiv:1409.0473. [10] Tensor2Tensor for Neural Machine Translation. arXiv preprint arXiv:1803.07416. [11] A Convolutional Encoder Model for Neural Machine Translation. arXiv preprint
arXiv:1611.02344. [12] Sequence-to-Sequence Learning as Beam-Search Optimization. arXiv preprint
arXiv:1606.02960. [13] A Deep Reinforced Model For Abstractive Summarization. arXiv preprint
arXiv:1705.04304. [14] SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient. arXiv
preprint arXiv:1609.05473. [15] Generating sequences with recurrent neural networks. CoRR,abs/1308.0850.

2015  2016  2016  2016  2018   2016  

<695
AI Challenger 2018 
 2018  8  -12                   "AI Challenger 2018  AI " 81 1000 " " - " " 

 2018  AI   NLP  

696> 2019 
 6 20   NLP  

   6  20   
1. 
 TensorFlow  PyTorch  RNet  MnemonicReader  BERT  
2. 
 20  Aspect   LSTM End2End  Aspect 
 2018  2  Kaggle  "" 2018   Kaggle NLP  ELMo  

<697
3. 
 Kaggle Toxic   LSTM Encode + Pooling  Kaggle   RNN(LSTM/GRU)  RNN  RCNNCapsule + RNN  CNN   RNN 

698> 2019 
4. 
 Self Attention   Attention  Attention  LSTM   Gate(RNet)  Semantic Fusion(MnemonicReader) 

<699
5.  
 LSTM  GRU  Hidden size 400 > 200 > 100  Topk Pooling + Attention Pooling         Max   Attention
Pooling  Pooling   Fc  aspect 
  20  Aspect Aspect  4  
   BERT 
 Word + Char    UNK 
   Kaggle Toxic 
 Word + Ngram  NLP  
  14.4WJieba 19.8WSentence Piece Unigram  fastText   Finetune     

700> 2019 
 UNK  UNK   UNK  
6. 
 ELMo Loss  ELMo  ELMo ELMo  Loss 
 LSTM Encoder  ELMo   LSTM  ELMo Loss  Finetune   LSTM  ELMo   ELMo  fastText   ELMo  Finetune   1  ELMo ELMo  Self Attention 

<701
7. 
 Jieba   SentencePiece SentencePiece  Jieba   Finetune Char  Word + Char 
 RNet  MnemonicReader  BERT 
 F1  Aspect  Valid  F1   7 
8.  BERT
 Char  BERT  ELMo   512  Char  BERT 

702> 2019 
Train Loss  Valid Loss   BERT 
9. 
F1  F1  Batch  Batch  
BERT  BERT  Transformer   LSTM  BERT  Loss  LSTM  Transformer   Transformer  BERT ELMo  
 AI Challenger 2018 
Q  AI   ELMoBERT         Aspect  F1AUCLoss   Q  

<703
 AI ChallengerKaggle  Q 
 Q  ELMo 
 Q  TensorFlow  PyTorch 
PyTorch  
  

704> 2019 
WSDM Cup 2019 
 WSDMWeb Search and Data Mining Wisdom  SIGIR   Top2  12  WSDM  NLP  NLP  Travel  WSDM Cup 2019 "" 2  15  
1. 
 ""  ""

<705
   WSDM Cup  
  NLP  Travel  NLP "" (NLI)  ----  NLP  BERT  
2. 
  32  8    AgreedDisagreedUnrelated  3  
""Travel   
 1 Unrelated   70% Disagreed  3%   

706> 2019 
 1
Travel  2   20  100 

<707
 2
3. 
"" Travel 
   
    3 

708> 2019 
 3
 A  B  A  C  B  C  A  B  A  D   B  D Travel  
4. 
BERT  Google  Transformer    11  NLP      SOTA        NLP BERT     Transformer Transformer  Self-Attention  RNN "" BERT 12  24 ""12  16   BERT  12×12=224  24×16=384   BERT 

<709 NLI BERT Travel   BERT  BERT  4 
 4BERT 
Travel  Google  BERT  Finetune 5   Finetune 
 5 BERT 

710> 2019 
5. 
  VotingAveragingBlendingStacking    VotingAveraging    Stacking  
 BERT  BERT  GPU  BERT   Stacking  Blending   BERT 
Travel     
                              Blending  25  BERT  5  Stacking  25  SVMLRK KNN NB  LR   6 

<711
 6
 7    Train Data  Val Data
Train Data     BERT        BERT       Val Data  Test Data BERT  Val Data  Test Data   New Train Data   New Test Data   New Train Data New Test Data           New Train Data       5   " " 5  SVM  5   5  NewTrainingData2 5   NewTestData2 LRKNNNB    NewTrainingData2 NewTestDa-

712> 2019  ta2  LR  NewTestData2   5 
 7
6. 
6.1
 
y  i  i   Agreed  1/15Disagreed   1/5Unrelated  1/16

<713
6.2 
      Travel               0.8675025  BERT  0.87700+0.95PP25  BERT   0.87702+0.952PP  0.88156+1.406PP NLP  
 8
7. 
   BERT    BERT  

[1] Dagan, Ido, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge, Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment. Springer, Berlin, Heidelberg, 177-190.
[2] Bowman S R, Angeli G, Potts C, et al. 2015. A large annotated corpus for learning natural language inference. In proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP).

714> 2019 
[3] Adina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In NAACL.
[4] Rajpurkar P, Zhang J, Lopyrev K, et al. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.
[5] Luisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.
[6] Hector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47.
[7] Bowman, Samuel R., et al. 2015."A large annotated corpus for learning natural language inference."arXiv preprint arXiv:1508.05326.
[8] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. arXiv preprint arXiv:1804.07461.
[9] Chen, Q., Zhu, X., Ling, Z., Wei, S., Jiang, H., & Inkpen, D. 2016. Enhanced lstm for natural language inference. arXiv preprint arXiv:1609.06038.
[10] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.
[11] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
[12] David H. Wolpert. 1992. Stacked generalization. Neural Networks (1992). https: // doi.org/10.1016/S0893- 6080(05)80023- 1.

 NLP  NLP  NLU   CCTV-1    NLP  NLP    NLP  NLP   BERT   NLP  NLP   YunOS    NLP  NLP   

<715
  30  KDDWWWAAAIIJCAITKDETIST   ICDM2013  1   NLP  30  ICDE 2015  ACL 2016 Tutorial"Understanding Short Texts"  3  5   Facebook  Research Scientist Facebook  NLP Service

716> 2019 
 ETA 

1. 
ETAEstimated Time of Arrival"" " " 
ETA  
 ETA    ETA  
 ETA  -  -  -    
ETA 

<717  ETA   ETA  ETA    ETA 
 ETA  ETA    ETA  ETA    ETA   ETA    ETA   
 ETA

718> 2019  
 -  -  -   ETA  " -  - " ETA   LR-XGB-FM-DeepFM-  
ETA 
 ETA  
2. 
2.1
 CTR  ETA  LR->  ->Embedding->DeepFM->  
  Embedding  ID  FM 

<719
   /  /  /  /  / 
 /  /  / 
 Wide&DeepDeepFMAFM   DeepFM  Base  DeepFM   Embedding  FMFactorization Machine deep FM  DNN  Feed-Forward   Learning Decay/Clip Gradient/  /Dropout/  
2.2
 ETA  Square  Absolute  MAE  ME  Learning Decay 
 ETA  N  1min  1min   N  3  absolute   1.2  absolute  1.8  absolute   ETA  

720> 2019 

2.3
" + " ETA     
 TF  TF  
  (a*b+c)*d  TF  OP   bd  ac 
   OP 
 TF   

<721

 Shared Parameters   TF   Label 
  
  Label  Regularization 
  Loss 
    

2.4
 "" x   TF  w1   w2*x w1  w2 

722> 2019   / 

3. 
3.1 + 
  ETA  
   
  "" RF  RF 

<723  """"  RF 

 <   >   RF  ETA  + 
4. 
4.1 
 Spark  -> Spark  TFRecord ->  -> TensorFlow Serving  GPU  -> CPU Inference   Epoch  4  TF   TF  IO  Spark 

724> 2019 
TFRecord  3.6 16   PS   PS Serving  GPU   Chief Worker  Valid   Spark  Serving GPU  
  AFO       TF    PS-Worker             tf.train.MonitoredTrainingSession  PS Step   Worker   Worker  1~2W/s Epoch   16  Worker  16  4-6      PS-Worker   RPC  Worker     GPU  OP  Device Device   Loss  Grad 

<725

TF  ID  Vocab   Spark   Libsvm  avg/std    TF     ID TF 
list_arr inference  ph_vals ph_idx
tf_look_up = tf.constant(list_arr, dtype=tf.int64) table = tf.contrib.lookup.HashTable(tf.contrib.lookup. KeyValueTensorInitializer(tf_look_up, idx_range), 0) ph_idx = table.lookup(ph_vals) + idx_bias
 Spark  avg/std  TF   constant  ph_in  ph_out

726> 2019 
constant_avg = tf.constant(feat_avg, dtype=tf.float32, shape=[feat_dim], name="avg") constant_std = tf.constant(feat_std, dtype=tf.float32, shape=[feat_dim], name="std") ph_out = (ph_in - constant_avg) / constant_std
4.2TF 
  
ETA  DeepFM  TensorFlow  SavedModel   Tensorflow SavedModel 
 S  TensorFlow SavedModel 
     TensorFlow Serving CPU     gRPC API  RESTful API  Thrift  
  AFO GPU  TensorFlow Serving    JNI  TensorFlow  Java API TensorFlow
Java API SavedModel 
 TensorFlow Java API  SavedModel  CPU  batch=1  1ms  3 
 TensorFlow Java API  C++  libstdc++.so   GCC  4.8.3 CPU  CentOS 6,  GCC  4.4.7 TensorFlow SavedModel  GCC    Input 

<727  TensorFlow Remote Remote  Output  

  TP99   5ms 


 
   
  Embedding  LSTM/ CNN/ 
 

728> 2019 

 2017  ETA  2018  

<729


1. 
 App    
   
  

730> 2019 
   

   
  TP99  10ms  5ms   RPC  CPU  GPU  
 5ms TP99  10ms       End-to-End   
2. 


<731
 5ms  
 3 
2.1 
  
  
                               addrbuildingunit floor  1  2  5 1 2 5         
  12 MAE

732> 2019 

2.2 +  
 LSHPQ  
 NLP  70%+ 20%+    Embedding  
7 7   
                Word2Vec  charLevel     Embedding GPS 

<733

  12.20pp  ME  87.14sMAE  38.13s 1min  14.01pp2min  18.45pp3min   15.90pp 2.3End-to-End     100% NLP 

734> 2019 
  VC                        GPS     ID   Embedding   ETA   5msTP99  10ms    Fusion    Fusion   Flops     Robust  LSTM GPS  Embedding
  LSTM  charLevel   20%  

<735
 charLevel 
 charLevel  GPS   GPS   Embedding GPS  Embedding  GPS  
 Embedding  GPS  Embedding   ID ID   Trainable

736> 2019 

  End-to-End                                   Feature Permutation   GPS Shuffle  GPS  GPS  ME   

<737

  GPS  

 1 2 ......

 GPS  >>  >>   GPS  

 End-to-End   100%ME  4.96sMAE  8.17s1min  2.38pp2min  5.08pp3min   3.46pp 

3. 
    Faiss  TensorFlow Operation  
 End-to-End  Word2Vec  
3.1
Nearest Neighbor Search   

738> 2019   ANNApproximate Nearest Neighbor
  3  K-D  LSH  PQ  
 ANN-Benchmarks  Erikbern/ANN-Benchmarks  ANN  GPU  Faiss  Benchmark
Faiss  FaceBook  2017    C++/Python  GPU 
 Faiss 
  8W  GPS 

<739  8W   Mac  CPUCPU  GPU
3.2
 TensorFlow  C API           Client              Protobuf    OP TensorFlow  OP  
 Profile  Profile  Timeline  OP 

740> 2019 
               LSTM/GRU/SRU       TensorFlow LSTM  BasicLSTMCellLSTMCell LSTMBlockCellLSTMBlockFusedCell  CuDNNLSTM   CPU  CuDNNLSTM FullyConnect 
 2.3pp   OP  BasicLSTM    contrib   LSTMBlockFusedCell    GRU/ SRU 
 LSTMBlockFusedCell  LSTM  Loop   OP Timeline 
This is an extremely efficient LSTM implementation, that uses a single TF op for the entire LSTM. It should be both faster and more memory-efficient than LSTMBlockCell defined above.   Tensorflow1.10.0CentOS 7  CPU inference 1000 

<741

 LSTMBlockFused FullyConnect  
 

lstmOP Fully Connect SRU GRU Block GRU LSTMBlockFused LSTM Block LSTM BasicLSTM

(ms) FLOPs 

1.18

27.83M 7.00M

4.00

27.96M 7.06M

3.64

28.02M 7.10M

4.44

28.02M 7.10M

2.48

28.09M 7.13M

4.34

28.09M 7.13M

4.85

28.09M 7.13M

4.92

28.09M 7.13M

(MB) 29.1 29.4 29.6 29.6 29.7 29.7 29.7 29.7

 -2.3pp       

3.3 
 
  lstm  Embedding  Word2Vec  

End-to-End 

ME End-to-End  Word2Vec 

MAE 4.14

1min 2min 3min

-0.45

-0.31%

0.05%

 End-to-End  char embedding  Word2Vec  Word2Vec  char embedding  char embedding   Word2Vec 


742> 2019 

 1min |pred-label|<=60s  2min |pred-label|<=120s  3min |pred-label|<=180s  2  :  a charLevel  Word2Vec LSTM  bWord2Vec  End-to-
End GPS    
    

 b   Word2Vec  MAE  15s char embedding  Word2Vec  char embedding 2.3   End-to-End 

ME End-to-End  Word2Vec 

MAE -1.28

1min 2min 3min

0.64

0.90%

0.85%

 End-to-End  Embedding   Word2Vec  
 End-to-End 

<743
 Embedding  
End-to-End  Word2Vec End-to-End  Word2Vec
End-to-End   Word2Vec  
  <=>    End-to-End  Word2Vec  Case End-to-End  
4. 
   Word2Vec  End-to-End 
   -  -  TF OP  Embedding  

744> 2019 
5. 
 ETA    ETA  ETA 
 ETA    ETA  
6. 
 
7. 
 AI ----  AI    AI  tech@meituan.com AI 

<745
ICDAR 2019 

     8-neighbor  8-neighbor    backbone 
 ICDAR2019 (International Conference on Document Analysis and Recognition)  "As it is of more practical uses"
ICDAR  (IAPR)  ICDAR                                   ICDAR 2003 "Robust Reading Competitions" 89  3500 ICDAR 2019  9  20-25   ""ICDAR 2019 Robust Reading Challenge onReading Chinese Text on Signboards

   

746> 2019   1
 1
 CNN Faster RCNNSSD  FPN[1] 

<747
      Faster RCNN  SSD  

1.     
2.   

1 2
1 FCN [2] FCN, fully convolutional network (fc)   FCN   FCN  FCN  

748> 2019 
 2
2 Textboxes [3]  SSD      NMS 
 3Textboxes 

 SSD 4  

<749  4   
 4
1    5 Conv6_2 Conv7_2  Conv8_2 
 5
2 

750> 2019   5  conv4_3 Fc7 4     conv4_3_f,fc7_f,conv6_2_f,conv7_2_f,conv8_2_f  conv9_2_f 
 6   3*3   d=256
 6
3  7segment link  Faster-RCNN  
 7

<751 2  8  
 8
 9 
ab c  A  Bd A+B/2 AB  AB 
 9

752> 2019 

ICDAR2013ICDAR2015  ICDAR2013  229  233 ICDAR2015   1000  500 
1  ICDAR2015  1
 1
"baseline" ssd  + " " baseline " +  " baseline    1  3  73.4->76.3  
2 2  3
 2ICDAR2013 

 3ICDAR2015 

<753

 ICDAR2015  PixelLink FPS  TextBoxes++  FPS  10 
 10
3  11   500  SegLink  5  
 4

754> 2019  1

<755
 11

   ICDAR2013  ICDAR2015   PixelLink [4]   

Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie."Feature Pyramid Networks for Object Detection."arXiv preprint. arXiv: 1612.03144, 2017. J. Long, E. Shelhamer, and T. Darrell."Fully convolutional networks for semantic segmentation."In CVPR, 2015.

756> 2019 
M. Liao, B. Shi, and X. Bai."Textboxes++: A single-shot oriented scene text detector." IEEE Trans. on Image Processing, vol. 27, no. 8, 2018. D. Deng, H. Liu, X. Li, and D. Cai."Pixellink: Detecting scene text via instance segmentation."In AAAI, pages 6773­ 6780, 2018.



   
 tech@meituan.com 

<757
CVPR 2019 


CVPR 2019   5160  1294   ""
         Trajectory Prediction Challenge   CVPR 2019 Workshop on Autonomous Driving -- Beyond Single Frame Perception  

758> 2019   
 3   3   2   1.3425  

  
 2   ID
 3  2   3 

Average displacement errorADE 

<759
Final displacement errorFDE 
 

 1.  2. 
 
   
  1. Social GAN Encoder  Pooling 

760> 2019   2. StarNet LSTM  Hub 
 Host 


 
  

<761  12   6  6   
 

  
           LSTM  Encoder-Decoder      Encoder  Decoder  Noise Noise 

762> 2019 
 Encoder  LSTM  Decoder  LSTM  Noise   Noise 

 


 Loss  Weighted Sum of ADEWSADE  Adam  WSADE  1.3425





StarNet

TrafficPredictApolloScape Baseline 

WSADE 1.3425 1.8626 8.5881

<763

  Trajectory Prediction Challenge   

Yanliang Zhu, Deheng Qian, Dongchun Ren and Huaxia Xia. StarNet: Pedetrian Trajectory Prediction using Deep Neural Network in Star Topology[C]//Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2019. Gupta A, Johnson J, Fei-Fei L, et al. Social gan: Socially acceptable trajectories with generative adversarial networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018: 2255-2264. Apolloscape. Trajectory dataset for urban traffic. 2018. http://apolloscape.auto/ trajectory.html.

 PNC   PNC   PNC   PNC 

764> 2019 
  StarNet  

1. 
" "    StarNet  IROS 2019IROS  IEEE/RSJ International Conference on Intelligent Robots and SystemsIEEE      ICRARSS 
1.1
  1  
 1       ""

<765
 1
1.2
       

766> 2019 
 
1.3
 Kalman Filter, KFHidden Markov Model, HMM Gaussian Process, GP
( ) ( )  X t = f X t-1  p Xt X t-1 
 ""    
                      Long Short Term MemoryLSTM  5   CVPRIEEE Conference on Computer Vision and Pattern Recognition 2019  10  2    CVPR 2016  Social-LSTM Social-LSTM  LSTM   Social Pooling Layer   NxN  

<767
 2Social LSTM  Social Pooling 
 CVPR 2019  &  &   LSTM    
 3

768> 2019 
                /    Occupancy Grid Map, OGMMessage Passing, MP Graph Neural Network, GNNGCN/GAT 

2. StarNet 
  LSTM  4   GCN  

  N   Step 1  i  t   x  y
( ) Interaction1t = f P2t - P1t , P3t - P1t ,, PNt - P1t

 f

 Step 2 

( ) 

Pt +1 1

=

g

P1t , Interaction1t



 g  LSTM

<769
 4  StarNet
 1.  2  3  1  2  3 
  2.  N  N   N   1  N  

770> 2019  
 Attention   Message Passing  
""      1245 " + " 
 5StarNet 
 5 Host Network  LSTM  Hub Network  LSTM  Hub Network   st  LSTM   st  rt Host Network  P1t  rt   q1t  Attention 

<771



P1t



q1t



LSTM



Pt +1 1



 4  UCY&ETH 

         4        ZARA-1/ZARA-2UNIVETH

HOTEL 3.2 

 3.2  0.4  3.2  8 



a ADEAverage Displacement Error

 8 b FDEFinal Displace-

ment Errorc





 80%   LSTM  0.029  
 StarNet 
  

772> 2019 
 Hub Network 
3. 
  
  
   1  2   

[1] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. Li and S. Savarese,"Social lstm: Human trajectory prediction in crowded spaces,"in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE 2016, pp. 961-971.
[2] H. Wu, Z. Chen, W. Sun, B. Zheng and W. Wang,"Modeling trajectories with recurrent neural networks,"in 28th International Joint Conference on Artificial Intelligence (IJCAI). 2017, pp. 3083-3090.
[3] A. Gupta, J. Johnson, F. Li, S. Savarese and A. Alahi,"Social GAN: Socially acceptable trajectories with generative adversarial networks,"in 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018, pp. 2255-2264.
[4] A. Vemula, K. Muelling and J. Oh,"Social attention: Modeling attention in human crowds,"in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1-7.
[5] Y. Xu, Z. Piao and S. Gao S,"Encoding crowd interaction with deep neural network for pPedestrian trajectory prediction,"in 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018, pp. 5275-5284.
[6] D. Varshneya, G. Srinivasaraghavan,"Human trajectory prediction using spatially

<773
aware deep attention models,"arXiv preprint arXiv:1705.09436, 2017. [7] T. Fernando, S. Denma, S. Sridharan and C. Fookes,"Soft+hardwired attention:
An lstm framework for human trajectory prediction and abnormal event detection," arXiv preprint arXiv:1702.05552, 2017. [8] J. Liang, L. Jiang, J. C. Niebles, A. Hauptmann and F. Li,"Peeking into the future: Predicting future person activities and locations in videos,"in 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019, pp. 5725-5734. [9] A. Sadeghian, V. Kosaraju, Ali. Sadeghian, N. Hirose, S. H. Rezatofighi and S. Savarese,"SoPhie: An attentive GAN for predicting paths compliant to social and physical constraints,"in 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019, pp. 5725-5734. [10] R. Chandra, U. Bhattacharya and A. Bera,"TraPHic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions,"in 2019 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019, pp. 8483-8492. [11] J. Amirian, J. Hayet and J. Pettre," Social Ways: Learning multi-modal distributions of pedestrian trajectories with GANs,"arXiv preprint arXiv:1808.06601, 2018.

   

  C++  Python   TensorFlow  Pytorch    tech@meituan.com


   
2019  Hadoop YARN  OneData SaaS  Jupyter  
 

<775
Hadoop YARN


YARN  Hadoop  Hadoop  
 YARN  2.7.1  YARN  
  Hive on MapReduceSpark SQL  
  Spark StreamingFlink    TensorFlowMXNetMLX

YARN   1000  100  CPU  1  CPU  1  10  CPU   5  50000/ (100*1000)=0.5 50% 
  5  5000  50000/(100*5000) = 10% 90% 


776> 2019 
  Hadoop YARN 
 Hadoop YARN  

YARN 
YARN  
YARN  Hadoop 

YARN  cpumemory 
class Resource{ int cpu; //cpu  int memory-mb; //  MB 
}
 YARN List[ResourceRequest]
class ResourceRequest{ int numContainers; //  container  Resource capability;//  container 
}
YARN List[Container]
class Container{ ContainerId containerId; //YARN  container  Resource capability; //  container  String nodeHttpAddress; //  container  NodeManager  hostname
}

YARN 

<777

YARN 
  ResourceScheduler  YARN  Container   AsyncDispatcher   ResourceTrackerService  NodeMan-
ager   ApplicationMasterService  RPC 
  AppMaster  YARN  / 
 1. AppMaster  YARN  (List[Re-
sourceRequest])                     (List[Container])

778> 2019  2. Nodemanager  NodeManager  Container ResourceScheduler 
 (FairScheduler)  (CapacityScheduler) 


 (App) 




<779


1.  FairScheduler  2.  (node) ROOT 
  App App  node 

780> 2019 

1.  Quota
2.  /App /App 
3.  /App
          ROOT -> ParentQueueA -> LeafQueueA1 ->
App11 node  App11  Container

class FairScheduler{ /* inputNodeId * outputResource  app  container  * root  Queue  */ synchronized Resource attemptScheduling(NodeId node){ root.assignContainer(NodeId); }
}
class Queue{ Resource assignContainer(NodeId node){ if(! preCheck(node) ) return; //  sort(this.children); //  if(this.isParent){ for(Queue q: this.children) q.assignContainer(node); //  }else{ for(App app: this.runnableApps) app.assignContainer(node); } }
}
class App{ Resource assignContainer(NodeId node){ ...... }
}

<781

  FairScheduler   Container 

 scheduler LockFairScheduler   AllocationFileLoaderService
  Continuous Scheduling Thread
  Update Thread Container   Scheduler Event Dispatcher Thread:  App 
App node node 

 

782> 2019 
  
 QPS  TP99  RPCResourceManager  NodeManager  AppMaster  RPC   RPC  RPC  RPC  
 - 
                                               (validSchedule)        validSchedule 
 validSchedulePerMin  1  0
validPending = min(queuePending, QueueMaxQuota) if (usage / total > 90% || validPending == 0): validSchedulePerMin = 1 //  90% 0 if (validPending > 0 && usage / total < 90%) : validSchedulePerMin = 0;//  90%
 validPending   queuePending   QueueMaxQuota   usage   total 

<783
 90%   linux    
validSchedulePerDay  validSchedulePerDay = validSchedulePerMin /1440
              validSchedulePerMin > 0.9; validSchedulePerDay > 0.99
 -  Container 
 Container CPS­  Container   validSchedule   CPS 
CPS CPS  CPS 
  Container   cpumemory  
  App     

 1000  1000  App App  500  Queue  App  Container  1 

784> 2019   1000  Container  CPS  1000/s

   
    YARN 
 ­Scheduler Load Simulater(SLS)

 SLS ResourceManager  SchedulerApp  NM(NodeManager) 

<785
 NM  
  APP  NM  NM/App   cpu 
 SLS  Scheduler Wapper   SLS  FairScheduler 
 Queue Queue App  App  
 SLS   Scheduler Wapper  ResourceManager SLS  ResourceManager   YARN-7672

 validSchedule    FairScheduler   preCheck  preCheck  
 10 
  preCheck      preCheck 

786> 2019       

1000  1000 500  Container  40   
 CPS


 2   50  30  


<787 
 /  (resourceUsage) 2  resourceUsage   resourceUsage 
           App      Container      containerResource           resourceUsage      resourceUsage += containerResource App  Container  resourceUsage -= containerResource   resourceUsage  O(1) 

 /   30G30  5G5   YARN-5969

788> 2019 

 2  20  
 /  /   Children  / 
 20 YARN3547

   Container  0.02ms  , 

<789

 Container 
  Container 
 10  cpuT1  App1  10  cpu 10  cpu  App1T2  (T2 > T1)  App2 App2   App1  App2  
    Container 
 1.  2.  3.  / 
 / 


790> 2019    2000  5 
(2ms-5ms) 200   1  1.2  2000 Container 
 40  CPS  5  




<791
15:26 pending  0 15:27 resourceUsage  1.0 100% pending  4M400  mb  

 
    YARN 

 

 RD     
   
 

792> 2019 


 
"" resourceUsage   resourceUsage 
 100%   bug
 resourceUsage   
 resourceUsage   oldResourceUsage  resourceUsage newResourceUsage
 oldResourceUsage  newResourceUsage   bugnewResourceUsage   RD  

 Hadoop YARN 
1.  2.  3.  4. 

<793
 5. 
 6. 
 YARN  1   10 100 
  Hadoop 3.0  Global Scheduling YARN   Feature YARN Federation  YARN   YARN    YARN Federation



  3   90%  Serving  99.9% CPU/ GPU  GPU  CPU   NPU/FPGA     tech@meituan.com ""

794> 2019 



  
   
   
  

<795
 1
""   

 ACLAccess Control ListRBACRoleBased Access Control   RBAC  

796> 2019 
 2
 2   ACL 
  RBAC 

1. ACL  
2. RBAC   
 ACL  RBAC  1. 
    2. RBAC 

<797  /  /      1.     2.  /  /   /  /  3   "" 
 3
 3  3 

798> 2019 

  3   
    Mafka Kafka 
 ETL  
 4

  
  
   


<799
 5

   
  limit 10  10 
   
  

800> 2019 
 6

  
  
   QPS 

1.  Plugin  
2.  3. 



<801

 7
 7  3 
  Plugin  
   

Plugin 
  Plugin 
Plugin  Plugin  RPC   Plugin 

802> 2019 
 8Plugin 
 8  Plugin  Plugin  
  
 Plugin  SDK  Plugin  
  Plugin Plugin  SDK    BA 

<803


 
   
     SDK 

 9 

 9

804> 2019    SDK       

  
 10
 10  
  6  
   

<805       

 Elasticsearch 
 11
 11    appkey appkey 
   Elasticsearch  type
type 

806> 2019 
  type 


 
 

 12 

 12

  HTTP BA  Nginx     
  Client  Pigeon RPC 
 POM 

<807

  
 13

 
   HBase      MySQL  Binlog                                             Elasticsearch  MySQL 

808> 2019  

 14 

 14

  1MySQL  Redis 
  Elasticsearch  
   Crane 
   MySQL  Redis 
 Elasticsearch 

 MySQL  Zebra  

<809
 15
 15    MySQL 
   Redis  Redis 
 MySQL    Elasticsearch
  

 

810> 2019 

 16 

 16

  MQ   
  Crane   
  



  Plugin   Plugin  

<811

 17 

 17

  Plugin  Plugin   
  


 TechClub-Java 2006   IBM2014  BI   2017  BI  

 fuyishan#meituan.com


812> 2019 
OneData SaaS 


  
   SLA 
  
  


 OneData    OneData 
OneData 
OneData
 OneData  1 

<813
 1OneData 
OneData
 1.  OneData    OneData 
ETL      2.        OneData
OneData
                              OneData  OneData 
OneData   

814> 2019  OneData     
 2OneData 
OneData
OneData   
 OneData 

<815
OneData 

 ""   
 3

 

816> 2019  1.  
 1 
 
 4
2   SLA 
 5

<817
                      ODS­>DWD­ >DWA­>APP ODS->DWD->APP  ODS->DWD->DWT->APP  
    ODS>DWD->DWT->DWA->APP    ODS >DWD>DWA->APP                 DWD     DWT  DWD->DWA
  DWA  DWD  DWD DWT 
  DWT  DWT  ETL 
 DWTDWA  APP  ODS ODS  DWD 
  DWT  DWA 
2.   BDWMFS-LDMMLDM   
     
   DWA 

818> 2019 

3.    1  
  -trade   -USD
2   clienttype->client_type       64    Review 
     =  +  +  +  +  +  +  

<819

 6
3  A. 

  Hive MySQL    

   /  ......

count amout ratio ......

Bigint Decimal Decimal ......

Bigint Decimal Decimal ......

10

0

20

4

10

4

cnt amt ratio ......

0.9818

B.  trade-  C. 

   ......

 daily weekly ......

 d w ......



D. 

 



average

 wtd

......

......

 avg wtd ......

 

E.  +    -trade_amt
F.  +   -install_poi_cnt

820> 2019  G. 
 7
H.  +  +   / 
 8
I.  +  +  +  

<821

 9

4   24 

Hive  


  string

 

......

......

bigint ......

MySQL 
varchar
bigint ......

   

10
10 ......

0 ......

date
cnt ......

YYYY-MMDD  ......


 



 10

822> 2019 

 "" 
 11
     
 

    OneData 

<823 1.    
 12
  
2.   "" 

824> 2019 
 13
      
   OneData 


 OneData   OneData  

<825
 14

 OneData  
 15

826> 2019 


 16

  OneData                        OneData 

<827
1
 17

 OneData   OneData  OneData     One Entity  Data As a Service 

828> 2019 
  



<829



  
  ""----   ROI 
  ""

830> 2019  
  


  SRE  
1. 
"" 

<831
 PPlanDDoCCheckAAct 
  Plan  Standard  ->  ->  -> 
  Do  ->  ->  ->  
  Check  ->  ->   -> 
  Act  ->  ->  ->   -> 
2. Plan&Standard
 2-3   "" ""
  
  Leader    
 ""  

832> 2019 
 " " SMART           2   "" """"" " TA   
3. Do
3.1
 ->  ->  ->  12   ""
  "MECE "

<833    """"""  ""   "" "" ""
" "

834> 2019 
   +  """" "" ""  ""  
  Owner R R """""" 
  R    "" 80/20  20%   80% 

<835  
3.2
 
    .
""  PC " """  
3.2.1                                

836> 2019  CRM  DBK-V  CDN 
OWT   ""    
3.2.2                                 Yarn 2.0  ContainerVcore+Mem  +  HDFS   HDFS 

<837
 ETL  ""   

1.   HDDSSD  
2.  
    map  20%  
            OLAP               KylinElasticsearchDruidMySQL  HDFS   HDFS  KylinHBase 
  ETL  ETL  Storm Flink  MapReduce  ""

838> 2019 
3.2.3   """" """ "        

<839
3.2.4       3.2.5                                 CPC CPT    " """

840> 2019 
3.2.6   
4. Check
  
     List" " " ->   ->  -> "    A  B 

<841
   R   
  CDNTairRedis  RD  " "  CPUCDN  
5.  (Act)
  " +  + " " """ "" "" ""
  1-2  
  PDCA   ROI   

842> 2019 
 7  

  " "   

2017    

 

<843
Jupyter 


 Kaggle  Kaggle  Kaggle Kernels  Kaggle Kernels  Fork   Kernel  Kaggle Kernels  Kaggle Kernels  Jupyter  Kaggle Kernels  Introduction to Kaggle Kernels
 Kaggle Kernels 

844> 2019 
  Kaggle Kernels  Kaggle Kernels 
Kaggle Kernels----   Jupyter 


 
  SQL    SQL  ETL    Spark 
 ETL Spark  /    
  Spark   Bad Case 
  SQL ->  Excel -> 
   

 Bad Case 

<845


      Python 
Python 
 / ­> Python  / ­>  /  
       

 
 Jupyter
 Kaggle Kernels  Jupyter Notebook   Jupyter
 Jupyter 
  Spark Jupyter 

846> 2019 
    WiKi    
 Jupyter 
Jupyter 
Project Jupyter  

<847     Jupyter      Web              Jupyter 
Jupyter 
 Jupyter  JupyterLab Notebook ServerIPythonJupyterHub 
JupyterLab labextension JupyterLab  Jupyter   JupyterLab   / Notebook  JupyterLab    JupyterLab JupyterLab  TypeScript https://jupyterlab.readthedocs.io/en/stable/ developer/extension_dev.html

848> 2019 
JupyterLab 
Notebook Server serverextension Notebook Server   Python       Tornado  Web     Notebook Server        Web       Handler     Handler  1.  JupyterLab 
 JupyterLab  Notebook Server  2.                      jupyter-rsession-proxy JupyterHub  RStudio Notebook Server https://jupyter-notebook.readthedocs.io/en/stable/extending/handlers.html ##3# Jupyter Kernels Jupyter  Kernel ipykernel 

<849
 Kernel  Scala  almond R   irkernel
IPython Magics IPython Magics     %%%         Magics  %matplotlib inline Notebook  matplotlib  Notebook  Magics  Magics  Line Magics % Cell Magics %  Cell  IPython Magics    Line Magics  Cell Magcis  Cell IPython Magics  %%spark%%sql   Spark  SQL  Magics   Word2Vec  %%cython  Cython  Python  IPython Magics https://ipython.readthedocs.io/en/stable/ config/custommagics.html
IPython Widgetsipywidgets IPython Widgets      Jupyter Notebook  IPython        Python  Python  IPython Widgets   IPython Widgets  Widgets IPython Widgets https:// ipywidgets.readthedocs.io/en/stable/examples/Widget%20Custom.html

850> 2019 
ipyleaflet
 JupyterHub Authenticators
JupyterHub  Authenticator  SSO   JupyterHubAuthenticator https://jupyterhub.readthedocs. io/en/stable/reference/authenticators.html
Spawners JupyterHub  Notebook Server  Notebook Server  Notebook Server   Docker K8s  PodYARN   Spawner Spawner   Spawner  Spark  K8s  Pod  KubeSpawner  Spawner  Spark Spawner https:// jupyterhub.readthedocs.io/en/stable/reference/spawners.html

<851

 Jupyter    Spark Spawner    JupyterLab  Notebook Server    JupyterLab  Notebook Server      Authenticators + K8s Spawner 
  JupyterHub on K8s Jupyter  1. 2. 3. 
  JupyterLab  Jupyter Server  Commuter Notebook   K8s  Cantor AirFlow

852> 2019 
  Spark-Submit 
   MSS  NB-RunnerNotebook Runner nbconvert 
Spark 
 Jupyter  Spark  
JupyterHub on K8s ProxyHubKubernetes Jupyter Server PodSSO 

<853  Proxy  Jupyter Server Pod  Jupyter Server Pod  
 Jupyter  Spark
Jupyter  Kaggle Kernel   Jupyter  SparkJupyter  Spark   Toree Python Kernel   PySpark
 Jupyter  SparkJupyter   PySpark 
#3## Jupyter   Jupyter  JupyterLab Jupyter Server Kernel IPython
Jupyter 

854> 2019   IPython  exec  PySpark ##3# PySpark   PySpark   PySpark  spark-submit    Python shellPythonIPython Spark    PySpark 
PySpark  SlideShare  Spark  Python  Py4J  Driver JVM  

PySpark 

<855

PySpark 
IPython 

 IPython  Spark 
Toree  spark-submit  Shell Spark 

856> 2019 
  PySpark  Kernel Kernel  IPython  PySpark  kernel.json  PySpark 
 QueueMemoryCores
 PySpark  Spark  IPython  Magics%spark  %sql
  IPython  Spark 
 JAVA_HOMEJava  /usr/local/jdk1.8.0_201  HADOOP_HOMEHadoop  /opt/hadoop  SPARK_HOMESpark  /opt/spark-2.2  PYTHONPATH    Python     $SPARK_HOME/py-
thon:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip  PYSPARK_PYTHON       Python    ./ARCHIVE/
notebook/bin/python Python  spark. yarn.dist.archives   PYSPARK_DRIVER_PYTHONSpark Driver    Python       Conda   Python            /opt/conda/envs/ notebook/bin/python 
         bin    PATH     $SPARK_HOME/ sbin:$SPARK_HOME/bin:$HADOOP_HOME/sbin:$HADOOP_HOME/ bin:$JAVA_HOME/bin:$PATH
 IPython  Spark 
import pyspark spark = pyspark.sql.SparkSession.builder.appName("MyApp").getOrCreate()

<857
 Spark  Notebook
 Notebook  nbconvertPython API   NB-Runner.py
# Import import nbconvert  ExecutePreprocessor  import nbformat from nbconvert.preprocessors import ExecutePreprocessor
#  notebook_filename  notebook  with open(notebook_filename) as f:
nb = nbformat.read(f, as_version=4)
#  notebook  ep = ExecutePreprocessor(timeout=600, kernel_name='python')
#  (preprocess) notebook  preprocess ep.preprocess(nb, {'metadata': {'path': 'notebooks/'}})
#  notebook  with open('executed_notebook.ipynb', 'w', encoding='utf-8') as f:
nbformat.write(nb, f)

  Notebook  Spark Python NB-Runner.py  
  Notebook  Spark Spark-Submit NB-Runner.py  
 2 Spark   Spark-Submit  NB-Runner.py
 nbconvert  Notebook 

858> 2019 
nbconvert 
 1  2 "PySpark "" IPython  Spark  ""nbconvert "

<859
Spark-Submit NB-Runner.py             IPython    Spark.builder.getOrCreate Driver JVM       Py4J Gateway Server  Spark.builder.getOrCreate "   IPython    Spark    "  Popen(spark-submit)        Py4J Gateway Server  Py4J Gateway Server 
 PySpark 
def launch_gateway(conf=None): """ launch jvm gateway :param conf: spark configuration passed to spark-submit :return: """ if "PYSPARK_GATEWAY_PORT" in os.environ: gateway_port = int(os.environ["PYSPARK_GATEWAY_PORT"]) else: SPARK_HOME = _find_spark_home() # Launch the Py4j gateway using Spark's run command so that we
pick up the # proper classpath and settings from spark-env.sh on_windows = platform.system() == "Windows" script = "./bin/spark-submit.cmd" if on_windows else "./bin/
spark-submit" ...
 IPython  PYSPARK_GATEWAY_PORT  Py4J Gateway Server  Spark-Submit 

860> 2019 
Py4J Gateway Server  PYSPARK_GATEWAY_PORT   Python  ExecutorPreprocessor   IPython 


                                MySQL  Hive  Notebook  SQL IPython Magics %%sql  SQL
SQL Magics 
%%sql <var> [--preview] [--cache] [--quiet] SELECT field1, field2
FROM table1 WHERE field3 == field4
SQL                  MySQL        Pandas DataFrame Hive  Spark DataFrame  Python 


<861

Notebook 
Notebook  Jupyter   Notebook 



862> 2019  
Notebook 

 PySpark  Spark  Spark ML      Jupyter            X-on-Spark     XGBoost-on-SparkLightGBM-on-Spark      IPython Magics %%spark 
Spark Magics 
%%spark [--conf <property-name>=<property-value>] [--conf <property-name>=<property-value>] ...
 %%spark  Spark  Notebook   spark  sc Spark  SparkSession  SparkContext

<863        LightGBM-on-Yarn          Azure/mmlspark  Notebook  Spark 
LightGBM on Spark Demo

 ipywidgets  

864> 2019 

 Jupyter   
 Jupyter  

 

Base  tech@meituan.com 


MIT "" NLP 
 2019  1  21  2018 "35   35 "35 Innovators Under 35 AI  NLP  ""
Innovators Under 35  1999    Google    Larry Page  Sergey BrinLinux    Linus Torvalds Facebook     ·    Mark Zuckberg         Marc

866> 2019 
AndreessenApple  Jonathan Ive  

  
 
   ""
                             WAMDM  Native XML  OrientX  Deep Web   
Deep Web   NDBC 2006  2007   SIGMOD 2007 Undergraduate Scholarship  7  

 IBM   offer
""

<867  " offer" 12   offer 
  
2010  10 · ""   · 
 AI  

868> 2019 
 6    VLDBICDE IJCAICIKM  30  ICDE 2015 
  
    
   
"   "
 NLP    
Facebook
2016 

<869
" "
 offer FacebookFacebook  "   "
 Facebook NLP Service Post  Facebook   Trump  
"    "
 Facebook  Query NLP   80%  Facebook  Facebook  

" 

870> 2019 
"2018  
 Facebook   
 
 
"""" ""  
"Food + Platform"  App "" "" ""
""   "Food + Platform"
AI   " "
"

<871
" ""
"""" 
 0  1 
 AI  NLP  
" 0  1  "
  AI  
 NLP  AI 

872> 2019 
""
"  AI   AI  AI  "
2018  5 ---- "" AI ""  ""
 AI  AI  SaaS   ToC  ToB 
  NLP   KPI  

<873
Facebook 
   ""
   
 Facebook Facebook "Move Fast and Break Things)"""Facebook "Go Big or Go Home"" X" Facebook 
 Facebook  "  Facebook  "
  
 AI 
 AI 

Q 

874> 2019 
  Geoffrey Hinton  
  
 NLP 80%  20%  
Q ""        """"    15      

<875


Q 
  80   1984  Cyc  Cyc      2000     2012  Google Knowledge Graph    AI   Facebook Q     

876> 2019 
 
  
Q NLP  NLP    NLP    NLP     NLP  23 18 600    NLP  

<877
   NLP  
 AI 
Q AI     AI   AI   AI "" AI   VRAR  AI   5G   ARVR  NLP   AI  20  ToC20  ToB 20  AI   Q AI   ""    

878> 2019 
  JavaPython 
  

Q                        "SIGMOD 2007 Undergraduate Award"        Q   Facebook      

<879 
Q  CEO ·    "" 

"35  35 "" "
" 


880> 2019 
 NLP   Facebook  ""
 "

 AI  NLP   Facebook  Research Scientist  Facebook  NLP Service Facebook    VLDBICDEIJCAICIKM   30  ICDE 2015  ACL 2016 Tutorial"Understanding Short Texts" 3  5  NLP  KG  

  35  
  SIGMOD07 Undergraduate Scholarship

<881









2013  " "  4  

 2002  
 

882> 2019 
"""  1 "
 2005  2009   App ""2013   "" 
"" " " ""
Q 
   
 ""    
 

<883
 "" 
Q    MSN Messenger   Tab   MSN Messenger    MSN Messenger   Tab    Yahoo! Messenger    Q    PC   ""  MRD PRD    

884> 2019 
""   
"" "   "
Q  Yahoo! Messenger  Tab   Sam    Offer PPT Sam  " " Q    
     BAT 

<885
  
" "  Q         """ " PHP   PHP   UED  UED       UI  IE""

886> 2019 
Q     2013  LocalStorage  MemCache    Q   ""    """"        PCB  """ """ 

<887
Q     Q " " ""  20   Q  3 

888> 2019 
 
 30   
2016   
 App  " " ""
 100   30% 
Q     Q  "" ""  "" 

<889
 Q  2000 
 DOS  Pascal "" """"
 PHP  2003  
Q      


2019    KDDCVPRIJCAIWWWGIS  SIGSPATIAL  IROS INFORMS
 ICDAR 2019  
 

<891
 The 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems

Effective Recycling Planning for Dockless Sharing Bikes

Cong Zhang
Beijing Uni. of Posts and Tele. cong1126@bupt.edu.cn

Yanhua Li
Worcester Polytechnic Institute, USA yli15@wpi.edu

Jie Bao
JD Finance baojie@jd.com

Sijie Ruan
Xidian University ruansijie@jd.com

Tianfu He
Harbin Institute of Technology Tianfu.D.He@outlook.com

Hui Lu, Zhihong Tian
Guangzhou University {luhui,tianzhihong}@gzhu.edu.cn

Cong Liu
The University of Texas at Dallas cong@utdallas.edu

Chao Tian
Tencent astortian@tencent.com

Jianfeng Lin, Xianen Li
Mobike {linjianfeng,lixianen}@mobike.com

ABSTRACT
Bike-sharing systems become more and more popular in the urban transportation system, because of their convenience in recent years. However, due to the high daily usage and lack of effective maintenance, the number of bikes in good condition decreases significantly, and vast piles of broken bikes appear in many big cities. As a result, it is more difficult for regular users to get a working bike, which causes problems both economically and environmentally. Therefore, building an effective broken bike prediction and recycling model becomes a crucial task to promote cycling behavior. In this paper, we propose a predictive model to detect the broken bikes and recommend an optimal recycling program based on the large scale real-world sharing bike data. We incorporate the realistic constraints to formulate our problem and introduce a flexible objective function to tune the trade-off between the broken probability and recycled numbers of the bikes. Finally, we provide extensive experimental results and case studies to demonstrate the effectiveness of our approach.

1 INTRODUCTION
Bike-sharing system is a popular transportation system in modern cities, as it not only provides an environment friendly choice for short-distance travelling, but also eases the traffic congestion. Currently, there are over 1,000 deployed bike-sharing systems world wide, and more than 300 systems are in the progress of deployment [29]. In recent years, station-less bike-sharing services, like Mobike1, which allow users to pick up and drop off bikes at any locations they want, become more popular.
Due to the sharing nature of the bike-sharing systems, the sharing bikes have much higher broken possibilities compared with private bikes due to the high ridden frequency and open-air parking problem. For example, the bike sharing system in New York saw 3.6 daily rides per bike 2. As a result, as shown in Figure 1(a), thousands of broken station-less sharing bikes are being kept in a bike graveyard.
%
85.1%

CCS CONCEPTS
· Applied computing  Transportation; Forecasting; Transportation; · Information systems  Spatial-temporal systems.

KEYWORDS
bike-sharing systems, predictive model, optimal recycling program
ACM Reference Format: Cong Zhang, Yanhua Li, Jie Bao, Sijie Ruan, Tianfu He, Hui Lu, Zhihong Tian, Cong Liu, Chao Tian, and Jianfeng Lin, Xianen Li. 2019. Effective Recycling Planning for Dockless Sharing Bikes. In SIGSPATIAL '19: 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, November 5­8, 2019, Chicago, Illinois, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3347146.3359340
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6909-1/19/11. . . $15.00 https://doi.org/10.1145/3347146.3359340

14.9%

(a) Bike Graveyards

(b) Mobike Users Malfunction Report

(c) Users Report Fault Bike Review

Figure 1: Issues with broken Sharing Bike.
Since the number of bikes put in the market is limited, without the proper maintenance, the number of bikes in good condition is continuously decreasing. The broken bikes not only cause economic losses to the companies but also lead to environmental pollution. Therefore, an effective bike recycling plan should be conducted. Currently, Mobike develops a broken bike report function in the app, so that the broken bikes can be discovered in a crowdsourcing way. As shown in Figure 1(b), users can report different types of bike problems in the mobile app, so that the company can arrange workers to collect and recycle them.
1 https://en.wikipedia.org/wiki/Mobike 2 https://bit.ly/2T6q5SE

892> 2019 

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA
However, there are three challenges to conduct such a broken bike recycling task:
Inaccurate and Inadequate Labels. Though the broken bike report function can help the company to quickly locate the broken bike, the report cannot be fully trusted. As shown in Figure 1(c), we manually exam the status of the reported broken bikes. Only 85.1% bikes are truly broken. Furthermore, not all of the users are willing to report the broken status of the bikes, as the broken report function is not a required step.
Arbitrary Spatial Distribution. Different from the stationbased systems, the parking location of each individual station-less bike is totally arbitrary, which makes the recycling routes vary from day to day.
Limited Recycle Capacity. Given a set of bikes to be recycled, the worker can only collect the limited number of broken bikes within the working hour. Besides, the capacity of the collecting vehicle is limited, and the worker has to drive back to the recycling site as soon as the vehicle is full of broken bikes.
In this paper, we design a broken bike recycling route planning system for the worker. This system consists of two main modules: 1) broken bike inference, which infers the broken probability of each sharing bike using its inherent characteristics and the user trajectories associated with it; and 2) recycling route planning, which plans multiple closed recycling routes for the worker to conduct in each day.
The contributions of the paper are summarized as follows: (1) We propose a novel broken sharing bike recycling problem, which takes the broken probability, working time constraint, and vehicle capacity into consideration. (2) We build a broken bike inference model using inherent features and trajectory features extracted from the sharing bike so that the status of every single bike can be accurately inferred. (3) We propose a scatter search-based heuristic algorithm for the broken sharing bike recycling problem. (4) Experiments show the recycling efficiency of the broken bikes recommended by scatter search algorithm is 2.5 times that of the regional random search method and 1.5 times that of the Nearest neighbor routing search method. At the same time, the result of the algorithm is twice the efficiency of Mobike employees' broken bikes recycling. The rest of the paper is organized as follows: Section 2 describes the problem and the system overview. Broken sharing bike inference model is discussed in Section 3. Section 4 gives the solution of broken sharing bike recycling routing problem. Experiments and case studies are given in Section 5. Related works are summarized in Section 6. Section 7 concludes the paper.
2 OVERVIEW
In this section, we define the broken prediction and recycling routing problem for Sharing Bike, and outline our solution framework.
2.1 Preliminaries
We define pi as the inferred broken probability of sharing bike bi . In the recycling task, we only consider bikes which are inferred as broken, i.e., pi > 0.5. The bike with high broken probability is preferred to collect in the priority given limited working time.

Cong Zhang and Yanhua Li, et al.

However, the bikes with high broken probability can distribute unevenly in the given region, which introduces large traveling time, and finally leads to less number of broken bike collected. As a result, we define a beneficial score scorei below to characterize the worthiness of collecting a particular bike bi . In the broken bike recycling mission, the dockless sharing bike can be at any location in the city, e.g., hiding in the residential area or close to the road network, where the parking location of collecting vehicles is usually along with the road network. As a result, the distance between them varies significantly, which we define vw (walking speed) and rt (registration time) below to better characterize the individual bike collecting events.

Definition 1. (Beneficial score) scorei captures the overall ben-

efit to recycle bike bi , which characterizes the trade-off between the

broken likelihood and the recycling cost of bi .

pi
scorei =  min p

 1

(1)

where the parameter  represents the trade-off preference on the broken probability pi vs recycling cost. min p is the minimum broken probability over all the bike in the region, which serves as a normalization term.

Each bike bi has a broken probability pi , i.e., the likelihood of being a broken bike. In practice, the trade off when choosing a bike is: If we seek for only bikes of high broken probability pi , we may end up with a small number of bikes collected (less efficient); on the other hand, if we seek for a large number of collected bikes, many bikes collected may not be broken (false positive). The beneficial score defined in definition 1 captures such a trade-off by the parameter . The reason for designing a score function using the exponential function is that the bike with higher broken probability will have a higher score ( > 1). When  is close to 1, the efficiency is highly considered, leading to a large number of collected bikes; on the other hand, when   1 is large, the broken probability pi is highly considered, thus only bikes with high pi will be collected. Especially,  = 1 means that we do not care about the broken probability of the bike, and every broken bike has the same beneficial score. The  is a tunable parameter (chosen by the service operators), which provides them the flexibility between the efficiency (i.e., the number of collected bikes) and the likelihood of the collected bike being broken. From the operator's perspective, there are different objectives under various circumstances, for example, in regions hard to access, the efficiency should be highly considered (i.e., choosing  close to 1), while in areas with bikes densely populated, e.g., downtown, accurately collecting each broken bike is preferred, thus the likelihood of broken bikes needs to be considered more (i.e., choosing a large ). As a result, the beneficial score measures the practical "benefit" of collecting each bike.

Definition 2. (Sub-route) Each closed route, which starts and ends at the collection site s, is considered as a sub-route.

Definition 3. (Time Cost) The time cost of sub-route Rj is com-

posed of the vehicle travelling time between consecutive locations and

the visiting time at each broken bike. Given a sub-route Rj = s  br1  · · ·  brn  s, the time cost Tj is calculated as follows:

n

Tj = Ttr avel (Rj ) + Tvisit (bri ).

(2)

i =1

<893

Effective Recycling Planning for Dockless Sharing Bikes

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA

Let us denote the shortest road network distance between broken bike bi and bj as dist(bi , bj ), and the vehicle driving speed as vd , then the travelling time cost is calculated as follows:

Ttr avel (Rj ) = dist (s, br1 ) +

n-1 i =1

dist

(bri

,

br i +1

)

+

dist

(brn

,

s)

.

vd

(3)

The broken bike visiting time includes the walking time between

the vehicle on the main road and the location of the broken bike,

and the broken bike registration time rt. We denote the walking

speed as vw , and the perpendicular distance of the broken bike bi to the nearest road segment as shi f ti , then the visiting time cost can be represented as

Tvisit (bri ) =

2shi f tri vw

+ rt.

(4)

Problem Definition. Given the road network RN , driving speed

vd , walking speed vw , collection site s, broken bike registration time rt, working hour T , vehicle capacity M, and a broken sharing bike distribution graph G = (V , E). The vertex set V = {b1, b2, · · · , bn }
represents all the broken bikes in the given service region of s, each

of which is associated with a spatial location and a collection score

scorei , and the edge set E denote the road network connectivity of broken bike pairs.

The objective of the broken bike recycling route planning problem

aims to plan multiple traveling routes for the worker, so that the

total score collected is maximized. The recycling route planning

problem fulfills three constraints: (1) each broken bike is collected

at most once; (2) the working time of the personnel is no more than

T ; and (3) the broken bikes collected in each sub-route are no more

than the vehicle capacity M. If we use ij to denote whether the broken bike bi is collected during sub-route Rj , the problem can be formulated as follows:

max
R

i jscorei
bi V Rj R

(5)

str .

i j  1, bi  V

(6)

Rj R

Tj  T

(7)

Rj R

i j  M, Rj  R

(8)

bi V

Such a problem of finding k budget constrained connected com-

ponents with a maximum beneficial score is NP-hard as proven in

Lemma 1 below.

Lemma 1 (NP-difficulty). When time and capacity constrained, collecting broken-sharing-bikes with a maximal beneficial score is NP-hard.

Proof. The broken sharing bikes collection problem is a combination of broken sharing bike vertex selection and determining the shortest path between the selected vertices. As a consequence, We can reduce our problem of collecting broken-sharing-bikes with maximal beneficial score from the Knapsack Problem (KP) and the Travelling Salesperson Problem (TSP), when time and capacity constrained. We can view each broken sharing bike bi  V as an item,

with an item size (i.e., Collecting time cost), and an item profit

(e.g., a beneficial score contribution). The set V of selected broken

sharing bikes is viewed as a knapsack, with a fixed size T (i.e., total

working time constraint). Furthermore, not all broken sharing bike

bi  V have to be path between the

visited in the problem. Determining the shortest

selected

vertices bi




V

will

be

helpful

to

visit

as many vertices as possible in the available time. our goal is to

maximize the total score collected. If a recycling worker with not

enough time and capacity to collect all possible broken sharing

bikes. He knows the number of beneficial scores to expect in each

broken bike and wants to maximize the total beneficial score, while

keeping the total travel time limited to T . Our problem boils down

to an Orienteering Problem problem (OP), which is known to be

NP-complete [41].

Given it is an NP-hard problem, we develop a heuristic-algorithm to tackle the issue.

2.2 System Overview

Broken Bike Inference Broken Probability Inference
Feature Extraction

Recycling Route Planning
Broken Bikes Road Network 90% 92%
85%

Multiple Routes
Route 1

Inherent Features Trajectory Features

Distribution Graph

Route 2

Scatter Search-Based Routing

Route 3

Figure 2: System Overview.
Figure 2 gives an overview of our system, which consists of two main components: (1) Broken Bicycle Inference, which calculates broken probability for each sharing bike, which takes the sharing bike's parameters, e.g., the bike inherent feature, and trajectory features, and outputs the bike broken probability and current status (detailed in Section 3) and (2) Recycling Route Planning component takes the results of the prediction model, the road network data and the recycling of historical data as input. It establishes the distribution graph of the broken bikes (detailed in Problem Definition) and recommends the optimal route for recycling the broken bike (detailed in Section 4).
3 BROKEN BIKE INFERENCE
Due to the fact that there is only a small proportion of sharing bikes reported as broken by the users, and not all of the reported bikes are truly broken, a broken bicycle inference model is required to detect the real broken bikes for the worker to collect. An inference model under the supervised-learning paradigm is used to assign a broken probability to each bicycle. In the later routing algorithm, the bike with high broken possibility is preferred to collect.
The training bike samples are selected as follows: 1) If a bike is reported as broken by Mobike user and the broken status is confirmed by the worker, we regard it as a broken bike sample; 2) If

894> 2019 

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA

0.4

0.3

Broken 0.6

Good
0.15

0.2

0.4

0.1

0.1

0.2

0.5

0

0

0

(m/s)

(m/s)

(a) Difference of speed probability density distributions

Cong Zhang and Yanhua Li, et al.

Broken 0.08

Good

0.06

0.04

0.02

0

(Minute)

(Minute)

(b) Difference of duration probability density distributions

Figure 3: Mobike Trip Characteristics.

a bike is rode repeatedly in a time period (i.e., one month), and the user does not report the status of the bike as a broken, we regard it as a good bike sample.

Feature Extraction. Whether a bike is broken can be inferred mainly from two aspects: 1) inherent features, such as the life time of the bicycle, the number of ridden times, the total duration of cycling, and the number of maintenance; and 2) trajectory features, which include the average travel speed and trip duration distributions. The selected trajectory features are derived from the analytics of Mobike trajectories. As shown in Figure 3(a), the probability of the average riding speed less than 1m/s for the broken bike is much higher than the good bike. This may be because some broken bikes are more cumbersome, the cycling speed will be slower. And from Figure 3(b), the trip duration of the broken bike is much shorter compared with the good one. This may be because the user finds that there is some problem with the bike after scanning the bicycle to ride, thereby terminating the cycling behavior. This phenomenon of user riding helps to determine the state of the sharing bike.
Broken Probability Inference. Since the sharing-bike status takes two values: good or broken (not good), we use a 0-1 valued binary variable y to denote the status outcome, where 1 stands for broken and 0 stands for good. We use pi to denote the broken probability of the bike bi . The probability depends on many factors, such as the trip duration and speed of a bike, etc. Such information can be encoded into a feature vector Xi , which is associated with the inherent features and the trajectory features of sharing bikes. Given extracted feature vector Xi , we can estimate the acceptance probability as: pi = p(y = broken|Xi ). Since then, the broken inference task can be formulated as a typical binary classification problem, and the traditional classification model, such as Logistic Regression [11], can be employed.
4 RECYCLING ROUTE PLANNING
After the broken probability of each bike in the service region of a collection site is obtained, the distribution graph is constructed using bike locations with broken probabilities and the road network data. In this section, we describe the scatter search-based routing algorithm for the broken bike recycling problem using the constructed distribution graph.
In broken bike recycling problem, the instance size is surely beyond the solvability of standard solver, for example, as shown in Figure 4, there are typically hundreds of broken bikes in some regions, and 39 broken sharing bike collection site in Beijing. The collection

(a) Distribution of Broken Sharing Bikes (b) Distribution of Collection Site
Figure 4: Boken Mobike sharing Bike and collection site Distribution in Beijing
site of broken sharing bike need to occupy certain resources, so each collection site has its own service range. The departure and return locations of the workers are the same collection site in the area. If there is no limit to the capacity of the recycling vehicle and there are no restrictions on the working hours of the recycling workers. Our problem of recycling broken bikes with maximal beneficial score can be converted into a problem of recycling all broken sharing bikes and minimizing the overall recycling path, which can be converted into a tsp problem. However, in the case of working hours and the limited capacity of the recovered vehicle. The problem can be described as workers with not enough time and vehicle capacity to collect all possible broken bikes. He knows the beneficial score which is uniquely defined by the practical broken-bike collection problem (detailed in 2) of each broken bikes, and wants to maximize beneficial scores, while with the working hours and vehicle capacity limited.
Main Idea. Due to the capacity limitation M of the recycling vehicle, the worker can only collect the limited number of bikes during one sub-route. The main idea is that during each sub-route, we first try to collect at most M bikes with high broken probabilities (i.e., high beneficial scores), which are spatially close to each other, and then carefully plan the visiting order, so that the traveling time in each sub-route is minimized. We continuously find such sub-route until the working time is used up. The discovery of each sub-route is explained in following three stages: 1) broken bike clustering; 2) sub-route selection; 3) status update.
Stage 1: Broken Bike Clustering. In this stage, the bikes inferred to be broken are clustered using spatial clustering algorithm, e.g., kMeans [18], so that the broken bikes in each cluster are spatially close to each other. The number of clusters k is computed

<895

Effective Recycling Planning for Dockless Sharing Bikes

according to both recycling vehicle capacity M and The total number of broken bikes in the area n. We initialize k as k = round(n/M).
Stage 2: Sub-route Selection. In this stage, the algorithm finds the best sub-route in each cluster, and the best sub-route over all the clusters is selected. The goodness of a sub-route is defined as the beneficial score per time cost. The sub-route selection in each cluster is conducted in an iteratively way following the scatter

search idea. We first select bikes with top M high scores as the

initial bike set to recycle, and then design recycling route for it
using TSP algorithm. Then we randomly replace a bike in the sub-
route with a bike outside the sub-route but inside the cluster, to
check whether there is any improvement. This process is repeated
N times to obtain a stable sub-route in each cluster.
Stage 3: Status Update. In each iteration, the algorithm puts the best sub-route Rj into the final recycling route set R, and updates the working time by subtracting the time spent recycling
broken bikes in Rj and broken bike vertex set V by subtracting the recycled broken bikes in sub-route Rj . The algorithm terminates when working time T is used up, and then returns the recycling route set R as the recommended broken bikes recycling plan.

Algorithm Design. Algorithm 1 gives the pseudo-code of our scatter search-based heuristic algorithm. In each iteration of the Scatter Search stage(Line 2), the algorithm first partition the vertex set of broken bike nodes V into k clusters. The value of K is determined by the number of broken sharing bikes and the capacity of the recycling vehicle. Then, optimal broken sharing bikes collection scheme in the cluster is then selected separately in each independent cluster. When initializing the sub-route set in each cluster, two initialization strategies are employed depending on the value

of a . If the number of broken bikes in the candidate set Si is greater than recycling vehicle capacity M, the initial recovery of
the bicycle is selected using two methods. If tuning parameter 
is equal to 1, the algorithm random select M broken bikes point
in set Si . otherwise, the algorithm select M broken bike in set Si by the probability value of each broken bike as the candidate set
Ci (Line 4-10). After selecting the initial result set Ci in cluster i, we use Function RecyRoute to solve the optimal recycling order of

the broken bike in the result set and calculate the gain of recycling

benefit score i . In the set Si in which the number of each broken

bicycle is larger than the recycling vehicle capacity, Take the broken

bicycle not included in the set Ci which are randomly selected from

the set During

Si to replace random replace the process, we keep track of

a broken the set Ci

bike in and Ci ,

the set Ci . which has

the maximum score gain in the iteration. If the number of broken

bikes in the candidate set Si is less than recycling vehicle capacity M, we just calculate the corresponding beneficial score gain. Select
the best set Ci which has the maximum score gain from all clusters, and puts the best set Rj in recycling route set R base on Function RecyRoute. Then, Ri is removed from broken sharing bikes set V , the remaining working time is updated by subtracting the time cost Ri .time.At the same time, due to the reduction of the number of broken bikes, the number of clusters is also reduced (Line 11- 19).

Finally, when all the working time budget is used up, the algorithm terminates, and broken sharing-bikes recycling route set R
is returned as the recommended broken bike recycling plan.

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA

Algorithm 1 Scatter Search-based Routing Algorithm

Input: Broken sharing-bikes distribution graph G = (V , E), working

time T , parameter  , capacity M , initial number of clusters k and the

maximum number of iterations N . Output: Recycling route set R. 1: while T > 0 do

//Stage 1: Broken Bike Clustering

2: (S1, S2, · · · , Sk )  Kmeans(V , k )

//Stage 2: Sub-route Selection

3: for i  1 to k do

4:

if Si > M then

5:

if  = 1 then

6:

Random select M points in Si as Ci

7:

else

8:

Select the top M of broken probability in Si as Ci

9:

else

10:

Select all point in Si as Ci

11:

Ri , i  RecyRoute(Ci )

12:

for l  1 to N do

13: 14: 15: 16:

Randomly swap bm  Ci by b  iRfi, Ci ii>iRCetihc; yeRRniouteR(iC;i)i  i

Si

- Ci

as Ci

17: j i i

//Stage 3: Status Update
18: R  R  {Rj }; T  T - Rj .t ime; V  V - Rj 19: k  k - 1
20: return R

Function RecyRoute(Ci )

Ri

 TSP(Ci );

i



Ri .scor e Ri .time

return Ri , i

5 EXPERIMENTS
In this section, we conduct extensive experiments to evaluate the effectiveness of our system. We first describe the real dataset used in the paper. Then, we present comparison results with other baseline methods over different values of  and working time constraints. Finally, we present real-world case studies to evaluate our broken bike detection and recycling route planning algorithm.
5.1 Datasets
Road Networks. The road network data in Beijing and Guangzhou, China is collected from Open Street Map 3. Mobike Order Data. Each Mobike order contains a bike ID, a user ID. The dataset used in the paper includes the entire Mobike orders in the City of Beijing and Guangzhou from 01/08/2018 to 12/31/2018. Mobike Recycling Data. Each Mobike recycling record contains a bike ID, a worker ID, the start time and the end time to recycle the bike. The dataset is collected in the City of Beijing, with the time span of 01/06/2017 - 12/31/2018. Mobike Trajectories. Each Mobike trajectory contains a bike ID, a user ID, the time interval of the trajectory, the start/end locations, and a sequence of intermediate GPS points. The dataset includes the
3 https://www.openstreetmap.org/

896> 2019 

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA
entire Mobike trajectory data in the City of Beijing and Guangzhou from 01/08/2018 to 12/31/2018.
5.2 Data Pre-Processing
Data Pre-processing takes the road network, the Mobike order data, Mobike recycling data, and the Mobike trajectories as input, and performs the following three tasks to prepare the data for further processing: Data Cleaning. Data Cleaning cleans the raw order data, trajectories, and recycling data from Mobike. Essentially as a type of crowdsensing data, Mobike trajectories are generated by the GPS modules from mobile phones. As a result, a noticeable portion of trajectories has different data errors, which significantly affect the accuracy of the broken bike inference model. This step cleans the raw trajectories from Mobike users by filtering the noisy GPS points with a heuristic-based outlier detection method [43]. Map-Matching. In this module, we map the GPS points onto the corresponding segments in road networks, which is crucial for the broken sharing bike collection. The Mobike sharing bike can be at any location in the city, e.g., hiding in the residential area or close to the road network, where the parking location of collecting vehicles is usually along with the road network. As a result, we should employ vw (walking speed) to better characterize the individual bike collecting events. This step evaluates the distance of each broken sharing bikes to the nearest corresponding segments in road networks with a global map matching method [28]. Map Griding. For the ease of assessing the regional rt (registration time), we adopt the griding based method, which simply partitions the map into equal side-length grids [23, 24]. our approach divides the urban area into equal-size grids with a pre-defined side-lengths in 100 meters.
5.3 Effectiveness Evaluation
In this subsection, we study the effectiveness of both broken bike prediction and recycling. Unless mentioned otherwise, the default parameters used in the experiments are: recycling vehicle capacity M = 20, the average speed of the worker's walking is vw = 1m/s, and the average speed of the worker's driving is vd = 25km/h.
5.3.1 Broken Bike Prediction. In the broken bike prediction model, we tried two popular models:
logistic regression (LR) [11] and random forest (RF) [2] algorithms. We train the models for different cities and evaluate both methods in terms of Accuracy (ACC) and Area under the Curve of ROC (AUC). Experimental results for Beijing and Guangzhou are shown in Table 1, where we observe that 1) LR outperforms RF slightly and 2) both models get good results, which validates the effectiveness of our feature extraction scheme.
5.3.2 Performance of Different TSP Methods in Recycling Route Planning.
We study the effect of different TSP methods in our recycling route planning. The test data select from Haidian District, Beijing, which the inference model give 537 broken bikes in this area as shown in Figure 7. In this work, we tried five popular models: Simulated Annealing Algorithms (SA) [13], Genetic Algorithms

Cong Zhang and Yanhua Li, et al.

Table 1: Results of LR and RF

Model LR RF
Model LR RF

Beijing ACC AUC Recall 0.9768 0.9965 0.9763 0.9750 0.9934 0.9746
Guangzhou ACC AUC Recall 0.9757 0.9948 0.9759 0.9746 0.9933 0.9745

F-score 0.97796 0.97608
F-score 0.9756 0.9750



 

!"



 #$

%






























Figure 5: The Evaluation of Different TSP Methods.
(GA) [31], Ant Colony Optimizations Algorithms (ACO)[9], LinKernighan(LK)[14] and Self-organizing Feature Maps (SOFM)[3].
We evaluate five methods in terms of the total beneficial scores in our recycling broken sharing bike problem. Experimental results for Beijing are shown in Figure 5. Our experiments show that for our test data, these TSP algorithms do not make a significant difference as well. SA is slightly more accurate. Therefore, we choose SA as the TSP method in our recycling route planning model.
5.3.3 Recycling Route Planning. We study the effect of different parameter settings of  and
working time, and we compare our method, i.e. Scatter Searchbased Routing (SSR), with two other baselines.
· Baseline 1: Random selection (RS). If there is no inference model in the collection problem, we assume that workers collect broken bikes according to user reports which occur randomly. We directly take the next car after each collection of a broken bike for collection. When the number of broken bikes collected reaches the vehicle capacity, return to the broken bike station in the area and repeat the random collection process for the next round. The collection process terminates when the total collection time exceeds the working time.
· Baseline 2: Nearest neighbor routing (NNR). The location where the broken bike is relatively densely distributed is selected as the starting area for collecting the first broken bike. In NNR, the recycling vehicle starts at the recycling parking spot, repeatedly visits the nearest broken bikes node until the capacity of the vehicle and the working hours of the workers exceed the constraint and returns back to the parking spot.

<897

Effective Recycling Planning for Dockless Sharing Bikes

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA

Beneficial Score Beneficial Score Number of Broken Bike

Work Time(hour) (a) Results with ¢=1

Work Time(hour) (b) Results with ¢=2

Work Time(hour) (c) Result of Recycling of Different ¢

Figure 6: Effectiveness Evaluation.
is not quite sensitive to it and the bike moves within a certain range. This has resulted in a higher recovery cost for workers. The SSR method can adjust the recovery of the beneficial score by controlling the parameter , which can solve the phenomenon and make the overall recovery more effective as shown in Figure 6(b). Figure 6(c) provides the results with different  settings, It is interesting that, when  is large, the number of recycling broken sharing bikes will be reduced to some extent. Moreover, with a higher , the number of recycling broken bikes is smaller, but the degree of reduction will gradually decrease. The reason behind these phenomena is that a bicycle with a higher probability of failure prediction has a higher score. Where the value of  is larger, and it is more preferable to collect a bicycle which has higher broken probability when collecting broken sharing bikes. However, the distance between bikes also affects the time cost of recycling, so when the value of a is larger, the number of broken bikes collected will become smaller.

Region A

Region B

Figure 7: Broken sharing bike distribution cluster in Haidian District.
Effects on Total Working Time Budget. Figure 6 illustrates the total beneficial scores with different total working time budgets for a worker with Mobike, from 1 Hour to 8 Hour. The experimental results of finding a broken bike based on a random walk of RS is the average value of the income score after the algorithm solves the problem 1000 times independently. From the figure, we make the following observations: 1) the scatter search-based heuristic SSR method performs better than other baseline models. 2) When working hours are between 5 hours and 6 hours, the NNR method will have a useful period of slow growth. This is because, during this time, the NNR method took a long time in a broken bicycle. It is interesting that, because of the slight damage to the bike, the user

Collection Site
Figure 8: A Real Case Study in Haidian District, Beijing.
5.4 Case Studies
To better understand the effectiveness of our bike prediction and recycling model, we conduct a field case study. We choose to visit the area near Zhichun Road, Dazhongsi, and Beitucheng subway station in Haidian District, Beijing.
Figure 8 gives the path that Mobike operators use to recycle broken bikes in this area. The workers recovered a total of 32 broken bikes in the vicinity in 8 hours, and mainly concentrated near the temporary parking spots. The traditional recycling methods of worker are similar to the NNS method. The worker first finds the area where the broken bikes are densely distributed near the temporary parking point through the location reported by bikes. Then,

898> 2019 

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA

a broken bike in the dense area is selected and recycled according to the scheme of the shortest travel distance of the map navigation. When the target bicycle is found, another broken bike which is closest to the current location is selected for recycling. However, this will make recyclers tend to pay more attention to broken bikes that are closer to temporary parking spots. The distribution of broken bikes in the area is not fully considered, resulting in low efficiency and high cost of recycling. Figure 9 shows the results of the broken recovery path recommended by the SSR method. Mobike's worker recycles the broken bikes in a given area three times and collect 59 broken bikes in an 8 hour working time limit. It is interesting that, the recommended recovery path of the algorithm is not only the broken bikes concentrated near the temporary parking point, but also the broken bikes far from the temporary parking point are also included in the recommended collection of recycling. This is because the algorithm parameters take into account of the overall distribution of the bikes and the optimal recovery sequence in the actual recycling process, and the parameters are tuned according to the efficiency gain of each bike recovery. This makes the recycling of bikes more efficient. At the same time, the two broken bikes at the bottom left of Figure 9 are the broken bikes that are seriously broken in the recovery. One is the chain is broken, and the other is the seat is lost. The confidence of the two cars in the model is 0.99988294 and 0.99961954 respectively. These two bikes belong to the broken bike that is preferentially collected when the value of  is greater than one. This shows that the model is sensitive to bike with a relatively high degree of failure.

Region A

Region B

Collection Site

Broken Chain

Region C

Broken Seat
Figure 9: A Real Case Study Base on Scatter Search Model Result in Haidian District, Beijing.
6 RELATED WORK
The research of fault sharing bikes recycling can be summarized in two main areas: 1) Urban Crowd Sourcing, and 2) Route Planning. Urban Crowd Sourcing. Essentially, we take advantage of the massive Mobike users in a city to perform the fault bike detection task. Similar problems are addressed with the crowdsourcing techniques[19, 26]. For example, The literature [42] quantifies the fragility of cities through detecting the delay in commuting activities using GPS data collected from smartphones. The literatures [34, 36] infer noise levels for locations by smartphone users. The literature [27] proposes a bike sharing network optimization approach by extracting fine-grained discriminative features from

Cong Zhang and Yanhua Li, et al.
human mobility data, point of interests (POI), as well as station network structures. The literatures [7, 15] identify potholes or classify road quality from vehicle's accelerometer data. Differing from the above works, we focus on the problem of broken sharing bikes detection and collecting path planning.
Route Planning. The fault sharing bike recycling problem is related to the multiple Traveling Salesman Problem (mTSP) [1, 38] and orienteering Problem (OP) [4, 39, 41]. mTSP and OP can be considered as a relaxation of our problem, with the capacity or working time restrictions removed. The solutions for these two problems are primarily in two fold: 1) optimal algorithms and 2) heuristic algorithms. In literatures [21, 35], the authors use branchand-bound to solve instances with less than 20 and 150 vertices, respectively. The authors in [22] use a cutting plane method to obtain better upper bounds. In literatures [10, 12], the authors propose branch-and-cut algorithms. However, the branch-and-cut procedure with instances up to 500 vertices cannot be performed. GAs are relatively stochastic search algorithms based on evolutionary biology and computer science principles [16]. Using GAs to the mTSP problem have several representations, like one chromosome technique [33], the two chromosome technique [32] and the latest two-part chromosome technique. The authors in [25] propose an ant colony optimization approach and a tabu search algorithm. In literature [37], the authors develop a Pareto ant colony optimization algorithm and a multi-objective variable neighborhood search algorithm. In [40], the authors propose a Variable Neighbourhood Search (VNS) algorithm and embed an exact algorithm to deal with a path feasibility subproblem. In [20], the authors present two polynomial size formulations for OP. The authors in [30] discuss several vehicle routing algorithms, and present a heuristic method which searches over a solution space formed by the large number of feasible solutions to an mTSP. The authors in [17] study the adaptive stochastic knapsack problem with deterministic size and stochastic rewards. Their problem objective is to find a sequential inserting policy to maximize the probability of the reward exceeding a threshold value without violating the capacity constraint. In [5], the authors study the adaptive stochastic knapsack problem with items of deterministic reward and stochastic size. Their goal is to maximize expected value while fitting all the items in the knapsack. The authors demonstrate the benefit of an adaptive policy and provide an approximation approach. In [6], the authors study an orienteering problem with stochastic travel times and present adaptive path planning methods to take advantage of dynamically updating data; combine the orienteering problem and optimal path finding into a single model. The authors in [8] discuss the vehicle routing problem with hard time windows and stochastic service times (VRPTW-ST). They adopt the dynamic programming algorithm to account for the probabilistic resource consumption by extending the label dimension and by providing new dominance rules. In this paper two recourse strategies are proposed and the resulting problems are solved by branch-price-and-cut algorithms. However, all of these works cannot be directly used for broken sharing bikes recycling, because these works simply test on benchmark instances and fail to consider the realistic constraints and road network distance.

<899

Effective Recycling Planning for Dockless Sharing Bikes
7 CONCLUSION
In this paper, we introduce a novel approach to detect broken sharing bikes and recommend the appropriate bicycle recycling path to the worker based on the real sharing bikes data collected from Mobike (a major station-less bike sharing system). Our system can address the problem of recycling efficiency of broken sharing bikes in a more realistic fashion, considering the constraints and requirements from sharing bike worker's perspective: 1) working time limitations, 2) vehicle capacity constraints, and 3) broken sharing bike recovery benefit. We also propose a flexible beneficial score function to adjust preferences between the number of bikes recovered and the predicted probability of damage to bikes. The formulated problem is proven to be NP-hard, thus we propose a scatter search-based heuristic algorithm. We perform extensive experiments on a large scale Mobike data and demonstrate the effectiveness of our proposed broken sharing bike predict model and bike recycling routing model, where our model can predict the broken sharing bikes with above 97% accuracy and recommends that the number of real broken bikes recovered by the recycling path of the broken bikes is two to three times that of the Mobike traditionally recycling broken bikes.
REFERENCES
[1] Tolga Bektas. 2006. The multiple traveling salesman problem: an overview of formulations and solution procedures. Omega 34, 3 (2006), 209­219.
[2] Leo Breiman. 2001. Random forests. Machine learning 45, 1 (2001), 5­32. [3] Lukasz Brocki and Danijel Korzinek. 2007. Kohonen self-organizing map for
the traveling salesperson problem. In Recent Advances in Mechatronics. Springer, 116­119. [4] I-Ming Chao, Bruce L Golden, and Edward A Wasil. 1996. The team orienteering problem. European journal of operational research 88, 3 (1996), 464­474. [5] Brian C Dean, Michel X Goemans, and Jan Vondrdk. 2004. Approximating the stochastic knapsack problem: The benefit of adaptivity. In 45th Annual IEEE Symposium on Foundations of Computer Science. IEEE, 208­217. [6] Irina Dolinskaya, Zhenyu Edwin Shi, and Karen Smilowitz. 2018. Adaptive orienteering problem with stochastic travel times. Transportation Research Part E: Logistics and Transportation Review 109 (2018), 1­19. [7] Jakob Eriksson, Lewis Girod, Bret Hull, Ryan Newton, Samuel Madden, and Hari Balakrishnan. 2008. The pothole patrol: using a mobile sensor network for road surface monitoring. In Proceedings of the 6th international conference on Mobile systems, applications, and services. ACM, 29­39. [8] Fausto Errico, Guy Desaulniers, Michel Gendreau, Walter Rei, and L-M Rousseau. 2018. The vehicle routing problem with hard time windows and stochastic service times. EURO Journal on Transportation and Logistics 7, 3 (2018), 223­251. [9] Jose B Escario, Juan F Jimenez, and Jose M Giron-Sierra. 2015. Ant colony extended: experiments on the travelling salesman problem. Expert Systems with Applications 42, 1 (2015), 390­410. [10] Matteo Fischetti, Juan Jose Salazar Gonzalez, and Paolo Toth. 1998. Solving the orienteering problem through branch-and-cut. INFORMS Journal on Computing 10, 2 (1998), 133­148. [11] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 2001. The elements of statistical learning. Vol. 1. Springer series in statistics New York. [12] Michel Gendreau, Gilbert Laporte, and Frederic Semet. 1998. A branch-and-cut algorithm for the undirected selective traveling salesman problem. Networks: An International Journal 32, 4 (1998), 263­273. [13] Xiutang Geng, Zhihua Chen, Wei Yang, Deqian Shi, and Kai Zhao. 2011. Solving the traveling salesman problem based on an adaptive simulated annealing algorithm with greedy search. Applied Soft Computing 11, 4 (2011), 3680­3689. [14] Keld Helsgaun. 2009. General k-opt submoves for the Lin­Kernighan TSP heuristic. Mathematical Programming Computation 1, 2-3 (2009), 119­163. [15] Marius Hoffmann, Michael Mock, and Michael May. 2013. Road-quality classification and bump detection with bicycle-mounted smartphones. In Proceedings of the 3rd International Conference on Ubiquitous Data Mining-Volume 1088. CEUR-WS. org, 39­43. [16] John Holland. 1975. Adaptation in natural and artificial systems: an introductory analysis with application to biology. Control and artificial intelligence (1975). [17] Taylan lhan, Seyed MR Iravani, and Mark S Daskin. 2011. The adaptive knapsack problem with stochastic rewards. Operations research 59, 1 (2011), 242­248.

SIGSPATIAL '19, November 5­8, 2019, Chicago, Illinois, USA
[18] Anil K Jain. 2010. Data clustering: 50 years beyond K-means. Pattern recognition letters 31, 8 (2010), 651­666.
[19] Shenggong Ji, Yu Zheng, and Tianrui Li. 2016. Urban Sensing Based on Human Mobility. UbiComp 2016. https://www.microsoft.com/en-us/research/publication/ urban- sensing- based- human- mobility/
[20] Imdat Kara, Papatya Sevgin Bicakci, and Tusan Derya. 2016. New formulations for the orienteering problem. Procedia Economics and Finance 39 (2016), 849­854.
[21] Gilbert Laporte and Silvano Martello. 1990. The selective travelling salesman problem. Discrete applied mathematics 26, 2-3 (1990), 193­207.
[22] Adrienne C Leifer and Moshe B Rosenwein. 1994. Strong linear programming relaxations for the orienteering problem. European Journal of Operational Research 73, 3 (1994), 517­523.
[23] Yanhua Li, Jun Luo, Chi-Yin Chow, Kam-Lam Chan, Ye Ding, and Fan Zhang. 2015. Growing the charging station network for electric vehicles with trajectory data analytics. In 2015 IEEE 31st International Conference on Data Engineering. IEEE, 1376­1387.
[24] Yanhua Li, Moritz Steiner, Jie Bao, Limin Wang, and Ting Zhu. 2014. Region sampling and estimation of geosocial data with dynamic range calibration. In 2014 IEEE 30th International Conference on Data Engineering. IEEE, 1096­1107.
[25] Yun-Chia Liang, Sadan Kulturel-Konak, and Alice E Smith. 2002. Meta heuristics for the orienteering problem. In Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No. 02TH8600), Vol. 1. IEEE, 384­389.
[26] Dongyu Liu, Di Weng, Yuhong Li, Jie Bao, Yu Zheng, Huamin Qu, and Yingcai Wu. 2016. Smartadp: Visual analytics of large-scale taxi trajectories for selecting billboard locations. IEEE transactions on visualization and computer graphics 23, 1 (2016), 1­10.
[27] J. Liu, Q. Li, M. Qu, W. Chen, J. Yang, H. Xiong, H. Zhong, and Y. Fu. 2015. Station Site Optimization in Bike Sharing Systems. In 2015 IEEE International Conference on Data Mining. 883­888. https://doi.org/10.1109/ICDM.2015.99
[28] Yin Lou, Chengyang Zhang, Yu Zheng, Xing Xie, Wei Wang, and Yan Huang. 2009. Map-matching for low-sampling-rate GPS trajectories. In Proceedings of the 17th ACM SIGSPATIAL international conference on advances in geographic information systems. ACM, 352­361.
[29] Russell Meddin and Paul DeMaio. 2015. The bike-sharing world map. (2015). URLhttp://www.bikesharingworld.com
[30] RH Mole, DG Johnson, and K Wells. 1983. Combinatorial analysis for route first-cluster second vehicle routing. Omega 11, 5 (1983), 507­512.
[31] Yuichi Nagata and Shigenobu Kobayashi. 2013. A powerful genetic algorithm using edge assembly crossover for the traveling salesman problem. INFORMS Journal on Computing 25, 2 (2013), 346­363.
[32] Yang-Byung Park. 2001. A hybrid genetic algorithm for the vehicle scheduling problem with due times and time deadlines. International Journal of Production Economics 73, 2 (2001), 175­188.
[33] Jean-Yves Potvin, Guy Lapalme, and Jean-Marc Rousseau. 1989. A generalized k-opt exchange procedure for the MTSP. INFOR: Information Systems and Operational Research 27, 4 (1989), 474­481.
[34] Zhaokun Qin and Yanmin Zhu. 2016. NoiseSense: A crowd sensing system for urban noise mapping service. In 2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS). IEEE, 80­87.
[35] R Ramesh, Yong-Seok Yoon, and Mark H Karwan. 1992. An optimal algorithm for the orienteering tour problem. ORSA Journal on Computing 4, 2 (1992), 155­165.
[36] Rajib Kumar Rana, Chun Tung Chou, Salil S Kanhere, Nirupama Bulusu, and Wen Hu. 2010. Ear-phone: an end-to-end participatory urban noise mapping system. In Proceedings of the 9th ACM/IEEE international conference on information processing in sensor networks. ACM, 105­116.
[37] Michael Schilde, Karl F Doerner, Richard F Hartl, and Guenter Kiechle. 2009. Metaheuristics for the bi-objective orienteering problem. Swarm Intelligence 3, 3 (2009), 179­201.
[38] Joseph A Svestka and Vaughn E Huckfeldt. 1973. Computational experience with an m-salesman traveling salesman algorithm. Management Science 19, 7 (1973), 790­799.
[39] Tommy Thomadsen and Thomas K Stidsen. 2003. The quadratic selective travelling salesman problem. (2003).
[40] Fabien Tricoire, Martin Romauch, Karl F Doerner, and Richard F Hartl. 2010. Heuristics for the multi-period orienteering problem with multiple time windows. Computers & Operations Research 37, 2 (2010), 351­367.
[41] Pieter Vansteenwegen, Wouter Souffriau, and Dirk Van Oudheusden. 2011. The orienteering problem: A survey. European Journal of Operational Research 209, 1 (2011), 1­10.
[42] Takahiro Yabe, Kota Tsubouchi, and Yoshihide Sekimoto. 2017. CityFlowFragility: Measuring the Fragility of People Flow in Cities to Disasters using GPS Data Collected from Smartphones. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 117.
[43] Yu Zheng. 2015. Trajectory data mining: an overview. ACM Transactions on Intelligent Systems and Technology (TIST) 6, 3 (2015), 29.

900> 2019 
2019 IEEE/RSJ International Conference on Intelligent Robots and Systems

StarNet: Pedestrian Trajectory Prediction using Deep Neural Network in Star Topology

Yanliang Zhu, Deheng Qian, Dongchun Ren, Huaxia Xia

Abstract-- Pedestrian trajectory prediction is crucial for many important applications. This problem is a great challenge because of complicated interactions among pedestrians. Previous methods model only the pairwise interactions between pedestrians, which not only oversimplifies the interactions among pedestrians but also is computationally inefficient. In this paper, we propose a novel model StarNet to deal with these issues. StarNet has a star topology which includes a unique hub network and multiple host networks. The hub network takes observed trajectories of all pedestrians to produce a comprehensive description of the interpersonal interactions. Then the host networks, each of which corresponds to one pedestrian, consult the description and predict future trajectories. The star topology gives StarNet two advantages over conventional models. First, StarNet is able to consider the collective influence among all pedestrians in the hub network, making more accurate predictions. Second, StarNet is computationally efficient since the number of host network is linear to the number of pedestrians. Experiments on multiple public datasets demonstrate that StarNet outperforms multiple state-of-the-arts by a large margin in terms of both accuracy and efficiency.

I. INTRODUCTION
Pedestrian trajectory prediction is an important task in autonomous driving [1], [2], [3] and mobile robot applications [4], [5], [6]. This task allows an intelligent agent, e.g., a selfdriving car or a mobile robot, to foresee the future positions of pedestrians. Depending on such predictions, the agent can move in a safe and smooth route.
However, pedestrian trajectory prediction is a great challenge due to the intrinsic uncertainty of pedestrians' future positions. In a crowded scene, each pedestrian dynamically changes his/her walking speed and direction, partly attributed to his/her interactions with surrounding pedestrians.
To make an accurate prediction, existing algorithms focus on making full use of the interactions between pedestrians. Early works model the interactions [7], [8], [9], [10] by handcrafted features. Social Force [7] models several force terms to predict human behaviors. The approach in [8] constructs an energy grid map to describe the interactions in crowded scenes. However, their performances are limited by the quality of manually designed features. Recently, data-driven methods have demonstrated their powerful performance [11], [12], [13], [14]. For instance, Social LSTM [11] considers interactions among pedestrians close to each other. Social

*This work was supported by the Meituan-Dianping Group. Yanliang Zhu, Deheng Qian, Dongchun Ren and Xia are with the Meituan-Dianping Group, Beijing, zhuyanliang@meituan.com

Huaxia China.

Fig. 1: The structure of StarNet. StarNet mainly consists a centralized hub network and several host networks. The hub network collects movement information and generates a feature which describes joint interactions among pedestrians. Each host network, corresponding to a certain pedestrian, queries the hub network and predicts the pedestrian's trajectory.
GAN [13] models interactions among all pedestrians. Social Attention [14] captures spatio-temporal interactions.
Previous methods have achieved great success in trajectory prediction. However, all these methods assume that the complicated interactions among pedestrians can be decomposed into pairwise interactions. This assumption neglects the collective influence among pedestrians in the real world. Thus previous methods tend to fail in complicated scenes. In the meanwhile, the number of pairwise interactions increases quadratically as the number of pedestrians increases. Hence, existing methods are computationally inefficient.
In this paper, we propose a new deep neural network, StarNet, to model complicated interactions among all pedestrians together. As shown in Figure 1, StarNet has a star topology, and hence the name. The central part of StarNet is the hub network, which produces a representation r of the interactions among pedestrians. To be specific, the hub network takes the observed trajectories of all pedestrians and produces a comprehensive spatio-temporal representation r of all interactions in the crowd. Then, r is sent to each host network. Each host network predicts one pedestrian's trajectory. Specifically, depending on r, each host network exploits an efficient method to calculate the pedestrian's

<901

interactions with others. Then, the host network predicts one pedestrian's trajectory depending on his/her interactions with others, as well as his/her observed trajectory.
StarNet has two advantages over previous methods. First, the representation r is able to describe not only pairwise interactions but also collective ones. Such a comprehensive representation enables StarNet to make accurate predictions. Second, the interactions between one pedestrian and others are efficiently computed. When predicting all pedestrians' trajectories, the computational time increases linearly, rather than quadratically, as the number of pedestrians increases. Consequently, StarNet outperforms multiple state-of-the-arts in terms of both accuracy and computational efficiency.
Our contributions are two-folded. First, we propose to describe collective interactions among pedestrians, which results in more accurate predictions. Second, we devise an interesting topology of the network to take advantage of the representation r, leading to computational efficiency.
The rest of this paper is organized as follows: Section II briefly reviews related work on pedestrian trajectory prediction. Section III formalizes the problem and elaborates our method. Section IV compares StarNet with state-of-the-arts on multiple public datasets. Section V draws our conclusion.
II. RELATED WORK
Our work mainly focuses on human path prediction. In this section, we give a brief review of recent researches on this domain.
Pedestrian path prediction is a great challenge due to the uncertainty of future movements [7], [8], [10], [11], [13], [14], [15]. Conventional methods tackle this problem with manually crafted features. Social Force [7] extracts force terms, including self-properties and attractive effects, to model human behaviors. Another approach [8] constructs an energy map to indicate the traffic capacity of each area in the scene, and uses a fast matching algorithm to generate a walking path. Mixture model of Dynamic pedestrian-Agents (MDA) [10] learns the behavioral patterns by modeling dynamic interactions and pedestrian beliefs. However, all these methods can hardly capture complicated interactions in crowded scenes, due to the limitation of hand-crafted features.
Data-driven methods remove the requirement of handcrafted features, and greatly improve the ability to predict pedestrian trajectories. Some attempts [11], [13], [14], [26], [27] receive pedestrian positions and predict determined trajectories. Social LSTM [11] devises social pooling to deal with interpersonal interactions. Social LSTM divides pedestrian's surrounding area into grids, and computes pairwise interactions between pedestrians in a grid. Compared with Social LSTM, other approaches [13], [15] eliminate the limitation on a fixed area. Social GAN [13] combines Generative Adversarial Networks (GANs) [16] with LSTMbased encoder-decoder architecture, and sample plausible

trajectories from a distribution. Social Attention [14] estimates multiple Gaussian distributions of future positions, then generates candidate trajectories through Mixture Density Network (MDN) [17].
However, existing methods compute pairwise features, and thus oversimplified the interactions in the real word environment. Meanwhile, they suffer from a huge computational burden in crowded scenes. In contrast, our proposed StarNet with novel architecture is capable of capturing joint interactions over all pedestrians, which is more accurate and efficient.
III. APPROACH
In this section, we first describe the formulation of the pedestrian prediction problem. Then we provide the details of our proposed method.
A. Problem Formulation
We assume the number of pedestrians is N. The number of observed time steps is Tobs. And the number of time steps to be predicted is Tpred. For the i-th pedestrian, his/her observed trajectory is denoted as Oi = {pti | t = 1, 2, · · · , Tobs}, where pti represents his/her coordinates at time step t. Similarly, the future trajectory of ground truth is denoted as Fi =
pti | t = Tobs + 1, Tobs + 2, · · · , Tobs + Tpred . Given such notations, our goal is to build a fast and ptaoocethcde{uerFrsatiwr}teioNia=rmn1ds.os,,dWbwealeesteotdermyporptneoldotifihycnet ditarhaeodbffeuusenetpurcvrtineeodetnuratrmrjaaelajceptncopetriotinwergiseofs{rrkFo{,imO}wNii={}h1ONii=coi1h}f.Nia=Iiln1sl called StarNet, to embody this function. Specifically, StarNet consists of two novel parts, i.e., a hub network and N host networks. The hub network computes a representation r of the crowd. Then, each host network predicts the future trajectory of one pedestrian depending on the pedestrian's observed trajectory and r. We first describe the hub network and then present host networks.
B. The hub network
The hub network takes all of the observed trajectories simultaneously and produces a comprehensive representation r of the crowd of pedestrians. The representation r includes both spatial and temporal information of the crowd, which is the key to describe the interactions among pedestrians.
Note that our algorithm should be invariant against isometric transformation (translation and rotation) of the pedestrians' coordinates. The invariance against rotation is achieved by randomly rotate our training data during the training process. While the invariance against translation is guaranteed by calculating a translation invariant representation r.
As shown in Figure 2, the hub network produces r by two steps. First, the hub network produces a spatial representation of the crowd for each time step. The spatial representation is invariant against the translation of the coordinates. Then, the spatial representation is fed into a LSTM to produce the spatio-temporal representation r.

902> 2019 

Fig. 2: The process of predicting the coordinates. At time step t, StarNet takes the newly observed (or predicted) coordinates

{pti}Ni=1 (or {pti}Ni=1) and outputs the predicted coordinates

pti+1

N i=1

.

1) Spatial representation: In the first step, in order to make the representation invariant against translation, the hub network preprocesses the coordinates of pedestrians by subtracting the central coordinates of all pedestrians at time step Tobs from every coordinate.

 pti



pti

-

1 N

N
pTnobs .
n=1

(1)

Thus, the centralized coordinates are invariant against translation. Such coordinates of each pedestrian are mapped into a new space using an embedding function  (·) with parameters W1,

eti =

 (pti;W1) , i f t  [1, Tobs] ,  (pti;W1) , i f t  Tobs + 1, Tobs + Tpred ,

(2)

where pti is the predicted position of the i-th pedestrian at time step t. eti is the spatial representation of the i-th pedestrian's trajectory at time step t. The embedding function
is defined as:

 (x;W ) W x.

(3)

Then, we use a maxpooling operation to combine the spatial representation of all pedestrians, obtaining the spatial representation of the crowd at time step t,

st = MaxPooling et1, et2, · · · , etN ,

(4)

Spatial representation st contains information of the crowd at a single time step. However, pedestrians interact with each other dynamically. To improve the accuracy of predictions, a spatio-temporal representation is required.
2) Spatio-temporal representation: In the second step, the hub network feeds a set of spatial representations
s1, s2, · · · , sTobs of sequential time steps into a LSTM. Then, the LSTM combines all the spatial representations in its hidden state. Thus, the hidden state of the LSTM is a spatio-temporal representation rt of all pedestrians.

Specifically, we 

can

calculate

rt

as

follows:



h0c = 0, et =  (st ;W2) ,



[otc, htc] = LST M rt =  (otc;W4) ,

htc-1, et ;W3

,

(5)

where W2 and W4 are the embedding weights, W3 is the weight of LSTM. otc and htc are the output and hidden state
of the LSTM respectively. Note that, rt depends on the observed trajectories of

all pedestrians. Hence, our algorithm is able to consider

complicated interactions among multiple pedestrians. This

property allows our algorithm to produce accurate predictions. Meanwhile, rt is able to be obtained in a single forward

propagation of the hub network at each time step. In other

words, the time complexity of computing interactions among

pedestrians is linear to the number of pedestrians N. This

property allows our algorithm to be computationally effi-

cient. By contrast, conventional algorithms compute pairwise

interactions, leading to oversimplification of the interactions

among pedestrians. Also, the number of pairwise interactions

increases quadratically as N increases.

C. The host networks

The spatio-temporal representation rt is then employed by

host networks. For the i-th pedestrian, the host network first

embeds the observed trajectory Oi, and then combines the
embedded trajectory with the spatio-temporal representation rt , predicting the future trajectory. Specifically, the host

network predicts the future trajectory by two steps.

First, the host network takes the observed trajectory Oi and the spatio-temporal representation rt as input and generates an integrated representation qti,

qti =

rt  (pti;W5) , i f t  [1, Tobs] , rt  (pti;W5) , i f t  Tobs + 1, Tobs + Tpred ,
(6)

where W5 is the embedding weight, and denotes the point-wise multiplication. qti depends on both the trajectory

<903

of the i-th pedestrian and the interactions between the i-th

pedestrian and others in the crowd.

Second, the host network predicts the future trajectory of

the i-th pedestrian depending on the observed trajectory Oi and the integrated representation qti. To encourage the host network to produce non-deterministic predictions, a random

noise z, which is sampled from a Gaussian distribution with

mean 0 and variance 1, is concatenated to the input of the

host network. Specifically, the host network encodes the

observed trajectory Oi with

 

dp0i = 0, dpti-1 = pti - pti-1,

the

hidden

state

hTeiobs ,

i.e.,



[otei, htei] = LST ME t  [1, Tobs],

hte-i 1,

qti, dpti-1

;W6

,

(7)

where LST ME (·) with weight W6 denotes the encoding

procedure. Then, the host network proceeds with

 

otdi, htdi dpti = 

= LST MD otdi;W8 ,

htd-i 1,

qti, dpti-1, z

;W7

,



pti = pti-1 + dpti, t  [Tobs + 1, Tobs + Tpred ],

(8)

where LST MD(·) with weight W7 is the decoding function.

W8 is the embedding weight of the output layer. And the

initial states are set according to,



 hTdoibs = hTeiobs ,



pTi obs = pTi obs , dpTi obs = pTi obs - piTobs-1.

(9)

D. Implementation Details

The network configuration of StarNet is detailed in TABLE I.

TABLE I: Network Configuration of AstoridNet

Weight W1 W2 W3 W4 W5 W6 W7 W8

Weight Dimension 64x2 64x64
64x32, 32x1(bias) 32x64 64x2
64x66, 64x1(bias) 64x74, 64x1(bias)
2x64

We train the proposed StarNet with the loss function
applied in [13]. Specifically, at the training stage, StarNet
produces multiple predicted trajectories for each pedestrian. Each predicted trajectory {Fik}Kk=1 has a distance to the ground truth trajectory Fi. Only the smallest distance is minimized. Mathematically, the loss function is,

  L

=

1 N Tpred

minKk=1

N Tobs+Tpred j=1 t=Tobs+1

ptjk - ptj

2
,

(10)

where K is the number of sampled trajectories. This loss function improves the training speed and stability. Moreover, we employ an Adam optimizer and set the learning rate to 0.0001.
In practice, all host networks share the same weights, since pedestrians in a scenario have the same behavioral patterns, such as variable-speed movement, sharp turning and so on. In our approach, we use shared weights to learn the aforementioned behavioral patterns. Each host network contains specific LSTM state which captures certain pedestrian's behavior, and predicts the pedestrian's future trajectory. The observed trajectories of all pedestrians form a batch, which is fed into one single implementation of the host network. In this way, the prediction for all pedestrians is able to be obtained in a single forward propagation.
IV. EXPERIMENTS
We evaluate our model on two human crowded trajectory datasets: ETH [24] and UCY [25]. These datasets have 5 sets with 4 different scenes. In these scenes, there exist challenging interactions, such as walking side by side, collision avoidance and changing directions. Following the settings in [11], [13], [14], we train our model on 4 sets and test on the remaining one.
We compare our StarNet with three state-of-the-arts including Social LSTM, Social GAN and Social Attention. Besides, we test the basic LSTM-based encoder-decoder model, which does not consider the interactions among pedestrians, as a baseline.
Following [11], [13], [14], we compare these methods in terms of the Average Displacement Error (ADE) and Final Displacement Error (FDE). The ADE is defined as the mean Euclidean distance between predicted coordinates and the ground truth. Specifically, all methods output 8 coordinates uniformly sampled from the predicted trajectory. Then the distance between such 8 points with the ground truth is accumulated as the ADE. The FDE is the distance between the final point of the predicted trajectory and the final point of the ground truth. All these methods are trained with the loss Eq. (10) to deal with multimodal distribution during evaluation. Besides, we compare the computational time of all these methods. All experiments are conducted on the same computational platform with an NVIDIA Tesla V100 GPU.
A. Experimental Results
1) Accuracy: As shown in TABLE II, StarNet outperforms the others in most cases. A possible explanation is that StarNet considers the collective influence among pedestrians all together to make more accurate predictions. In comparison, other state-of-the-arts only model the pairwise interactions between pedestrians.
Interestingly, we notice that the test datasets include multiple senses. In these scenes, StarNet has the smallest variances of ADE and FDE, which means that StarNet is robust against the changes of scenes.

904> 2019 

Metric
ADE
Average ADE Variance of ADE
FDE
Average FDE Variance of FDE

Dataset ZARA-1 ZARA-2
UNIV ETH HOTEL
-
ZARA-1 ZARA-2
UNIV ETH HOTEL
-

Metric Inference Time (Seconds) Number of Paramters (Kilo)

TABLE II: Comparison of Prediction Errors

LSTM 0.25 0.31 0.36 0.70 0.55 0.43 0.028
0.53 0.65 0.77 1.45 1.17 0.91 0.118

Social LSTM 0.27 0.33 0.41 0.73 0.49 0.45 0.026
0.56 0.70 0.84 1.48 1.01 0.91 0.101

Social GAN 0.21 0.27 0.36 0.61 0.48 0.39 0.021
0.42 0.54 0.75 1.22 0.95 0.78 0.802

Social Attention 1.66 2.30 2.92 2.45 2.19 2.30 0.166
2.64 4.75 5.95 5.78 4.94 4.81 1.394

TABLE III: Comparison of Computational Time

LSTM 0.029 22.87

Social LSTM 0.504 156.06

Social GAN 0.202 108.03

Social Attention 3.714 874.95

StarNet (Ours) 0.25 0.26 0.21 0.31 0.46 0.30 0.008 0.47 0.53 0.40 0.54 0.91 0.57 0.031
StarNet (Ours) 0.073 31.90

(a) Scene 1

(b) Scene 2

(c) Scene 3

(d) Scene 4

Fig. 3: Predicted trajectories and the corresponding ground truths. Different colors indicate different trajectories. The trajectories of ground truth are labeled with dots. The predicted trajectories are labeled with triangles.

To assess StarNet qualitatively, we illustrate the prediction results in 4 scenes, as shown in Figure 3. In each scene, the left sub-figure presents the observed trajectories and the

predicted trajectories of all pedestrians. The right sub-figure shows the trajectories of ground truth.
We can observe that StarNet could handle complicated

<905

interactions among pedestrians. Most predicted trajectories accurately reflect the pedestrians' movements and have no collisions with other trajectories. However, there are some failure cases due to the multimodal distribution of future trajectories. For example, in 3(c), the predictions for the blue and green trajectories fail to match the ground truth. We argue that although these predicted trajectories do not match the ground truth, these trajectories are still plausible in crowded scenes.
2) Computational time cost: When deployed in mobile robots and autonomous vehicles, the prediction algorithm needs to be invoked with a high frequency. Hence the computational time of the prediction algorithm is a crucial property.
As shown in TABLE III, the basic LSTM model is the fastest model since the model takes no interactions among pedestrians into consideration. StarNet is the second fastest model. Specifically, StarNet is 51 times faster than Social Attention, 7 times faster than Social LSTM, and 3 times faster than Social GAN. Meanwhile, the number of parameters employed by StarNet is less than state-of-the-arts by a large margin. StarNet is computationally efficient since the interpersonal interactions among pedestrians are computed in a single forward propagation, as discussed in Section II.
V. CONCLUSION
In this paper, we propose StarNet, which has a star topology, for pedestrian trajectory prediction. StarNet learns complicated interpersonal interactions and predicts future trajectories with low time complexity. We apply a centralized hub network to model the spatio-temporal interactions among pedestrians. Then the host network takes full advantage of the spatio-temporal representation and predicts pedestrians' trajectories. We demonstrate that StarNet outperforms stateof-the-arts in multiple experiments.
REFERENCES
[1] D. Ferguson, D. Michael, U. Chris and K. Sascha, "Detection, prediction, and avoidance of dynamic obstacles in urban environments," in 2008 IEEE International Conference on Intelligent Vehicles Symposium (IVS). IEEE, 2008, pp. 1149-1154.
[2] Y. Luo, P. Cai, A. Bera, D. Hsu, W. S. Lee and D. Manocha, "Porca: Modeling and planning for autonomous driving among many pedestrians," IEEE Robotics and Automation Letters, vol. 3, no. 4, pp. 3418-3425, 2018.
[3] F. Large, D. Vasquez, T. Fraichard and C. Laugier, "Avoiding cars and pedestrians using velocity obstacles and motion prediction," in 2004 IEEE International Conference on Intelligent Vehicles Symposium (IVS). IEEE, 2004, pp. 375-379.
[4] B. D. Ziebart, N. Ratliff, G. Gallagher, C. Mertz, K. Peterson, J. A. Bagnell, M. Hebert, A. K. Dey and S. Srinivasa, "Planningbased prediction for pedestrians," in 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2009, pp. 3931-3936.
[5] P. Trautman and A. Krause, "Unfreezing the robot: Navigation in dense, interacting crowds," in 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2010, pp. 797803.
[6] N. E. D. Toit and J. W. Burdick, "Robot motion planning in dynamic, uncertain environments," IEEE Transactions on Robotics, vol. 28, no. 1, pp. 101-115, 2012.

[7] D. Helbing and P. Molnar, "Social force model for pedestrian dynamics," Physical review E, vol. 51, no. 5, pp. 4282, 1995.
[8] S. Yi, H. Li and X. Wang, "Understanding pedestrian behaviors from stationary crowd groups," in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2015, pp. 3488-3496.
[9] S. Yi, H. Li and X. Wang, "Pedestrian behavior modeling from stationary crowds with applications to intelligent surveillance," IEEE transactions on image processing, vol. 25, no. 9, pp. 4354-4368, 2016.
[10] B. Zhou, X. Wang and X. Tang, "Understanding collective crowd behaviors: Learning a mixture model of dynamic pedestrian-agents," in 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2012, pp. 2871-2878.
[11] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, F. Li and S. Savarese, "Social lstm: Human trajectory prediction in crowded spaces," in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE 2016, pp. 961-971.
[12] H. Wu, Z. Chen, W. Sun, B. Zheng and W. Wang, "Modeling trajectories with recurrent neural networks," in 28th International Joint Conference on Artificial Intelligence (IJCAI). 2017, pp. 3083-3090.
[13] A. Gupta, J. Johnson, F. Li, S. Savarese and A. Alahi, "Social GAN: Socially acceptable trajectories with generative adversarial networks," in 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018, pp. 2255-2264.
[14] A. Vemula, K. Muelling and J. Oh, "Social attention: Modeling attention in human crowds," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1-7.
[15] Y. Xu, Z. Piao and S. Gao S, "Encoding crowd interaction with deep neural network for pPedestrian trajectory prediction," in 2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018, pp. 5275-5284.
[16] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio, "Generative adversarial nets," in 28th Conference on Neural Information Processing Systems (NIPS). 2014, pp. 2672-2680.
[17] C. M. Bishop, "Mixture density networks," Technical Report NCRG/4288, Aston University, Birmingham, UK, 1994.
[18] D. Ha and D. Eck, "A neural representation of sketch drawings," arXiv preprint arXiv:1704.03477, 2017.
[19] E. Schmerling, K. Leung, W. Vollprecht and M. Pavone, "Multimodal probabilistic model-based planning for human-robot interaction," in 2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1-9.
[20] K. Cho, B. V. Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk and Y. Bengio, "Learning phrase representations using RNN encoder-decoder for statistical machine translation," arXiv preprint arXiv:1406.1078, 2014.
[21] K. Cho, B. V. Merrienboer, D. Bahdanau and Y. Bengoi, "On the properties of neural machine translation: Encoder-decoder approaches," arXiv preprint arXiv:1409.1259, 2014.
[22] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel and Y. Bengio, "End-to-end attention-based large vocabulary speech recognition," in 2015 International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4945-4949.
[23] C. R. Qi, H. Su, K and J. G. Leonidas, "Pointnet: Deep learning on point sets for 3d classification and segmentation," in 2017 Conference on Computer Vision and Pattern Recognition (CVPR), IEEE, 2017, pp. 652-660.
[24] S. Pellegrini, A. Ess, K. Schindler and L. V. Gool, "You'll never walk alone: Modeling social behavior for multi-target tracking," in 2009 IEEE International Conference on Computer Vision (ICCV). IEEE, 2009, pp. 261-268.
[25] A. Lerner, Y. Chrysanthou and D. Lischinski, "Crowds by example," Computer Graphics Forum, vol. 26, no. 3, pp. 655-664, 2007.
[26] D. Varshneya, G. Srinivasaraghavan, "Human trajectory prediction using spatially aware deep attention models," arXiv preprint arXiv:1705.09436, 2017.
[27] T. Fernando, S. Denma, S. Sridharan and C. Fookes, "Soft+hardwired attention: An lstm framework for human trajectory prediction and abnormal event detection," arXiv preprint arXiv:1702.05552, 2017.

906> 2019 



2019 INFORMS Annual Meeting

2019 INFORMS Annual Meeting Seattle, WA, October 20-23, 2019
c 2019 INFORMS | ISBN 978-0-9906153-1-6 https://doi.org/10.1287/infp.2019.XXXX pp. 000­000

A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

Huanyu Zheng, Shengyao Wang, Ying Cha, Feng Guo, Jinghua Hao, Renqing He, Zhizhao Sun
Meituan-Dianping Group, No. 4 Wangjing East Street, Chaoyang District, Beijing, China, {zhenghuanyu, wangshengyao, chaying, guofeng13, haojinghua, herenqing, sunzhizhao}@meituan.com

Abstract

Online food-delivery platforms are expanding choice, allowing customers to order from a wide variety of restaurants. As an industrial level technology, route planning algorithm is required to be fast enough. This paper proposes a two-stage fast heuristic for route planning, which solves the problem at millisecond level. To speed up the algorithm, we further utilize geographic information so that invalid search attempts are prevented. Finally, we compare our algorithm with brute-force algorithm and several state-of-the-art algorithms to show its effectiveness and efficiency.

Keywords food delivery, pickup and delivery, time windows, heuristics

1. Introduction
"Tap, order, and eat, all at home." Food ordering and delivery is a fast growing market all over the world. Worldwide, food ordering and delivery companies, such as Grubhub, Uber Eats, and Just Eat, are developing a $94 billion online food ordering and delivery business [4]. In China, over 300 billion customers order food from Meituan-Dianping food delivery platform with more than 3.6 million restaurants to choose, exceeding 24 million daily orders and deliveries in 2018 [9].
As shown in Figure 1, after a customer orders a meal, the food delivery platform pushes the order to a restaurant. At the same time, the platform dispatches this order to a driver, and plans a route for him. The dispatching system is shown in Figure 2. A real route is shown in Figure 3, in which the driver is planned to pick up four meals before delivering two of the meals to corresponding customers. Then, the driver pick up the last meal and deliver the rest meals to customers. At rush hours like lunch time, a driver may run with 10 orders for example, which means that he has to scurry across 10 restaurants and 10 customers. Under this setting, there are 2.38 × 1015 ways of route planning, which is hard to solve in limited time.
In this paper, we introduce an algorithm to find a satisfactory route, aiming to minimize delays and the route length. The algorithm is required to be highly efficient because it is an essential part of dispatching algorithm. Different from traditional logistics, food delivery dispatching algorithm is an online algorithm, which matches thousands of orders with thousands of drivers each time. Thus, route planning algorithm has to plan a route within milliseconds.
1

<907

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

2

2019 INFORMS Annual Meeting, c 2019 INFORMS

Figure 1. Food ordering and delivery process.

Figure 2. Dispatching algorithm.
Figure 3. A real route.
2. Literature Review
Food delivery route planning problem is rarely studied in the literature. The most relevant literature is the Pickup and Delivery Problem with Time Windows (PDPTW), which models

908> 2019 

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

2019 INFORMS Annual Meeting, c 2019 INFORMS

3

Figure 4. A typical route.

a fleet of vehicles serving a collection of transportation requests. Each request specifies a pickup location and a delivery location. Vehicles are routed to serve all requests, optimizing a certain objective function such as total distance traveled, with precedence constraint and capacity constraint.
Problem in this paper can be modeled as the single vehicle PDPTW, which considers only one vehicle and several customers. [14] propose a variable-depth search algorithm, which produces near optimal solutions most of the time, but may end up with an infeasible solution. Then, they have to spend a large computation time with simulated annealing. [5] present a genetic algorithm, a simulated annealing algorithm and a hill climbing approach. They find simulated annealing algorithm is superior to other algorithms but requires longer running time.
A mathematical formulation of the PDPTW involving a single depot is given in [2]. [13] further formulate a general version of the PDPTW, and present a survey of the problem. In the past two decades, several exact algorithms and heuristics are designed to solve the PDPTW. Exact algorithms include column generation [2], branch-and-cut [7], and branchand-cut-and-price [11, 1]. Heuristics include tabu search [10], insertion-based heuristic [8], adaptive large neighborhood search [12] and simulated annealing [15].
The PDPTW is applied to problems arising in logistics and public transit, such as transporting goods [2] and home health care [6]. The problems are usually in large scale and are acceptable to be solved in hours. However, food delivery route planning problem demands very low computational complexity. As a core algorithm supporting the food delivery platform, the time complexity of route planning algorithm is limited in only several milliseconds. In other words, algorithms proposed by papers above are not applicable to our problem. In this paper, we propose a new heuristic approach for PDPTW problem to meet this restrict time complexity requirement.

3. Algorithm
In this section, we present our Two-Stage Fast Heuristic (TSFH), including initialization and local search strategies. Figure 4 shows a typical route, where ti is the Estimated Time of Arrival (ETA) [3] of a delivery point, which is shown to customer and restaurant as soon as the order generates. Ti is estimated time of driver's action, calculated by our route planning algorithm, and di is traveling distance from previous point to i-th point. Then we have Formula 1 presenting our objective, which minimizes delays and route length.

n

min [max(Ti - ti, 0) + di]

(1)

i=1

The problem has two constraints. (1) precedence constraint: A driver has to pick up meals before delivery. (2) capacity constraint: The total capacity of a driver is limited during the route.

<909

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

4

2019 INFORMS Annual Meeting, c 2019 INFORMS

Figure 5. Possible ways of insertion.

The TSFH has two stages. Stage I initializes one feasible solution with greedy search mechanism. To speed up the insertion, we utilize geographic knowledge to avoid invalid search. Stage II adjusts the solution with two local search strategies.
3.1. Stage I: Initialization
3.1.1. Initialization with Greedy Insertion The initialization stage can be described as follows. First, we sort orders according to their ETA. Then, we plan the first order. Due to precedence constraint, the only plan is to plan pickup point before delivery point. For the following orders, their pickup and delivery points are inserted into the route according to the objective. For instance, as shown in Figure 5, we have 6 ways to insert the second order to the route. We greedily insert pickup point and delivery point to minimize delays and route length. According to this criterion, Figure 5(a) is the optimal way to insert. After all points are inserted, we formulate a feasible route.
3.1.2. Speeding up with Geographic Information In China, restaurants are geographically close to each other. For example, restaurants at a central business district building may serve 60% customers within 5 kilometers. Also, customers are geographically close, most customers are gathered in certain communities. Thus, we can cluster pickup and delivery points by hierarchical clustering. With these clusters, we speed up the initialization stage by reducing "bad" insertion attempts.
Clustering algorithm is described as follows, where D is a given range, say 100 meters. For each point i, if it is not classified, it generates a new group and makes itself a center point. For each point j, if it not classified, it is classified into group i if dij < D; if it is classified to group k and is not a center point, then if dij < dkj , we reclassify it into group i.
Lemma 1 (Insertion Before Own Group). If point j is classified to group i, then inserting point j to groups before group i is worse than inserting point j to group i.
Proof. Point j is classified to group i. Inserting point j to group k before group i must be worse than directly inserting it to group i, because the route length is longer, but no delivery point benefits from shorter delays.
Figure 6 gives an example illustrating the above lemma. Pickup and delivery points are classified into three groups. Consider insertion of `green' pickup and delivery points, given `blue' points and `orange' points inserted beforehand. We can see from Figure 6, inserting a point before its own group (Figure 6 (b)) is always worse than inserting it to its own group (Figure 6 (a)), as the driver travels more and customers may suffer from more delays.

910> 2019 

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

2019 INFORMS Annual Meeting, c 2019 INFORMS

5

Figure 6. Insertion before own group.

Figure 7. Insertion after own group.

Lemma 2 (Insertion After Own Group). If point j is classified to group i, then we can always find an insertion better than the insertion of point j to group k after group i.
Proof. Point j is classified to group i. Inserting point j to group k after group i must be worse than inserting it between group k and group k + 1 (if group k is the last group, then inserting it to the end), because the route length is longer, but no delivery point benefits from shorter delays.
Figure 7 gives an example illustrating the above lemma. Pickup and delivery points are classified into three groups. Consider insertion of `blue' pickup and delivery points, given `green' points and `orange' points inserted beforehand. We can see from Figure 7, inserting a point between a group after its own group (Figure 7 (b)) is always worse than inserting it to the last position (Figure 7 (a)), as the driver travels more and customers may suffer from more delays.
From Lemma 1 and Lemma 2, we conclude that point j is only worth to insert when it is inserted into its group, say group i, or between groups after group i. This conclusion reduces invalid insertion attempts and speeds up the algorithm.
3.2. Stage II: Local Search
Figure 8 shows the local search stage. After initialization stage, the local search improves the route by looking for better solutions at solution neighborhood. Our algorithm considers two kinds of neighborhood. First, we find delivery points with most delays and move them backward to an optimal position, which is shown in Figure 8 (A). Second, we find delivery points with most sufficient time and move them forward to an optimal position, which is shown in Figure 8 (B). Each time we find a better solution, the best solution is replaced.

<911

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

6

2019 INFORMS Annual Meeting, c 2019 INFORMS

Figure 8. Local search.

4. Numerical Results and Experiments
In this section, we provide numerical experiments to compare different algorithms. First, we compare initialization without speeding up technique and fast initialization with speeding up technique to show the effectiveness of initialization stage. Then, we use brute-force search to find the optimal solution to each problem as a baseline. Moreover, we compare our TSFH algorithm with the optimal solution to show our TSFH can generate near optimal solutions in limited time. Moreover, the TSFH is compared with several state-of-the-art algorithms to show its effectiveness and efficiency.
Instances are randomly sampled from real route planning problems. We partition the instance set into three sets, `< 10', `10 to 20', and `> 20', according to the number of pickup and delivery points.
In the following tables, we show performance of algorithms by total score and average time. Total score is the sum of delays (in minutes) and route length (in kilometers). Average time is the computation time of one instance (in milliseconds).
All experiments are run on a MacBook Pro with 2.2 GHz processors / 16 GB RAM in Mac-OS. The algorithms are coded in Java using Eclipse.
4.1. Comparison of Initialization and Fast Initialization
To show the effectiveness of initialization and local search, and efficiency of speeding up technique, we compare initialization and fast initialization using geographic information.
From Table 1, we can see that initialization can be sped up considering geographic information. For example, the average running time of the `10 to 20' instance set is reduced from 0.37ms to 0.21ms, which is 43.2% faster. Moreover, effectiveness of initialization is not harmed by speeding up as total score is nearly the same. Therefore, we have shown that our algorithm can generate near optimal solutions within several milliseconds.
4.2. Optimal Solutions by Brute-force Algorithm
The optimal solution can be solved using a brute-force search algorithm. Consider a driver with n pickup tasks and n delivery tasks, the complexity of brute-force search algorithm can be derived as follows. A solution from 2n points' full permutation can only be feasible when

912> 2019 

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

2019 INFORMS Annual Meeting, c 2019 INFORMS

7

Table 1. Numerical results of initialization and fast initialization.

Instance

Algorithm

Total Score Delays Route Length Average Time

> 20

Initialization

240.88 232.17

8.71

2.60

> 20 Fast Initialization 242.48 233.74

8.73

1.08

10 to 20 Initialization

46.29

41.35

4.94

0.37

10 to 20 Fast Initialization 46.11

41.19

4.93

0.21

< 10

Initialization

28.59

24.09

4.50

0.14

< 10 Fast Initialization 28.67

24.18

4.50

0.10

Table 2. Numerical results of our TSFH and brute-force algorithm.

Instance Algorithm Total Score Delays Route Length Average Time

> 20

TSFH

223.30 214.79

8.51

7.08

10 to 20 TSFH

42.48 37.65

4.83

1.01

< 10

TSFH

27.35 22.94

4.40

< 10 Brute-force 27.17 22.79

4.38

0.41 199.31

Table 3. Numerical results of our TSFH and the state-of-the-art algorithms.

Instance Algorithm Total Score Delays Route Length Average Time

> 20

TSFH

223.30 214.79

8.51

> 20 VDS [14] 877.80 869.63

8.18

> 20

SA [5]

1195.50 1185.74

9.75

7.08 25.96 64.71

10 to 20 TSFH

42.48

37.65

4.83

10 to 20 VDS [14]

77.34

72.66

4.68

10 to 20 SA [5]

101.02 96.18

4.84

1.01 6.23 35.59

< 10

TSFH

27.35

22.94

4.40

< 10 VDS [14]

35.17

30.95

4.23

< 10

SA [5]

36.87

32.62

4.24

< 10 Brute-force 27.17

22.79

4.38

0.41 3.51 25.64 199.31

each pair of pickup and delivery points follows precedence constraint. Thus, the complexity (2n)!
is O 2n , if capacity constraint is not considered. As the complexity grows dramatically with the number of pickup or delivery points, we only provide results of `< 10' instance sets.
4.3. Comparison of Our TSFH and Brute-force Algorithm In this subsection, we compare the TSFH with brute-force algorithm, which generates optimal solutions to each instance. Table 2 shows the results. From `< 10' instance set, we can find that our full algorithm uses only 0.21% of average time but generates nearly the same performance than brute-force algorithm. We can also see that the average computation time of the TSFH is limited in milliseconds in `10 to 20' and `< 10' instances. As most instances generated in real life have less than 20 pickup and delivery points, we show the TSFH is efficient enough to solve food delivery route planning problem within several milliseconds.

<913

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

8

2019 INFORMS Annual Meeting, c 2019 INFORMS

4.4. Comparison of Our TSFH and the State-of-the-art Algorithms
In this subsection, we compare the best results of our TSFH to those of variable-depth search (VDS) [14] and simulated annealing (SA) [5]. The comparative results are listed in Table 3. We can see that the TSFH outperforms VDS and SA in terms of total score and average running time. Particularly, in `< 10' instances, the TSFH reaches near optimal solutions, which is 0.7% above optimal, while VDS and SA are 29.4% and 35.7% above optimal respectively. Moreover, the TSFH also solves `< 10' instances in 0.41ms per instance, while VDS and SA solve those instances in 3.51ms and 25.64ms, respectively. Real life food delivery route planning demands high running speed. The TSFH can solve all `< 10' instances in 1 millisecond, while the computation time of VDS and SA is not acceptable. Therefore, we conclude that the TSFH is more effective and efficient than the state-of-art algorithms, and more suitable to be applied to real life problem solving.

5. Conclusions
In this paper, food delivery route planning problem is modeled as the single vehicle pickup and delivery problem with time windows (PDPTW). Unlike traditional approaches solving the single vehicle PDPTW, our paper presents a two-stage fast heuristic (TSFH). In stage I, a feasible and near optimal solution is generated in a greedy heuristic way. With restaurants and customers location cluster information, our algorithm is accelerated by avoiding bad searching attempts. In stage II, we improve the solution in stage I by exploiting two neighborhoods. From our numerical results, we represent the effectiveness and efficiency by comparing our results with those from brute-force algorithm and some best algorithms in the literature. Our TSFH algorithm generates near optimal solutions within milliseconds.

References
[1] Baldacci R, Bartolini E, Mingozzi A (2011) An exact algorithm for the pickup and delivery problem with time windows. Operations Research 59(2):414­426.
[2] Dumas Y, Desrosiers J, Soumis F (1991) The pickup and delivery problem with time windows. European Journal of Operational Research 54(1):7­22.
[3] Fang Z, Huang L, Wierman A (2017) Prices and subsidies in the sharing economy. Proceedings of the 26th International Conference on World Wide Web, 53­62 (International World Wide Web Conferences Steering Committee).
[4] Hirschberg C, Rajko A, Schumacher T, Wrulich M (2016) The changing market for food delivery. https://www.mckinsey.com/industries/high-tech/our-insights/ the-changing-market-for-food-delivery.
[5] Hosny MI, Mumford CL (2010) The single vehicle pickup and delivery problem with time windows: intelligent operators for heuristic and metaheuristic algorithms. Journal of Heuristics 16(3):417­439.
[6] Liu R, Xie X, Augusto V, Rodriguez C (2013) Heuristic algorithms for a vehicle routing problem with simultaneous delivery and pickup and time windows in home health care. European Journal of Operational Research 230(3):475­486.
[7] Lu Q, Dessouky MM (2004) An exact algorithm for the multiple vehicle pickup and delivery problem. Transportation Science 38(4):503­514.
[8] Lu Q, Dessouky MM (2006) A new insertion-based construction heuristic for solving the pickup and delivery problem with time windows. European Journal of Operational Research 175(2):672­687.
[9] Meituan-Dianping Group (2019) Meituan to invest rmb 11 billion to support merchant development. https://www.prnewswire.com/news-releases/ meituan-to-invest-rmb11-billion-to-support-merchant-development-300782802. html.

914> 2019 

Zheng et al.: A Two-Stage Fast Heuristic for Food Delivery Route Planning Problem

2019 INFORMS Annual Meeting, c 2019 INFORMS

9

[10] Nanry WP, Barnes JW (2000) Solving the pickup and delivery problem with time windows using reactive tabu search. Transportation Research Part B: Methodological 34(2):107­121.
[11] Ropke S, Cordeau JF (2009) Branch and cut and price for the pickup and delivery problem with time windows. Transportation Science 43(3):267­286.
[12] Ropke S, Pisinger D (2006) An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation science 40(4):455­472.
[13] Savelsbergh MW, Sol M (1995) The general pickup and delivery problem. Transportation Science 29(1):17­29.
[14] Van der Bruggen L, Lenstra JK, Schuur P (1993) Variable-depth search for the single-vehicle pickup and delivery problem with time windows. Transportation Science 27(3):298­311.
[15] Wang C, Mu D, Zhao F, Sutherland JW (2015) A parallel simulated annealing method for the vehicle routing problem with simultaneous pickup­delivery and time windows. Computers & Industrial Engineering 83:111­122.

<915
 2019 International Joint Conference on Artificial Intelligence

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

Earlier Attention? Aspect-Aware LSTM for Aspect-Based Sentiment Analysis
Bowen Xing1 , Lejian Liao1 , Dandan Song1  , Jingang Wang 2 , Fuzhen Zhang2 , Zhongyuan Wang2 and Heyan Huang1
1Lab of High Volume language Information Processing & Cloud Computing Beijing Lab of Intelligent Information Technology
School of Computer Science & Technology, Beijing Institute of Technology 2Meituan-Dianping Group
{xingbowen,liaolj,sdd,hhy63}@bit.edu.cn, {wangjingang02,zhangfuzheng,wangzhongyuan02}@meituan.com

Abstract
Aspect-based sentiment analysis (ABSA) aims to predict fine-grained sentiments of comments with respect to given aspect terms or categories. In previous ABSA methods, the importance of aspect has been realized and verified. Most existing LSTMbased models take aspect into account via the attention mechanism, where the attention weights are calculated after the context is modeled in the form of contextual vectors. However, aspect-related information may be already discarded and aspectirrelevant information may be retained in classic LSTM cells in the context modeling process, which can be improved to generate more effective context representations. This paper proposes a novel variant of LSTM, termed as aspect-aware LSTM (AALSTM), which incorporates aspect information into LSTM cells in the context modeling stage before the attention mechanism. Therefore, our AALSTM can dynamically produce aspect-aware contextual representations. We experiment with several representative LSTM-based models by replacing the classic LSTM cells with the AA-LSTM cells. Experimental results on SemEval-2014 Datasets demonstrate the effectiveness of AA-LSTM.
1 Introduction
With increasing numbers of comments on the Internet, sentiment analysis is attracting interests from both research and industry. Aspect-based sentiment analysis is a fundamental and challenging task in sentiment analysis, which aims to infer the sentiment polarity of sentences with respect to given aspects. For example, "Great salad but the soup tastes bad". It's obvious that the opinion over the `salad' is positive while the opinion over the `soup' is negative. In this case, aspects are included in the comments, and predicting aspect sentiment polarities of this kind of comments is termed aspect term sentiment analysis (ATSA) or target sentiment analysis
This work was partially done during Bowen's internship at Meituan-Dianping Group.
Corresponding author.

(TSA). There is another case where the aspect is not explicitly included in the comment. For example, "Although the dinner is expensive, waiters are so warm-hearted!". We can observe that there are two aspect categories mentioned in this comment: price and service with completely opposite sentiment polarities. Predicting aspect sentiment polarities of this kind of comments is termed aspect category sentiment analysis (ACSA), and the aspect categories usually belong to a predefined set. In this paper, we collectively refer aspect category, aspect term/target as aspect. And our goal is aspect-based sentiment analysis (ABSA) including ATSA and ACSA.
As deep learning have been successfully exploited in various NLP tasks [Zhen et al., 2017; Yang and Mitchell, 2017; Xu et al., 2017; Devamanyu et al., 2018], many neural networks have been applied to ABSA. With the ability of handling long-term dependencies, Long Short-Term Memory neural network (LSTM) [Hochreiter and Schmidhuber, 1997] is widely used for context modeling in ABSA, and many recent best performing ABSA methods are based on LSTM because of its significant performance [Tang et al., 2016a; Wang et al., 2016; Chen et al., 2017; Ma et al., 2017; Devamanyu et al., 2018; Xin et al., 2018]. Current mainstream LSTM-based ABSA models adopt LSTM to model the context, obtaining hidden state vectors for each token in the input sequence. After obtaining contextual vector representations, they utilize attention mechanism to produce the attention weight vector.
Recent well performing LSTM-based ABSA models can be divided into three categories according to their way of modeling context: (1) "Attention-based LSTM with aspect embedding (ATAE-LSTM)" [Wang et al., 2016] and "modeling inter-aspect dependencies by LSTM (IAD-LSTM)" [Devamanyu et al., 2018] model the context and aspect together via concatenating the aspect vector to the word embeddings of context words in the embedding layer. (2) "Interactive attention networks (IAN)" [Ma et al., 2017] and "aspect fusion LSTM (AF-LSTM)" [Tay et al., 2018] model the context alone and utilize the aspect to compute context's attention vector in the attention mechanism. (3) "Recurrent attention network on memory (RAM)" [Chen et al., 2017] introduces relative position information of context words and the given target into their hidden state vectors.

5313

916> 2019 

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

The first category conducts simple joint modeling of contexts and aspects. In the secondary category, on behalf of most of the existing LSTM-based methods, the models only use context words as input when modeling the context, so they get the same context hidden states vectors when analysing comments containing multiple aspects. The second category models the context separately while utilizing the aspect information in context's attention calculation. And the method in the third category additionally multiplies a relative position weight. However, no aspect information is considered in the LSTM cells of all the above methods.Therefore, after context modeling, their hidden state vectors contain the information that is important to the "overall" comment semantics. This is determined by the functionality of classic LSTM. It retains important information and filters out useless information at the sentence-level semantic, in the hidden states corresponding to every context word.
In contrast, for the aspect-based sentiment analysis task, we think the context modeling should be aspect-aware. For a specific aspect, on one hand, some of the semantic information of the whole sentence is useless. These aspect irrelevant information would adversely harm the final sentiment representation, especially in the situation where multiple aspects exist in one comment. This is because when LSTM encounters an important token for the overall sentence semantics, this token's information is retained in every follow-up hidden state. Consequently, even if a good attention vector is produced via the attention mechanism, these hidden state vectors also contain useless information which is magnified to some extent. On the other hand, information that is important to the aspect may be not sufficiently kept in hidden states because of their small contribution to the overall semantic information of the sentence.
We take two typical examples to illustrate the two issues. First, "The salad is so delicious but the soup is unsatisfied.". There are two aspects (salad and soup) of opposite sentiment polarity. When judging the sentiment polarity of the `soup', the word `delicious' which modifies `salad' is also important to the sentence-level semantics of the whole comment, and its information is preserved in the hidden states vectors of subsequent context words, including `unsatisfied'. So even if `unsatisfied' is assigned a large weight in the attention vector, the information of `delicious' will still be integrated into the final context representation and enlarged. Second, "Pizza is wonderful compared to the last time we enjoyed at another place, and the beef is not bad, by the way." Obviously, this sentence is mainly about pizza so classic LSTM will retain a lot of information that modifies `pizza' when modeling context. But when judging the polarity of beef, because traditional LSTM does not know the aspect is `beef', much-retained `pizza' information causes that the information of `beef' is not valued enough in hidden state vectors. We define the above issues as the aspect-unaware problem in the context modeling process of current methods. To the best of our knowledge, this is the first time to propose this problem.
In this paper, we propose a novel LSTM variant termed aspect-aware LSTM (AA-LSTM) to introduce the aspect into context modeling process. In every time step, on one hand, the aspect vector can select key information in the context ac-

cording to the aspect and keep the important information in context words' hidden states. On the other hand, the vector formed aspect information can influent the process of context modeling and filter useless information for the given aspect. So AA-LSTM can generate more effective context hidden states based on the given aspect. This can be seen as an earlier attention operation on context words. It is worth mentioning that though our AA-LSTM model takes the aspect as input, it does not actually fuse the aspect vector into the representation of the context, but only utilize the aspect to influence the process of modeling context via controlling information flow.
The main contributions of our work can be summarized as follows:
· We propose a novel LSTM variant termed as aspectaware LSTM (AA-LSTM) to introduce the aspect into the process of modeling context.
· Considering that the aspect is the core information in this task, we fully exploit its potential by introducing it into the LSTM cells. We design three aspect gates to introduce the aspect into the input gate, forget gate and output gate in classic LSTM. AA-LSTM can utilize aspect to improve the information flow and then generate more effective aspect-specific context representation.
· We apply our proposed AA-LSTM to several representative LSTM-based models, and the experimental results on the benchmark datasets demonstrate the validity and generalization of our proposed AA-LSTM.
2 Related Work
In this section, we survey some representative studies in the aspect-based sentiment analysis (ABSA). ABSA is the task of predicting the sentiment polarity of a comment with respect to a set of aspects terms or categories included in the context. The biggest challenge faced by ABSA is how to effectively represent the aspect-specific sentiment information of the comment [Ma et al., 2018]. Although some traditional methods for target sentiment analysis also achieve promising results, they are labor intensive because they have mostly focused on feature engineering or massive extra linguistic resources [Kiritchenko et al., 2014; Wagner et al., 2014].
As deep learning achieved breakthrough success in representation learning, many recent works utilized deep neural networks to automatically extract features and generate the context embedding which is a dense vector formed representation of the comment.
Since the attention mechanism was first introduced to the NLP field [Bahdanau et al., 2014], many sequence-based approaches utilize it to generate more aspect-specific final representations. Attention mechanism in ABSA takes aspect information (usually aspect embedding) and the hidden states of every context word (generated by context modeling) as input and produces a probability distribution in which important parts of the context will be assigned bigger weights according to the aspect information.
There are some CNN-based [Xue and Li, 2018] and memory networks (MNs)-based models for context modeling [Tang et al., 2016b; Tay et al., 2017; Wang et al., 2018]. [Tay et

5314

<917

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

al., 2017] model dyadic interactions between aspect and sentence using neural tensor layers and associative layers with rich compositional operators. [Wang et al., 2018] argue that for the case where several sentences are the same except for different targets, relying attention mechanism alone is insufficient. It designed several memory networks having their own characters to solve the problem.
In particular, LSTM networks are widely used in context modeling because of its advantages for sequence modeling [Tang et al., 2016a; Ma et al., 2017; Devamanyu et al., 2018; Wang et al., 2016; Tay et al., 2018; Ma et al., 2018; Liu and Zhang, 2017; Yang et al., 2017]. We divide recent well-performing methods into three categories according to the process of modeling context: First, modeling the context and aspect via concatenating the aspect vector to the word embeddings of context words in the embedding layer. [Wang et al., 2016] firstly propose aspect embedding, and their ATAE-LSTM learns to attend to different parts of the context according to the aspect embedding. Although IADLSTM [Devamanyu et al., 2018] model inter-dependencies between multiple aspects of one comment through LSTM after getting the final representation of the context, it is consistent with ATAE-LSTM [Wang et al., 2016] in the way of context modeling.
Second, modeling the context alone and utilizing the aspect to compute context's attention vector in the attention mechanism. The main difference among this category of models is the calculation method of the attention mechanism. [Ma et al., 2017] propose an interactive attention network (IAN) which models targets and contexts separately. Then it learns the interactions between the context and target in attention mechanism utilizing the averages of context's hidden states and target's hidden states. [Tay et al., 2018] propose Aspect Fusion LSTM (AF-LSTM) model with a novel association layer after LSTM to model word-aspect relation utilizing circular convolution and circular correlation.
Third, introducing relative position information of the given target and context words to the hidden state vectors of context words. RAM [Chen et al., 2017] realizes that the hidden state vector of a word will be assigned a larger weight if it is closer to the target through a relative location vector. This operation is conducted before their recurrent attention layer consisting of GRU cells.
Unlike all the above methods, we propose to introduce he aspect information into the process of context modeling. Our proposed AA-LSTM introduces the aspect into the LSTM cells to control information flow. AA-LSTM can not only select key information in the context according to the aspect and keep the important information in context words' hidden state vectors, but also filter useless information for the given aspect. Then AA-LSTM can generate more effective aspectspecific context hidden state vectors.
3 Aspect-Aware LSTM
In this section we describe our proposed aspect-aware LSTM (AA-LSTM) in detail. Classic LSTM contains three gates (input gate, forget gate and output gate) to control the information flow. We argue that aspect information should be con-

sidered into LSTM cells to improve the information flow. It is intuitive that in every time step the degree that aspect is integrated into the three gates of classic LSTM should be different. Therefore, we incorporate aspect vector into classic LSTM cells and design three aspect gates to control how much the aspect vector is imported into the input gate, forget gate and output gate respectively. In this way, we can utilize the previous hidden state and the aspect itself to control how much the aspect is imported in the three gates of classic LSTM. Figure 1 illustrates the architecture of the AA-LSTM network and it can be formalized as follows:

ai = (Wai [A, ht-1] + bai)

(1)

It = (WI [xt, ht-1] + ai  A + bI )

(2)

af = (Waf [A, ht-1] + baf )

(3)

ft = (Wf [xt, ht-1] + af  A + bf )

(4)

Ct = tanh(WC [xi, ht-1] + bC )

(5)

Ct = ft  Ct-1 + It  Ct

(6)

ao = (Wao [A, ht-1] + bao)

(7)

ot = (Wo [xt, ht-1] + ao  A + bo)

(8)

ht = ot  tanh(Ct)

(9)

where xt represents the input embedding vector of the context word corresponding to time step t, A stands for the aspect vector, ht-1 is previous hidden state, ht is the hidden state of this time step,  and tanh are sigmoid and hyperbolic tangent functions,  stands for element-wise multiplication, Wai, Waf , Wao  Rda×(dc+da) and WI , Wf , WC , Wo  Rdc×2dc are the weighted matrices, bai, baf , bao  Rda, bI , bf , bC , bo  Rdc are biases and da, dc stand for the aspect vector's dimension and the number of hidden cells at AA-LSTM respectively. It, ft, ot  Rdc stand for the input gate, forget gate and output gate respectively. The input gate controls the extent of updating the information from the current input. The forget gate is responsible for selecting some information from last cell state. The output gate controls how much the information in current cell state is output to be the hidden state vector of this time step. Similarly, ai, af , ao  Rda stand for the aspect-input gate, aspect-forget gate and aspect-output gate respectively. The three aspectbased gates determine the extent of integrating the aspect information into the input gate, forget gate and output gate.
Our proposed AA-LSTM takes two strands of inputs: context word embeddings and the aspect vector. At each time step, the context word entering the AA-LSTM dynamically varies according to the sequence of words in the sentence, while the aspect vector is identical. Specifically, aspect vector is the representation of the target in TSA, and it is the aspect embedding in ACSA. Next, we describe the different components of our proposed AA-LSTM in detail.

3.1 Input Gates
The input gate It controls how much new information can be transferred to the cell state. While the aspect-input gate ai controls how much the aspect is transferred to the input gate It. The difference between the AA-LSTM and the classical LSTM lies in the weighted aspect vector input of It. The

5315

918> 2019 

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

predicting the given aspect's sentiment polarity can be preserved in the hidden states vectors. In addition, as shown in Equation 6, the alternative input content does not include the aspect information. So the AA-LSTM only utilizes the given aspect to influent the information flow instead of integrating the aspect information into the hidden state vectors.

Figure 1: The overall architecture of AA-LSTM network
weight of aspect vector ai is computed by ht-1 and A. ht-1 can be regarded as the previous semantic representation of the partial sentences which has been processed in the past time steps. Hence, the extent of the aspect's integration into It is decided by the previous semantic representation and the aspect vector A.
3.2 Forget Gates
The forget gate ft abandons trivial information and retains key information from last cell state Ct-1. Similarly, the aspect-input gate af controls how much the aspect vector is transferred to the forget gate ft. The difference between the AA-LSTM and the classical LSTM in ft is the introduction of weighted aspect vector. And the weight of aspect vector af is computed by ht-1 and A. Therefore, the extent of the aspect's integration into It is decided by the previous semantic representation and the aspect vector A.
3.3 Candidate Cell and Current Cell
The candidate cell Ct represents the alternative input content. The current cell Ct updates its cell state by selecting important information from last cell state Ct-1 and Ct.
From Equation 6 we can observe that the alternative input
content Ct aspects two strands of inputs: the last hidden state ht-1 and the input embedding xt of this time step. So the hidden states in each time step contain the semantic information of the previous sentence segment. In the classical LSTM, information that is more important to the overall sentence semantics is more likely to be preserved in the hidden states vectors of the subsequent time steps. However, for some information which is retained because of its contribution to the semantics of the whole sentence, it may be noisy when judging the sentiment polarity of a given aspect. And the information that is crucial to analyze the given aspect's sentiment may be neglected due to its less contribution to the overall sentence. As demonstrated in the Introduction section, we define this phenomenon as an aspect-unaware problem in the process of context modeling.
Our proposed AA-LSTM can solve this problem by introducing aspect to the process of modeling context to control the flow of information. Information that is important for

3.4 Output Gates
The output gate ot controls the extent of the information flow from the current cell state to the hidden state vector of this time step. Similarly, the aspect-output gate ao controls the extent of the aspect's influence on the output gate It. The difference between our proposed AA-LSTM and the classical LSTM in ot is the introduction of weighted aspect vector into ot. And the weight of aspect vector ao is computed by ht-1 and A. Therefore, the degree of how much the aspect information is integrated into ot is decided by the previous semantic representation and the aspect vector A.
4 Experiment
In this section, we introduce the tasks, the datasets, the evaluation metric, the models for comparison and the implementation details.
4.1 Tasks Definition
We conduct experiments on two subtasks of aspect sentiment analysis: aspect term sentiment analysis (ATSA) or target sentiment analysis (TSA) and aspect category sentiment analysis (ACSA). The former infers sentiment polarities of given target entities contained in the context. The latter infers sentiment polarities of generic aspects such as `service' or `food' which may or may not be found in the context, and the aspects belong to a predefined set. In this paper, these two kinds of tasks are both considered and they are collectively named as aspect-based sentiment analysis (ABSA).
4.2 Datasets
We experiment on SemEval 2014 [Pontiki et al., 2014] task 4 datasets which consist of laptop and restaurant reviews and are widely used benchmarks in many previous works [Tang et al., 2016a; Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Devamanyu et al., 2018; Tay et al., 2018; Wang et al., 2018]. We remove the reviews having no aspect or the aspects with sentiment polarity of "conflict". The dataset we used consists of reviews with at least one aspect labeled with sentiment polarities of positive, neutral and negative. For ATSA, we adopt Laptop and Restaurant datasets; And for ACSA, we adopt the Restaurant dataset. 20% of the training data is used as the development set. Full statistics of SemEval 2014 task 4 datasets are given in Table 1.
4.3 Evaluation Metric
Since the two tasks are both multi-class classification tasks, we adopt F1-Macro as our evaluation measure. And there are some other methods that use strict accuracy (Acc) [Wang et al., 2016; Ma et al., 2017; Chen et al., 2017; Devamanyu et al., 2018] for evaluation, which measures the percentage of correctly predicted samples in all samples. Therefore, we

5316

<919

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

Task Dataset

Pos Neg Neu

Restaurant Train 2164 807 637

ATSA

Restaurant Test Laptop Train

728 196 196 994 870 464

Laptop Test

341 128 169

ACSA

Restaurant Train Restaurant Test

2179 657

839 222

500 94

Table 1: Statistic of all datasets

use these two metrics (F1-Macro and Acc) to evaluate the models' performances.
Generally, higher Acc can verify the effectiveness of the system though it biases towards the majority class, and F1Macro provides more indicative information because the task is a multi-class problem.
4.4 Models for Comparison
In order to verify the advantages of our proposed AA-LSTM compared to classic LSTM, we choose some representative LSTM-based models to replace their original LSTM with our proposed AA-LSTM.
In Introduction and Related Work sections, we have divide recent well-performing methods into three categories according to their processes of modeling context. In order to prove the generalization ability of our model, we select a representative model from each of these categories for experiments. We choose ATAE-LSTM, IAN, and RAM as the representatives of the three categories of models because their architectures are novel and they are taken as comparative methods in many works. We also compare our model with the baseline LSTM model. We introduce them in detail as follows:
LSTM. This is the baseline that ignores targets and only models contexts using one LSTM network. The last hidden state is regarded as the final sentiment representation.
ATAE-LSTM. It concatenates the aspect embedding to the word embeddings of context words and uses aspect embedding to produce the attention vector. For ATSA, we take the average of the embeddings of the target words as the aspect embedding which is concatenated to the word embeddings of the context words.
IAN. It models context and target separately and selects important information from them via two interactive attention mechanisms. The target and context can have impacts on the generation of their representations interactively and their representations are concatenated as the final aspect-specific sentiment representations. For the ACSA task, we omit the modeling of the target and use the aspect embedding to produce the attention vector of context words.
RAM. It utilizes relative location to assign weights to original context hidden state vectors and then learns the attention vector in a recurrent attention mechanism consisting of GRU cell. It can only be applied to ATSA. For the consistent of comparison, we replaced the deep bidirectional LSTM in the original RAM with a unidirectional single-layer LSTM.
We also choose two state-of-the-art methods that are Memory Networks-based and LSTM-based respectively:

Target-sensitive Memory Network. [Wang et al., 2018] construct six target-sensitive memory networks (TMNs) which have their own characteristics to resolve target sensitivity and got some improvement. We choose the NP (hops) and JCI (hops) that perform best on Laptop and Restaurant, respectively.
Inter-Aspect Dependencies LSTM. [Devamanyu et al., 2018] model aspect-based sentential representations as a sequence to capture the inter-aspect dependencies.
We don't reimplement the above two models and the results are retrieved from their original papers.
4.5 Implementation Details
We implement the models in Tensorflow. We initialize all word embeddings by Glove [Jeffrey et al., 2014] and out-ofvocabulary words by sampling from the uniform distribution U (-0.1, 0.1). Initial values of all weight matrices are sampled from uniform distribution U (-0.1, 0.1) and initial values of all biases are zeros. All embedding dimensions are set to 300 and the batch size is set as 16. We minimize the loss function to train our models using Adam optimizer [Diederik and Jimmy, 2014] with the learning rate set as 0.001. To avoid over fitting, we adopt the dropout strategy with p = 0.5 and the coefficient of L2 normalization in the loss function is set to 0.01. All models use softmax classifier.
For ACSA, we initialize all aspect embeddings by sampling from the uniform distribution U (-0.1, 0.1). As for the input aspect vector (A) of our proposed AA-LSTM which is replaced with the classic LSTM in the above models, we set it as follows:
Aspect Term Sentiment Analysis. We use the average of word embeddings of the target words as A except for IAN. For IAN, we use the average of the hidden states vectors of target words as A.
Aspect Category Sentiment Analysis. For all models, we use the aspect embedding as A.
We implement all models under the same experiment settings to make sure the improvements based on the original models come from the replacement of classic LSTM with our proposed AA-LSTM.
5 Results and Analysis
Our experimental results are illustrated in Table 2. We can observe that our proposed AA-LSTM and its substitution in other models has an overall advantage over classic LSTMs on their corresponding original models. It especially achieves higher F1-Macro which can better illustrate the overall performances of the models in multiple classes as the classes are unbalanced. On the ATSA task, except for the F1-Macro score on Restaurant, the performances of our variants overpass the performances of the representative state-of-the-art models. In the implementation of the experiment, the only difference between original models and their variants is the substitution of classic LSTM. As we replace the original LSTM with our AA-LSTM, the performance improvement can demonstrate the pure effectiveness of our AA-LSTM.

5317

920> 2019 

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

Task and Dataset
Model LSTM ATAE-LSTM IAN RAM IAD-LSTM JCI (hops) NP (hops)
AA-LSTM ATAE-LSTM (AA)
IAN (AA) RAM (AA)

Laptop F1-Macro
59.77 61.28 64.54 67.05
67.2 67.8
61.45 62.10 65.62 68.47

Acc 65.99 66.93 70.53 71.32 72.5 71.8 72.4
66.93 69.28 71.94 73.20

ATSA

Restaurant F1-Macro
61.04 64.47 65.67 65.84
68.8 66.0
66.24 66.46 68.71 68.15

Acc 75.00 77.41 78.48 78.57 79.0 78.8 75.7
78.21 78.21 79.29 78.13

ACSA Restaurant F1-Macro 70.07 70.15 70.81 -
75.00 74.51 74.43
-

Acc 81.71 82.12 83.25
-
83.45 83.97 84.69
-

Table 2: Comparisons of all models on three datasets. Last four models are our proposed AA-LSTM models, and the last three models with suffix "(AA)" is the variants in which the original classic LSTM is replaced with our proposed AA-LSTM. The results of IAD-LSTM, JCI (hops) and NP (hops) are retrieved from the original papers. Best scores are marked in bold.

Compared with LSTM, AA-LSTM's improvements on macro are up to 7% and 6% on Restaurant for ATSA and ACSA respectively. Like LSTM, AA-LSTM also directly uses the last hidden state vector as the final sentiment representation sent to the classifier. But because the aspect is introduced into the process of modeling context, the semantics of the last hidden state vector of AA-LSTM is aspect-specific. In fact, not only in the last hidden layer, but also in all hidden states vectors, the information which is important for determining the emotional polarity of the aspect is kept, and other useless information is filtered, which makes the context modeling result much better than LSTM.As the classifiers are the same, the reason AA-LSTM performs better than LSTM is that the final sentiment representation of AA-LSTM is more effective.
AA-LSTM's performance even surpassed ATAE-LSTM and exceed all original models on F1-macro for ACSA. It is worth mentioning that all baselines utilizes the attention mechanism and ATAE-LSTM also models the context and aspect together via concatenating aspect to every word embeddings of context words. In contrast, AA-LSTM only models the context without any other processing. This proves that AA-LSTM's result of modeling context is aspect-specific and effective. This is because the aspect information is used in the modeling process to control the flow of information, retain and filter information, who performs as earlier attention.
ATAE-LSTM (AA)'s performance exceeds ATAE-LSTM and AA-LSTM. This shows that AA-LSTM can be compatible with other components of ATAE-LSTM, improving the whole model's performance. ATAE-LSTM represents a category of models that combine the context and aspect together via concatenating the aspect vector to context word embeddings. So the experimental results verify that although the input embeddings contain aspect information, it doesn't conflict with the aspect information introduced in AA-LSTM.
IAN represents a category of models which encode the context alone and utilize the aspect to compute contexts' attention vector in the attention mechanism. IAN-LSTM (AA)'s overall performance exceeds IAN and AA-LSTM. This proves that the hidden states vectors generated by

AA-LSTM can collaborate with the attention mechanism to achieve better performance.
RAM utilizes the relative location vector to assign weights to original context word hidden state vectors, and calculates the attention vector via a recurrent attention mechanism which is more complex than other baseline models. It is worth noting that compared with RAM, RAM (AA) has more improvement than other original models and their variants. This is because the advantage of AA-LSTM is amplified in RAM. In RAM (AA), while the tokens closer to the target are assigned larger weights, AA-LSTM keeps more important information about the target in the tokens closer to the target: adjectives, modifying phrases, clauses, etc. In addition, the context hidden states vectors generated by AA-LSTM and the recurrent mechanism work together to produce more effective final sentiment representation.
6 Conclusion
In this paper, we argue that aspect-related information may be discarded and aspect-irrelevant information may be retained in classic LSTM cells. To address this problem, we propose a novel LSTM variant termed as Aspect-Aware LSTM. Due to the introduction of the aspect into the process of modeling context, our proposed Aspect-aware LSTM can select important information about the given target and filter out the useless information via information flow control. Aspect-Aware LSTM can not only generate more effective contextual vectors than classic LSTM, but also be compatible with other modules.
Acknowledgements
The authors would like to thank the anonymous reviewers for their valuable comments. This work is supported by National Key Research and Development Program of China (Grant No. 2016YFB1000902), National Natural Science Foundation of China (NSFC, Grant Nos. 61866038 and 61751201), and Research Foundation of Beijing Municipal Science and Technology Commissions (No. Z181100008918002).

5318

<921

Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI-19)

References
[Bahdanau et al., 2014] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. Computer Science, 2014.
[Chen et al., 2017] Peng Chen, Zhongqian Sun, Lidong Bing, and Wei Yang. Recurrent attention network on memory for aspect sentiment analysis. In Empirical Methods in Natural Language Processing, pages 452­461, 2017.
[Devamanyu et al., 2018] Hazarika Devamanyu, Poria Soujanya, Vij Prateek, Krishnamurthy Gangeshwar, Cambria Erik, and Zimmermann Roger. Modeling inter-aspect dependencies for aspect-based sentiment analysis. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 266­270, 2018.
[Diederik and Jimmy, 2014] Kingma Diederik and Ba Jimmy. Adam: A method for stochastic optimization. CoRR abs/1412.6980, 2014.
[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Ju¨rgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735­1780, 1997.
[Jeffrey et al., 2014] Pennington Jeffrey, Socher Richard, and Manning Christopher. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing, pages 1532­1543, 2014.
[Kiritchenko et al., 2014] Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherrt, and Saif Mohammad and. Nrc-canada2014: Detecting aspects and sentiment in customer reviews. In International Workshop on Semantic Evaluation, pages 437­442, 2014.
[Liu and Zhang, 2017] Jiangming Liu and Yue Zhang. Attention modeling for targeted sentiment. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, page 572577, 2017.
[Ma et al., 2017] Dehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. Interactive attention networks for aspect-level sentiment classification. In International Joint Conference on Artificial Intelligence, pages 4068­ 4074, 2017.
[Ma et al., 2018] Yukun Ma, Haiyun Peng, and Erik Cambria. Targeted aspect-based sentiment analysis via embedding commonsense knowledge into an attentive lstm. In AAAI Conference on Artificial Intelligence, pages 5876­ 5883, 2018.
[Pontiki et al., 2014] Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis. In International Workshop on Semantic Evaluation, pages 27­35, 2014.
[Tang et al., 2016a] Duyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu. Effective lstms for target-dependent sentiment classification. In International Conference on Computational Linguistics, pages 3298­3307, 2016.

[Tang et al., 2016b] Duyu Tang, Bing Qin, and Ting Liu. Aspect level sentiment classification with deep memory network. In Empirical Methods in Natural Language Processing, pages 214­224, 2016.
[Tay et al., 2017] Yi Tay, Tuan Luu Anh, and Hui Siu Cheung. Dyadic memory networks for aspect-based sentiment analysis. In ACM on Conference on Information and Knowledge Management, CIKM, pages 107­116, 2017.
[Tay et al., 2018] Yi Tay, Luu Anh Tuan, and Siu Cheung Hui. Learning to attend via word-aspect associative fusion for aspect-based sentiment analysis. In AAAI Conference on Artificial Intelligence, pages 5956­5963, 2018.
[Wagner et al., 2014] Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman, Dasha Bogdanova, Jennifer Foster, and Lamia Tounsi. Dcu: Aspect-based polarity classification for semeval task 4. In International Workshop on Semantic Evaluation, pages 223­229, 2014.
[Wang et al., 2016] Yequan Wang, Minlie Huang, and Li Zhao. Attention-based lstm for aspect-level sentiment classification. In Empirical Methods in Natural Language Processing, pages 606­615, 2016.
[Wang et al., 2018] Shuai Wang, Sahisnu Mazumder, Bing Liu, Mianwei Zhou, and Yi Chang. Target-sensitive memory networks for aspect sentiment classification. In Annual Meeting of the Association for Computational Linguistics,, pages 957­967, 2018.
[Xin et al., 2018] Li Xin, Bing Lidong, Lam Wai, and Shi Bei. Transformation networks for target-oriented sentiment classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946­956, 2018.
[Xu et al., 2017] Jiacheng Xu, Xipeng Qiu, Kan Chen, and Xuanjing Huang. Knowledge graph representation with jointly structural and textual encoding. In International Joint Conference on Artificial Intelligence, pages 1318­ 1324, 2017.
[Xue and Li, 2018] Wei Xue and Tao Li. Aspect based sentiment analysis with gated convolutional networks. In Annual Meeting of the Association for Computational Linguistics,, pages 2514­2523, 2018.
[Yang and Mitchell, 2017] Bishan Yang and Tom M. Mitchell. Leveraging knowledge bases in lstms for improving machine reading. In Annual Meeting of the Association for Computational Linguistics, pages 1436­1446, 2017.
[Yang et al., 2017] Min Yang, Wenting Tu, Jingxuan Wang, Fei Xu, and Xiaojun Chen. Attention based lstm for target dependent sentiment classification. In AAAI Conference on Artificial Intelligence, page 50135014, 2017.
[Zhen et al., 2017] Xu Zhen, Liu Bingquan, Wang Baoxun, Sun Chengjie, and Wang Xiaolong. Incorporating loose-structured knowledge into conversation modeling via recall-gate lstm. In International Joint Conference on Neural Networks, pages 3506­3513, 2017.

5319

922> 2019 
 The 25th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems

Hongwei Wang
Stanford University hongweiw@cs.stanford.edu

Fuzheng Zhang
Meituan-Dianping Group zhangfuzheng@meituan.com

Mengdi Zhang
Meituan-Dianping Group zhangmengdi02@meituan.com

Jure Leskovec
Stanford University jure@cs.stanford.edu

Miao Zhao, Wenjie Li

Zhongyuan Wang

Hong Kong Polytechnic University

Meituan-Dianping Group

{csmiaozhao,cswjli}@comp.polyu.edu.hk wangzhongyuan02@meituan.com

ABSTRACT
Knowledge graphs capture structured information and relations between a set of entities or items. As such knowledge graphs represent an attractive source of information that could help improve recommender systems. However, existing approaches in this domain rely on manual feature engineering and do not allow for an end-to-end training. Here we propose Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) to provide better recommendations. Conceptually, our approach computes user-specific item embeddings by first applying a trainable function that identifies important knowledge graph relationships for a given user. This way we transform the knowledge graph into a user-specific weighted graph and then apply a graph neural network to compute personalized item embeddings. To provide better inductive bias, we rely on label smoothness assumption, which posits that adjacent items in the knowledge graph are likely to have similar user relevance labels/scores. Label smoothness provides regularization over the edge weights and we prove that it is equivalent to a label propagation scheme on a graph. We also develop an efficient implementation that shows strong scalability with respect to the knowledge graph size. Experiments on four datasets show that our method outperforms state of the art baselines. KGNN-LS also achieves strong performance in cold-start scenarios where user-item interactions are sparse.
KEYWORDS
Knowledge-aware recommendation; graph neural networks; label propagation
ACM Reference Format: Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao, Wenjie Li, and Zhongyuan Wang. 2019. Knowledge-aware Graph Neural Networks with Label Smoothness Regularization for Recommender Systems. In The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '19), August 4­8, 2019, Anchorage, AK, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3292500.3330836
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD '19, August 4­8, 2019, Anchorage, AK, USA © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6201-6/19/08. . . $15.00 https://doi.org/10.1145/3292500.3330836

1 INTRODUCTION
Recommender systems are widely used in Internet applications to meet user's personalized interests and alleviate the information overload [4, 29, 32]. Traditional recommender systems that are based on collaborative filtering [13, 22] usually suffer from the coldstart problem and have trouble recommending brand new items that have not yet been heavily explored by the users. The sparsity issue can be addressed by introducing additional sources of information such as user/item profiles [23] or social networks [22].
Knowledge graphs (KGs) capture structured information and relations between a set of entities [8, 9, 18, 24­28, 33, 34, 36]. KGs are heterogeneous graphs in which nodes correspond to entities (e.g., items or products, as well as their properties and characteristics) and edges correspond to relations. KGs provide connectivity information between items via different types of relations and thus capture semantic relatedness between the items.
The core challenge in utilizing KGs in recommender systems is to learn how to capture user-specific item-item relatedness captured by the KG. Existing KG-aware recommender systems can be classified into path-based methods [8, 33, 36], embedding-based methods [9, 26, 27, 34], and hybrid methods [18, 24, 28]. However, these approaches rely on manual feature engineering, are unable to perform end-to-end training, and have poor scalability. Graph Neural Networks (GNNs), which aggregate node feature information from node's local network neighborhood using neural networks, represent a promising advancement in graph-based representation learning [3, 5­7, 11, 15]. Recently, several works developed GNNs architecture for recommender systems [14, 19, 28, 31, 32], but these approaches are mostly designed for homogeneous bipartite useritem interaction graphs or user-/item-similarity graphs. It remains an open question how to extend GNNs architecture to heterogeneous knowledge graphs.
In this paper, we develop Knowledge-aware Graph Neural Networks with Label Smoothness regularization (KGNN-LS) that extends GNNs architecture to knowledge graphs to simultaneously capture semantic relationships between the items as well as personalized user preferences and interests. To account for the relational heterogeneity in KGs, similar to [28], we use a trainable and personalized relation scoring function that transforms the KG into a user-specific weighted graph, which characterizes both the semantic information of the KG as well as user's personalized interests. For example, in the movie recommendation setting the relation scoring function could learn that a given user really cares about "director" relation between movies and persons, while somebody else may care more

968

<923

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

about the "lead actor" relation. Using this personalized weighted graph, we then apply a graph neural network that for every item node computes its embedding by aggregating node feature information over the local network neighborhood of the item node. This way the embedding of each item captures it's local KG structure in a user-personalized way.
A significant difference between our approach and traditional GNNs is that the edge weights in the graph are not given as input. We set them using user-specific relation scoring function that is trained in a supervised fashion. However, the added flexibility of edge weights makes the learning process prone to overfitting, since the only source of supervised signal for the relation scoring function is coming from user-item interactions (which are sparse in general). To remedy this problem, we develop a technique for regularization of edge weights during the learning process, which leads to better generalization. We develop an approach based on label smoothness [35, 38], which assumes that adjacent entities in the KG are likely to have similar user relevancy labels/scores. In our context this assumption means that users tend to have similar preferences to items that are nearby in the KG. We prove that label smoothness regularization is equivalent to label propagation and we design a leave-one-out loss function for label propagation to provide extra supervised signal for learning the edge scoring function. We show that the knowledge-aware graph neural networks and label smoothness regularization can be unified under the same framework, where label smoothness can be seen as a natural choice of regularization on knowledge-aware graph neural networks.
We apply the proposed method to four real-world datasets of movie, book, music, and restaurant recommendations, in which the first three datasets are public datasets and the last is from MeituanDianping Group. Experiments show that our method achieves significant gains over state-of-the-art methods in recommendation accuracy. We also show that our method maintains strong recommendation performance in the cold-start scenarios where user-item interactions are sparse.
2 RELATED WORK
2.1 Graph Neural Networks
Graph Neural Networks (or Graph Convolutional Neural Networks, GCNs) aim to generalize convolutional neural networks to nonEuclidean domains (such as graphs) for robust feature learning. Bruna et al. [3] define the convolution in Fourier domain and calculate the eigendecomposition of the graph Laplacian, Defferrard et al. [5] approximate the convolutional filters by Chebyshev expansion of the graph Laplacian, and Kipf et al. [11] propose a convolutional architecture via a first-order approximation. In contrast to these spectral GCNs, non-spectral GCNs operate on the graph directly and apply "convolution" (i.e., weighted average) to local neighbors of a node [6, 7, 15].
Recently, researchers also deployed GCNs in recommender systems: PinSage [32] applies GCNs to the pin-board bipartite graph in Pinterest. Monti et al. [14] and Berg et al. [19] model recommender systems as matrix completion and design GCNs for representation learning on user-item bipartite graphs. Wu et al. [31] use GCNs on user/item structure graphs to learn user/item representations. The difference between these works and ours is that they are all

designed for homogeneous bipartite graphs or user/item-similarity graphs where GCNs can be used directly, while here we investigate GCNs for heterogeneous KGs. Wang et al. [28] use GCNs in KGs for recommendation, but simply applying GCNs to KGs without proper regularization is prone to overfitting and leads to performance degradation as we will show later. Schlichtkrull et al. also propose using GCNs to model KGs [17], but not for the purpose of recommendations.
2.2 Semi-supervised Learning on Graphs
The goal of graph-based semi-supervised learning is to correctly label all nodes in a graph given that only a few nodes are labeled. Prior work often makes assumptions on the distribution of labels over the graph, and one common assumption is smooth variation of labels of nodes across the graph. Based on different settings of edge weights in the input graph, these methods are classified as: (1) Edge weights are assumed to be given as input and therefore fixed [1, 37, 38]; (2) Edge weights are parameterized and therefore learnable [10, 21, 35]. Inspired by these methods, we design a module of label smoothness regularization in our proposed model. The major distinction of our work is that the label smoothness constraint is not used for semi-supervised learning on graphs, but serves as regularization to assist the learning of edge weights and achieves better generalization for recommender systems.
2.3 Recommendations with Knowledge Graphs
In general, existing KG-aware recommender systems can be classified into three categories: (1) Embedding-based methods [9, 26, 27, 34] pre-process a KG with knowledge graph embedding (KGE) [30] algorithms, then incorporate learned entity embeddings into recommendation. Embedding-based methods are highly flexible in utilizing KGs to assist recommender systems, but the KGE algorithms focus more on modeling rigorous semantic relatedness (e.g., TransE [2] assumes head +relation = tail), which are more suitable for graph applications such as link prediction rather than recommendations. In addition, embedding-based methods usually lack an end-to-end way of training. (2) Path-based methods [8, 33, 36] explore various patterns of connections among items in a KG (a.k.a meta-path or meta-graph) to provide additional guidance for recommendations. Path-based methods make use of KGs in a more intuitive way, but they rely heavily on manually designed metapaths/meta-graphs, which are hard to tune in practice. (3) Hybrid methods [18, 24, 28] combine the above two categories and learn user/item embeddings by exploiting the structure of KGs. Our proposed model can be seen as an instance of hybrid methods.
3 PROBLEM FORMULATION
We begin by describing the KG-aware recommendations problem and introducing notation. In a typical recommendation scenario, we have a set of users U and a set of items V. The user-item interaction matrix Y is defined according to users' implicit feedback, where yuv = 1 indicates that user u has engaged with item v, such as clicking, watching, or purchasing. We also have a knowledge graph G = {(h, r, t)} available, in which h  E, r  R, and t  E denote the head, relation, and tail of a knowledge triple, E and R are the set of entities and relations in the knowledge graph, respectively. For

969

924> 2019 

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA









!

"'(

" ' (

"' (

" ' (


" '! (

 



"#  
 



!  
 
 
 


   

  !   

!      " % ' "( "   #

&"# % ' "(
 %  &"# "# $  "

 
    '"( " 
  #

Figure 1: Overview of our proposed KGNN-LS model. The original KG is first transformed into a user-specific weighted graph,
on which we then perform feature propagation using a graph neural network with the label smoothness regularization. The two modules constitute the complete loss function L.

example, the triple (The Silence of the Lambs, film.film.star, Anthony Hopkins) states the fact that Anthony Hopkins is the leading actor in film "The Silence of the Lambs". In many recommendation scenarios, an item v  V corresponds to an entity e  E (e.g., item "The
Silence of the Lambs" in MovieLens also appears in the knowledge graph as an entity). The set of entities E is composed from items V (V  E) as well as non-items E\V (e.g. nodes corresponding
to item/product properties). Given user-item interaction matrix Y and knowledge graph G, our task is to predict whether user u has potential interest in item v with which he/she has not engaged before. Specifically, we aim to learn a prediction function y^uv = F (u, v |, Y, G), where y^uv denotes the probability that user u will engage with item v, and  are model parameters of function F .
We list the key symbols used in this paper in Table 1.

Symbol
U = {u1, · · · } V = {v1, · · · }
Y G = (E, R)
E = {e1, · · · } R = {r1, · · · }
E\V
su (r )
Au
Du
E H(l), l = 0, ..., L - 1 W(l), l = 0, ..., L - 1
lu (e), e  E lu (e), e  E l^u (v), v  V
R(Au )

Meaning Set of users Set of items User-item interaction matrix Knowledge graph Set of entities Set of relations Set of non-item entities User-specific relation scoring function Adjacency matrix of G w.r.t. user u Diagonal degree matrix of Au Raw entity feature Entity representation in the l-th layer Transformation matrix in the l-th layer Item relevancy labeling function Minimum-energy labeling function Predicted relevancy label for item v Label smoothness regularization on Au

Table 1: List of key symbols.

4 OUR APPROACH
In this section, we first introduce knowledge-aware graph neural networks and label smoothness regularization, respectively, then we present the unified model.
4.1 Preliminaries: Knowledge-aware Graph Neural Networks
The first step of our approach is to transform a heterogeneous KG into a user-personalized weighted graph that characterizes user's preferences. To this end, similar to [28], we use a user-specific relation scoring function su (r ) that provides the importance of relation r for user u: su (r ) = (u, r), where u and r are feature vectors of user u and relation type r , respectively, and  is a differentiable function such as inner product. Intuitively, su (r ) characterizes the importance of relation r to user u. For example, a user may be more interested in directors of movies, but another user may care more about the lead actors of movies.
Given user-specific relation scoring function su (·) of user u, knowledge graph G can therefore be transformed into a userspecific adjacency matrix Au  R| E |×| E |, in which the (i, j)-entry Aiuj = su (rei,ej ), and rei,ej is the relation between entities ei and ej in G.1 Aiuj = 0 if there is no relation between ei and ej . See the left two subfigures in Figure 1 for illustration. We also denote the raw feature matrix of entities as E  R| E |×d0 , where d0 is the dimension of raw entity features. Then we use multiple feed forward layers2 to update the entity representation matrix by aggregating representations of neighboring entities. Specifically, the layer-wise forward propagation can be expressed as
Hl +1 =  Du-1/2Au Du-1/2Hl Wl , l = 0, 1, · · · , L - 1. (1)
1In this work we treat G an undirected graph, so Au is a symmetric matrix. If both triples (h, r1, t ) and (t, r2, h) exist, we only consider one of r1 and r2. This is due to the fact that: (1) r1 and r2 are the inverse of each other and semantically related; (2) Treating Au symmetric will greatly increase the matrix density. 2There are several candidate designs for the architecture of our model, e.g., GCN [11] or GraphSAGE [7]. Here we use GCN [11] as our base model.

970

<925

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

In Eq. (1), Hl is the matrix of hidden representations of entities in layer l, and H0 = E. Au is to aggregate representation vectors of neighboring entities. In this paper, we set Au  Au + I, i.e., adding self-connection to each entity, to ensure that old representation
vector of the entity itself is taken into consideration when updating
entity representations. Du is a diagonal degree matrix with entries Duii = j Aiuj , therefore, Du-1/2 is used to normalize Au and keep the entity representation matrix Hl stable. Wl  Rdl ×dl+1 is the layer-specific trainable weight matrix,  is a non-linear activation function, and L is the number of layers.
A single GNN layer computes the representation of an entity via a
transformed mixture of itself and its immediate neighbors in the KG.
We can therefore naturally extend the model to multiple layers to
explore users' potential interests in a broader and deeper way. The final output is HL  R| E |×dL , which is the entity representations that mix the initial features of themselves and their neighbors up to L hops away. Finally, the predicted engagement probability of user u with item v is calculated by y^uv = f (u, vu ), where vu (i.e., the v-th row of HL) is the final representation vector of item v, and f is a differentiable prediction function, for example, inner
product or a multilayer perceptron. Note that vu is user-specific since the adjacency matrix Au is user-specific. Furthermore, note that the system is end-to-end trainable where the gradients flow from f (·) via GNN (parameter matrix W) to (·) and eventually to representations of users u and items v.

4.2 Label Smoothness Regularization
It is worth noticing a significant difference between our model and GNNs: In traditional GNNs, edge weights of the input graph are fixed; but in our model, edge weights Du-1/2Au Du-1/2 in Eq. (1) are learnable (including possible parameters of function  and feature vectors of users and relations) and also requires supervised training like W. Though enhancing the fitting ability of the model, this will inevitably make the optimization process prone to overfitting, since the only source of supervised signal is from user-item interactions outside GNN layers. Moreover, edge weights do play an essential role in representation learning on graphs, as highlighted by a large amount of prior works [10, 20, 21, 35, 38]. Therefore, more regularization on edge weights is needed to assist the learning of entity representations and to help generalize to unobserved interactions more efficiently.
Let's see how an ideal set of edge weights should be like. Consider a real-valued label function lu : E  R on G, which is constrained to take a specific value lu (v) = yuv at node v  V  E. In our context, lu (v) = 1 if user u finds the item v relevant and has engaged with it, otherwise lu (v) = 0. Intuitively, we hope that adjacent entities in the KG are likely to have similar relevancy labels, which is known as label smoothness assumption. This motivates our choice of energy function E:

E(lu , Au )

=

1 2

Aiuj
ei E,ej E

lu (ei ) - lu (ej ) 2 .

(2)

We show that the minimum-energy label function is harmonic by the following theorem:

Theorem 1. The minimum-energy label function

lu =

arg min

E(lu , Au )

(3)

lu :lu (v )=yuv , v V

w.r.t. Eq. (2) is harmonic, i.e., lu satisfies

lu (ei )

=

1 Dui i

Aiuj
ej E

lu (ej ),

ei



E \V .

(4)

Proof. Taking the derivative of the following equation

E(lu , Au )

=

1 2

Aiuj
i, j

lu (ei ) - lu (ej ) 2

with respect to lu (ei ) where ei  E\V, we have

E(lu , Au ) lu (ei )

=

j

Aiuj

lu (ei ) - lu (ej ) .

The minimum-energy label function lu should satisfy that

E(lu , Au ) lu (ei )

lu =lu

= 0.

Therefore, we have

lu (ei ) =

1 j Aiuj

j

Aiuj

lu (ej )

=

1 Dui i

Aiuj lu (ej ), ei  E\V.
j

The harmonic property indicates that the value of lu at each non-item entity ei  E\V is the average of its neighboring entities, which leads to the following label propagation scheme [39]:

Theorem 2. Repeating the following two steps: (1) Propagate labels for all entities: lu (E)  Du-1Aulu (E), where
lu (E) is the vector of labels for all entities; (2) Reset labels of all items to initial labels: lu (V)  Y[u, V],
where lu (V) is the vector of labels for all items and Y[u, V] = [yuv1 , yuv2 , · · · ] are initial labels; will lead to lu  lu .

Proof. Let lu (E) =

lu (V) lu (E\V)

. Since lu (V) is fixed on Y[u, V],

we are only interested in lu (E\V). We denote P = Du-1Au (the subscript u is omitted from P for ease of notation), and partition

matrix P into sub-matrices according to the partition of lu :

P=

PV V PEV

PV E PE E

.

Then the label propagation scheme is equivalent to

lu (E\V)  PEV Y[u, V] + PEElu (E\V).

(5)

Repeat the above procedure, we have

n

lu (E\V) = nlim(PEE )nlu(0)(E\V) +

(PEE )i-1
i =1

PEV Y[u, V],

(6)

where lu(0)(E\V) is the initial value for lu (E\V). Now we show that limn(PEE )nlu(0)(E\V) = 0. Since P is row-normalized and

PEE is a sub-matrix of P, we have

 < 1, PEE [i, j]  ,
j

971

926> 2019 

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

   
  
 

  


  


  

 




  





   


(a) Without the KG

(b) L = 1

(c) L = 2

(d) L = 1 for another user

(e) LS regularization

Figure 2: (a) Analogy of a physical equilibrium model for recommender systems; (b)-(d) Illustration of the effect of the KG; (e) Illustration of the effect of label smoothness regularization.

for all possible row index i. Therefore,

(PEE )n [i, j] =

(PEE )(n-1)PEE [i, j]

j

j

=

(PEE )(n-1)[i, k] PEE [k, j]

jk

= (PEE )(n-1)[i, k] PEE [k, j]

k

j

 (PEE )(n-1)[i, k] 
k
 · · ·  n.

As n goes infinity, the row sum of (PEE )n converges to zero, which implies that (PEE )nlu(0)(E\V)  0. It's clear that the choice of initial value lu(0)(E\V) does not affect the convergence.
Since limn(PEE )nlu(0)(E\V) = 0, Eq. (6) becomes

n

lu (E\V) = nlim

(PEE )i-1
i =1

PEV Y[u, V].

Denote

n



T

=

nlim

(PEE )i-1
i =1

=

(PEE )i-1,
i =1

and we have





T - TPEE = (PEE )i-1 - (PEE )i = I.

i =1

i =1

Therefore, we derive that

T = (I - PEE )-1,

and lu (E\V) = (I - PEE )-1PEV Y[u, V].
This is the unique fixed point and therefore the unique solution to Eq. (5). Repeating the steps in Theorem 2 leads to

lu (E)  lu (E) =

Y[u, V] (I - PEE )-1PEV Y[u, V]

.

Theorem 2 provides a way for reaching the minimum-energy of relevancy label function E. However, lu does not provide any signal for updating the edge weights matrix Au , since the labeled part of lu , i.e., lu (V), equals their true relevancy labels Y[u, V];

Moreover, we do not know true relevancy labels for the unlabeled nodes lu (E\V).
To solve the issue, we propose minimizing the leave-one-out loss [35]. Suppose we hold out a single item v and treat it unlabeled.
Then we predict its label by using the rest of (labeled) items and
(unlabeled) non-item entities. The prediction process is identical to label propagation in Theorem 2, except that the label of item v is
hidden and needs to be calculated. This way, the difference between the true relevancy label of v (i.e., yuv ) and the predicted label l^u (v) serves as a supervised signal for regularizing edge weights:

R(A) = R(Au ) =

J yuv , l^u (v) ,

(7)

u

uv

where J is the cross-entropy loss function. Given the regularization

in Eq. (7), an ideal edge weight matrix A should reproduce the

true relevancy label of each held-out item while also satisfying the

smoothness of relevancy labels.

4.3 The Unified Loss Function

Combining knowledge-aware graph neural networks and LS regularization, we reach the following complete loss function:

min L
W, A

=

min
W, A

u,v

J (yuv , y^uv ) + R(A) +  F 22,

(8)

where F 22 is the L2-regularizer,  and  are balancing hyperparameters. In Eq. (8), the first term corresponds to the part of
GNN that learns the transformation matrix W and edge weights A simultaneously, while the second term R(·) corresponds to the part
of label smoothness that can be seen as adding constraint on edge weights A. Therefore, R(·) serves as regularization on A to assist
GNN in learning edge weights.
It is also worth noticing that the first term can be seen as feature propagation on the KG while the second term R(·) can be seen as label propagation on the KG. A recommender for a specific user u
is actually a mapping from item features to user-item interaction labels, i.e., Fu : Ev  yuv where Ev is the feature vector of item v. Therefore, Eq. (8) utilizes the structural information of the KG on both the feature side and the label side of Fu to capture users' higher-order preferences.

4.4 Discussion
How can the knowledge graph help find users' interests? To intuitively understand the role of the KG, we make an analogy with a

972

<927

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

# users # items # interactions # entities # relations # KG triples

Movie 138,159 16,954 13,501,622 102,569
32 499,474

Book 19,676 20,003 172,576 25,787
18 60,787

Music 1,872 3,846 42,346 9,366
60 15,518

Restaurant 2,298,698
1,362 23,416,418
28,115 7
160,519

Table 2: Statistics of the four datasets: MovieLens-20M (movie), Book-Crossing (book), Last.FM (music), and Dianping-Food (restaurant).

physical equilibrium model as shown in Figure 2. Each entity/item is seen as a particle, while the supervised positive user-relevancy signal acts as the force pulling the observed positive items up from the decision boundary and the negative items signal acts as the force pushing the unobserved items down. Without the KG (Figure 2a), these items are only loosely connected with each other through the collaborative filtering effect (which is not drawn here for clarity). In contrast, edges in the KG serve as the rubber bands that impose explicit constraints on connected entities. When number of layers is L = 1 (Figure 2b), representation of each entity is a mixture of itself and its immediate neighbors, therefore, optimizing on the positive items will simultaneously pull their immediate neighbors up together. The upward force goes deeper in the KG with the increase of L (Figure 2c), which helps explore users' long-distance interests and pull up more positive items. It is also interesting to note that the proximity constraint exerted by the KG is personalized since the strength of the rubber band (i.e., su (r )) is user-specific and relation-specific: One user may prefer relation r1 (Figure 2b) while another user (with same observed items but different unobserved items) may prefer relation r2 (Figure 2d).
Despite the force exerted by edges in the KG, edge weights may be set inappropriately, for example, too small to pull up the unobserved items (i.e., rubber bands are too weak). Next, we show by Figure 2e that how the label smoothness assumption helps regularizing the learning of edge weights. Suppose we hold out the positive sample in the upper left and we intend to reproduce its label by the rest of items. Since the true relevancy label of the held-out sample is 1 and the upper right sample has the largest label value, the LS regularization term R(A) would enforce the edges with arrows to be large so that the label can "flow" from the blue one to the striped one as much as possible. As a result, this will tighten the rubber bands (denoted by arrows) and encourage the model to pull up the two upper pink items to a greater extent.
5 EXPERIMENTS
In this section, we evaluate the proposed KGNN-LS model, and present its performance on four real-world scenarios: movie, book, music, and restaurant recommendations.
5.1 Datasets
We utilize the following four datasets in our experiments for movie, book, music, and restaurant recommendations, respectively, in which the first three are public datasets and the last one is from

probability probability

1

without common rater

0.8

with common rater(s)

0.6

0.4

0.2

0

2

4

6

shortest distance

(a) MovieLens-20M

0.6 without common rater with common rater(s)
0.4
0.2
0 2 4 5 6 7 8 9 10 11 12 >12 shortest distance
(b) Last.FM

Figure 3: Probability distribution of the shortest path distance between two randomly sampled items in the KG under the circumstance that (1) they have no common user in the dataset; (2) they have common user(s) in the dataset.

Meituan-Dianping Group. We use Satori3, a commercial KG built by Microsoft, to construct sub-KGs for MovieLens-20M, BookCrossing, and Last.FM datasets. The KG for Dianping-Food dataset is constructed by the internal toolkit of Meituan-Dianping Group. Further details of datasets are provided in Appendix A.
· MovieLens-20M4 is a widely used benchmark dataset in movie recommendations, which consists of approximately 20 million explicit ratings (ranging from 1 to 5) on the MovieLens website. The corresponding KG contains 102,569 entities, 499,474 edges and 32 relation-types.
· Book-Crossing5 contains 1 million ratings (ranging from 0 to 10) of books in the Book-Crossing community. The corresponding KG contains 25,787 entities, 60,787 edges and 18 relation-types.
· Last.FM6 contains musician listening information from a set of 2 thousand users from Last.fm online music system. The corresponding KG contains 9,366 entities, 15,518 edges and 60 relation-types.
· Dianping-Food is provided by Dianping.com7, which contains over 10 million interactions (including clicking, buying, and adding to favorites) between approximately 2 million users and 1 thousand restaurants. The corresponding KG contains 28,115 entities, 160,519 edges and 7 relation-types.
The statistics of the four datasets are shown in Table 2.
5.2 Baselines
We compare the proposed KGNN-LS model with the following baselines for recommender systems, in which the first two baselines are KG-free while the rest are all KG-aware methods. The hyperparameter setting of KGNN-LS is provided in Appendix B.
· SVD [12] is a classic CF-based model using inner product to model user-item interactions. We use the unbiased version (i.e., the predicted engaging probability is modeled as yuv = uv). The dimension and learning rate for the four datasets are set as: d = 8,  = 0.5 for MovieLens-20M, Book-Crossing; d = 8,  = 0.1 for Last.FM; d = 32,  = 0.1 for Dianping-Food.
3 https://searchengineland.com/library/bing/bing-satori 4 https://grouplens.org/datasets/movielens/ 5 http://www2.informatik.uni- freiburg.de/~cziegler/BX/ 6 https://grouplens.org/datasets/hetrec- 2011/ 7 https://www.dianping.com/

973

928> 2019 

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

Model

MovieLens-20M

Book-Crossing

Last.FM

Dianping-Food

R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100 R@2 R@10 R@50 R@100

SVD

0.036 0.124 0.277 0.401 0.027 0.046 0.077 0.109 0.029 0.098 0.240 0.332 0.039 0.152 0.329 0.451

LibFM 0.039 0.121 0.271 0.388 0.033 0.062 0.092 0.124 0.030 0.103 0.263 0.330 0.043 0.156 0.332 0.448

LibFM + TransE 0.041 0.125 0.280 0.396 0.037 0.064 0.097 0.130 0.032 0.102 0.259 0.326 0.044 0.161 0.343 0.455

PER

0.022 0.077 0.160 0.243 0.022 0.041 0.064 0.070 0.014 0.052 0.116 0.176 0.023 0.102 0.256 0.354

CKE

0.034 0.107 0.244 0.322 0.028 0.051 0.079 0.112 0.023 0.070 0.180 0.296 0.034 0.138 0.305 0.437

RippleNet 0.045 0.130 0.278 0.447 0.036 0.074 0.107 0.127 0.032 0.101 0.242 0.336 0.040 0.155 0.328 0.440

KGNN-LS 0.043 0.155 0.321 0.458 0.045 0.082 0.117 0.149 0.044 0.122 0.277 0.370 0.047 0.170 0.340 0.487

Table 3: The results of Recall@K in top-K recommendation.

Model SVD LibFM LibFM + TransE PER CKE RippleNet KGNN-LS

Movie 0.963 0.959 0.966 0.832 0.924 0.960 0.979

Book 0.672 0.691 0.698 0.617 0.677 0.727 0.744

Music 0.769 0.778 0.777 0.633 0.744 0.770 0.803

Restaurant 0.838 0.837 0.839 0.746 0.802 0.833 0.850

Table 4: The results of AUC in CTR prediction.

· LibFM [16] is a widely used feature-based factorization model for CTR prediction. We concatenate user ID and item ID as input for LibFM. The dimension is set as {1, 1, 8} and the number of training epochs is 50 for all datasets.
· LibFM + TransE extends LibFM by attaching an entity representation learned by TransE [2] to each user-item pair. The dimension of TransE is 32 for all datasets.
· PER [33] is a representative of path-based methods, which treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity between users and items. We use manually designed "useritem-attribute-item" as meta-paths, i.e., "user-movie-directormovie", "user-movie-genre-movie", and "user-movie-star-movie" for MovieLens-20M; "user-book-author-book" and "user-bookgenre-book" for Book-Crossing, "user-musician-date_of_birthmusician" (date of birth is discretized), "user-musician-countrymusician", and "user-musician-genre-musician" for Last.FM; "user-restaurant-dish-restaurant", "user-restaurant-business_arearestaurant", "user-restaurant-tag-restaurant" for Dianping-Food. The settings of dimension and learning rate are the same as SVD.
· CKE [34] is a representative of embedding-based methods, which combines CF with structural, textual, and visual knowledge in a unified framework. We implement CKE as CF plus a structural knowledge module in this paper. The dimension of embedding for the four datasets are 64, 128, 64, 64. The training weight for KG part is 0.1 for all datasets. The learning rate are the same as in SVD.
· RippleNet [24] is a representative of hybrid methods, which is a memory-network-like approach that propagates users' preferences on the KG for recommendation. For RippleNet, d = 8, H = 2, 1 = 10-6, 2 = 0.01,  = 0.01 for MovieLens20M; d = 16, H = 3, 1 = 10-5, 2 = 0.02,  = 0.005 for

Last.FM; d = 32, H = 2, 1 = 10-7, 2 = 0.02,  = 0.01 for Dianping-Food.
5.3 Validating the Connection between G and Y
To validate the connection between the knowledge graph G and user-item interaction Y, we conduct an empirical study where we investigate the correlation between the shortest path distance of two randomly sampled items in the KG and whether they have common user(s) in the dataset, that is there exist user(s) that interacted with both items. For MovieLens-20M and Last.FM, we randomly sample ten thousand item pairs that have no common users and have at least one common user, respectively, then count the distribution of their shortest path distances in the KG. The results are presented in Figure 3, which clearly show that if two items have common user(s) in the dataset, they are likely to be more close in the KG. For example, if two movies have common user(s) in MovieLens-20M, there is a probability of 0.92 that they will be within 2 hops in the KG, while the probability is 0.80 if they have no common user. This finding empirically demonstrates that exploiting the proximity structure of the KG can assist making recommendations. This also justifies our motivation to use label smoothness regularization to help learn entity representations.
5.4 Results
5.4.1 Comparison with Baselines. We evaluate our method in two experiment scenarios: (1) In top-K recommendation, we use the trained model to select K items with highest predicted click probability for each user in the test set, and choose Recall@K to evaluate the recommended sets. (2) In click-through rate (CTR) prediction, we apply the trained model to predict each piece of user-item pair in the test set (including positive items and randomly selected negative items). We use AU C as the evaluation metric in CTR prediction.
The results of top-K recommendation and CTR prediction are presented in Tables 3 and 4, respectively, which show that KGNNLS outperforms baselines by a significant margin. For example, the AU C of KGNN-LS surpasses baselines by 5.1%, 6.9%, 8.3%, and 4.3% on average in MovieLens-20M, Book-Crossing, Last.FM, and Dianping-Food datasets, respectively.
We also show daily performance of KGNN-LS and baselines on Dianping-Food to investigate performance stability. Figure 4 shows their AU C score from September 1, 2018 to September 30, 2018. We notice that the curve of KGNN-LS is consistently above baselines over the test period; Moreover, the performance of KGNN-LS is also

974

Research Track Paper

<929
KDD '19, August 4­8, 2019, Anchorage, AK, USA

Figure 4: Daily AUC of all methods on Dianping-Food in September 2018.

Figure 5: Effectiveness of LS regularization on Last.FM.

Figure 6: Running time of all methods w.r.t. KG size on MovieLens-20M.

r SVD LibFM LibFM+TransE PER CKE RippleNet KGNN-LS

20% 0.882 0.902 0.914 0.802 0.898 0.921 0.961

40% 0.913 0.923 0.935 0.814 0.910 0.937 0.970

60% 0.938 0.938 0.949 0.821 0.916 0.947 0.974

80% 0.955 0.950 0.960 0.828 0.921 0.955 0.977

100% 0.963 0.959 0.966 0.832 0.924 0.960 0.979

Table 5: AUC of all methods w.r.t. the ratio of training set r .

with low variance, which suggests that KGNN-LS is also robust and stable in practice.
5.4.2 Effectiveness of LS Regularization. Is the proposed LS regularization helpful in improving the performance of GNN? To study the effectiveness of LS regularization, we fix the dimension of hidden layers as 4, 8, and 16, then vary  from 0 to 5 to see how performance changes. The results of R@10 in Last.FM dataset are plotted in Figure 5. It is clear that the performance of KGNN-LS with a non-zero  is better than  = 0 (the case of Wang et al. [28]), which justifies our claim that LS regularization can assist learning the edge weights in a KG and achieve better generalization in recommender systems. But note that a too large  is less favorable, since it overwhelms the overall loss and misleads the direction of gradients. According to the experiment results, we find that a  between 0.1 and 1.0 is preferable in most cases.
5.4.3 Results in cold-start scenarios. One major goal of using KGs in recommender systems is to alleviate the sparsity issue. To investigate the performance of KGNN-LS in cold-start scenarios, we vary the size of training set of MovieLens-20M from r = 100% to r = 20% (while the validation and test set are kept fixed), and report the results of AUC in Table 5. When r = 20%, AU C decreases by 8.4%, 5.9%, 5.4%, 3.6%, 2.8%, and 4.1% for the six baselines compared to the model trained on full training data (r = 100%), but the performance decrease of KGNN-LS is only 1.8%. This demonstrates that KGNN-LS still maintains predictive performance even when user-item interactions are sparse.
5.4.4 Hyper-parameters Sensitivity. We first analyze the sensitivity of KGNN-LS to the number of GNN layers L. We vary L from 1 to 4 while keeping other hyper-parameters fixed. The results are shown in Table 6. We find that the model performs poorly when L = 4, which is because a larger L will mix too many entity embeddings

L MovieLens-20M Book-Crossing
Last.FM Dianping-Food

1
0.155 0.077 0.122 0.165

2 0.146 0.082 0.106 0.170

3 0.122 0.043 0.105 0.061

4 0.011 0.008 0.057 0.036

Table 6: R@10 w.r.t. the number of layers L.

d MovieLens-20M Book-Crossing
Last.FM Dianping-Food

4 0.134 0.065 0.111 0.155

8 0.141 0.073 0.116 0.170

16 0.143 0.077 0.122 0.167

32 0.155 0.081 0.109 0.166

64
0.155 0.082 0.102 0.163

128 0.151 0.080 0.107 0.161

Table 7: R@10 w.r.t. the dimension of hidden layers d.

in a given entity, which over-smoothes the representation learning on KGs. KGNN-LS achieves the best performance when L = 1 or 2 in the four datasets.
We also examine the impact of the dimension of hidden layers d on the performance of KGNN-LS. The result in shown in Table 7. We observe that the performance is boosted with the increase of d at the beginning, because more bits in hidden layers can improve the model capacity. However, the performance drops when d further increases, since a too large dimension may overfit datasets. The best performance is achieved when d = 8  64.
5.5 Running Time Analysis
We also investigate the running time of our method with respect to the size of KG. We run experiments on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM. The size of the KG is increased by up to five times the original one by extracting more triples from Satori, and the running times of all methods on MovieLens-20M are reported in Figure 6. Note that the trend of a curve matters more than the real values, since the values are largely dependent on the minibatch size and the number of epochs (yet we did try to align the configurations of all methods). The result show that KGNN-LS exhibits strong scalability even when the KG is large.
6 CONCLUSION AND FUTURE WORK
In this paper, we propose knowledge-aware graph neural networks with label smoothness regularization for recommendation. KGNNLS applies GNN architecture to KGs by using user-specific relation

975

930> 2019 

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

scoring functions and aggregating neighborhood information with
different weights. In addition, the proposed label smoothness con-
straint and leave-one-out loss provide strong regularization for
learning the edge weights in KGs. We also discuss how KGs ben-
efit recommender systems and how label smoothness can assist
learning the edge weights. Experiment results show that KGNN-
LS outperforms state-of-the-art baselines in four recommendation
scenarios and achieves desirable scalability with respect to KG size.
In this paper, LS regularization is proposed for recommendation
task with KGs. It is interesting to examine the LS assumption on
other graph tasks such as link prediction and node classification.
Investigating the theoretical relationship between feature propaga-
tion and label propagation is also a promising direction.
Acknowledgements. This research has been supported in part by NSF OAC-1835598, DARPA MCS, ARO MURI, Boeing, Docomo,
Hitachi, Huawei, JD, Siemens, and Stanford Data Science Initiative.
REFERENCES
[1] Shumeet Baluja, Rohan Seth, D Sivakumar, Yushi Jing, Jay Yagnik, Shankar Kumar, Deepak Ravichandran, and Mohamed Aly. 2008. Video suggestion and discovery for youtube: taking random walks through the view graph. In Proceedings of the 17th international conference on World Wide Web. ACM, 895­904.
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems. 2787­2795.
[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann Lecun. 2014. Spectral networks and locally connected networks on graphs. In the 2nd International Conference on Learning Representations.
[4] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems. ACM, 191­198.
[5] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems. 3844­3852.
[6] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan P Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In Advances in Neural Information Processing Systems. 2224­2232.
[7] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems. 1024­1034.
[8] Binbin Hu, Chuan Shi, Wayne Xin Zhao, and Philip S Yu. 2018. Leveraging metapath based context for top-n recommendation with a neural co-attention model. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 1531­1540.
[9] Jin Huang, Wayne Xin Zhao, Hongjian Dou, Ji-Rong Wen, and Edward Y Chang. 2018. Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks. In the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval. ACM, 505­514.
[10] Masayuki Karasuyama and Hiroshi Mamitsuka. 2013. Manifold-based similarity adaptation for label propagation. In Advances in neural information processing systems. 1547­1555.
[11] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In the 5th International Conference on Learning Representations.
[12] Yehuda Koren. 2008. Factorization meets the neighborhood: a multifaceted collaborative filtering model. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 426­434.
[13] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 8 (2009), 30­37.
[14] Federico Monti, Michael Bronstein, and Xavier Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In Advances in Neural Information Processing Systems. 3697­3707.
[15] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. 2016. Learning convolutional neural networks for graphs. In International Conference on Machine Learning. 2014­2023.
[16] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 3 (2012), 57.
[17] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional

networks. In European Semantic Web Conference. Springer, 593­607. [18] Zhu Sun, Jie Yang, Jie Zhang, Alessandro Bozzon, Long-Kai Huang, and Chi Xu.
2018. Recurrent knowledge graph embedding for effective recommendation. In Proceedings of the 12th ACM Conference on Recommender Systems. ACM, 297­305. [19] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph Convolutional Matrix Completion. stat 1050 (2017), 7. [20] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of the 6th International Conferences on Learning Representations. [21] Fei Wang and Changshui Zhang. 2008. Label propagation through linear neighborhoods. IEEE Transactions on Knowledge and Data Engineering 20, 1 (2008), 55­67. [22] Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, and Minyi Guo. 2017. Joint topic-semantic-aware social recommendation for online voting. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 347­356. [23] Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, and Qi Liu. 2018. Shine: Signed heterogeneous information network embedding for sentiment link prediction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 592­600. [24] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM, 417­426. [25] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019. Exploring High-Order User Preference on the Knowledge Graph for Recommender Systems. ACM Transactions on Information Systems (TOIS) 37, 3 (2019), 32. [26] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep Knowledge-Aware Network for News Recommendation. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. 1835­1844. [27] Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019. Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation. In Proceedings of the 2019 World Wide Web Conference on World Wide Web. [28] Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, and Minyi Guo. 2019. Knowledge graph convolutional networks for recommender systems. In Proceedings of the 2019 World Wide Web Conference on World Wide Web. [29] Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. 2018. Billion-scale commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 839­848. [30] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering 29, 12 (2017), 2724­2743. [31] Yuexin Wu, Hanxiao Liu, and Yiming Yang. 2018. Graph Convolutional Matrix Completion for Bipartite Edge Prediction. In the 10th International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management. [32] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 974­983. [33] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining. ACM, 283­292. [34] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 353­362. [35] Xinhua Zhang and Wee S Lee. 2007. Hyperparameter learning for graph based semi-supervised learning algorithms. In Advances in neural information processing systems. 1585­1592. [36] Huan Zhao, Quanming Yao, Jianda Li, Yangqiu Song, and Dik Lun Lee. 2017. Metagraph based recommendation fusion over heterogeneous information networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 635­644. [37] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Schölkopf. 2004. Learning with local and global consistency. In Advances in neural information processing systems. 321­328. [38] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International conference on Machine learning. 912­919. [39] Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. 2005. Semi-supervised learning with graphs. Ph.D. Dissertation. Carnegie Mellon University, language technologies institute, school of computer science.

976

<931

Research Track Paper

KDD '19, August 4­8, 2019, Anchorage, AK, USA

APPENDIX
A Additional Details on Datasets
MovieLens-20M, Book-Crossing, and Last.FM datasets contain explicit feedbacks data (Last.FM provides the listening count as weight for each user-item interaction). Therefore, we transform them into implicit feedback, where each entry is marked with 1 indicating that the user has rated the item positively. The threshold of positive rating is 4 for MovieLens-20M, while no threshold is set for BookCrossing and Last.FM due to their data sparsity. Additionally, we randomly sample an unwatched set of items and mark them as 0 for each user, the number of which equals his/her positively-rated ones.
We use Microsoft Satori to construct the KGs for MovieLens20M, Book-Crossing, and Last.FM datasets. In each triple of Satori KG, the head and the tail are either IDs or textual content, and the relation follows the format "domain.head_category.tail_category" (e.g., "book.book.author"). We first select a subset of triples from the whole Satori KG with a confidence level greater than 0.9. Then we collect Satori IDs of all valid movies/books/musicians by matching their names with the tail of triples (head, film.film.name, tail), (head, book.book.title, tail), or (head, type.object.name, tail), respectively, for the three datasets. Items with multiple matched or no matched entities are excluded for simplicity. After obtaining the set of item IDs, we match these item IDs with the head of all triples in Satori sub-KG, and select all well-matched triples as the final KG for each dataset.
Dianping-Food dataset is collected from Dianping.com, a Chinese group buying website hosting consumer reviews of restaurants similar to Yelp. We select approximately 10 million interactions between users and restaurants in Dianping.com from May 1, 2015 to December 12, 2018. The types of positive interactions include clicking, buying, and adding to favorites, and we sample negative interactions for each user. The KG for Dianping-Food is collected from Meituan Brain, an internal knowledge graph built for dining and entertainment by Meituan-Dianping Group. The types of entities include POI (restaurant), city, first-level and second-level category, star, business area, dish, and tag; The types of relations correspond to the types of entities (e.g., "organization.POI.has_dish").

Movie Book Music Restaurant

S 16

8

8

4

d 32

64

16

8

L

1

2

1

2

 1.0

0.5

0.1

 10-7 2 × 10-5 10-4

 2 × 10-2 2 × 10-4 5 × 10-4

0.5 10-7 2 × 10-2

Table 8: Hyper-parameter settings for the four datasets (S: number of sampled neighbors for each entity; d: dimension of hidden layers, L: number of layers, : label smoothness regularizer weight,  : L2 regularizer weight, : learning rate).

B Additional Details on Hyper-parameter Searching
In KGNN-LS, we set functions  and f as inner product,  as ReLU for non-last-layers and tanh for the last-layer. Note that the number of neighbors of an entity in a KG may be significantly different from each other. Therefore, we uniformly sample a fixed-size set of neighbors for each entity instead of using all of its neighbors to keep the computation more efficient. The number of sampled neighbors for each entity is denoted by S. Hyper-parameter settings for the four datasets are given in Table 8, which are determined by optimizing R@10 on a validation set. The search spaces for hyper-parameters are as follows:
· S = {2, 4, 8, 16, 32}; · d = {4, 8, 16, 32, 64, 128}; · L = {1, 2, 3, 4}; ·  = {0, 0.01, 0.1, 0.5, 1, 5}; ·  = {10-9, 10-8, 10-7, 2 × 10-7, 5 × 10-7, 10-6, 2 × 10-6, 5 ×
10-6, 10-5, 2 × 10-5, 5 × 10-5, 10-4, 2 × 10-4, 5 × 10-4, 10-3}; ·  = {10-5, 2 × 10-5, 5 × 10-5, 10-4, 2 × 10-4, 5 × 10-4, 10-3, 2 ×
10-3, 5 × 10-3, 10-2, 2 × 10-2, 5 × 10-2, 10-1}.
For each dataset, the ratio of training, validation, and test set is 6 : 2 : 2. Each experiment is repeated 5 times, and the average performance is reported. All trainable parameters are optimized by Adam algorithm.

977

932> 2019 



The Web Conference 2019

arXiv:1901.08907v1 [cs.IR] 23 Jan 2019

Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation

Hongwei Wang1,2, Fuzheng Zhang3, Miao Zhao4, Wenjie Li4, Xing Xie2, Minyi Guo1
1Shanghai Jiao Tong University, Shanghai, China 2Microsoft Research Asia, Beijing, China 3Meituan-Dianping Group, Beijing, China
4The Hong Kong Polytechnic University, Hong Kong, China
wanghongwei55@gmail.com,zhangfuzheng@meituan.com,{csmiaozhao,cswjli}@comp.polyu.edu.hk
xingx@microsoft.com,guo-my@cs.sjtu.edu.cn

ABSTRACT
Collaborative filtering often suffers from sparsity and cold start problems in real recommendation scenarios, therefore, researchers and engineers usually use side information to address the issues and improve the performance of recommender systems. In this paper, we consider knowledge graphs as the source of side information. We propose MKR, a Multi-task feature learning approach for Knowledge graph enhanced Recommendation. MKR is a deep end-to-end framework that utilizes knowledge graph embedding task to assist recommendation task. The two tasks are associated by cross&compress units, which automatically share latent features and learn high-order interactions between items in recommender systems and entities in the knowledge graph. We prove that cross&compress units have sufficient capability of polynomial approximation, and show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning. Through extensive experiments on real-world datasets, we demonstrate that MKR achieves substantial gains in movie, book, music, and news recommendation, over state-of-theart baselines. MKR is also shown to be able to maintain a decent performance even if user-item interactions are sparse.
KEYWORDS
Recommender systems; knowledge graph; multi-task learning
ACM Reference Format: Hongwei Wang, Fuzheng Zhang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2019. Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation In Proceedings of The 2019 Web Conference (WWW 2019). ACM, New York, NY, USA, 11 pages. https://doi.org/xxxxx
1 INTRODUCTION
Recommender systems (RS) aims to address the information explosion and meet users personalized interests. One of the most popular recommendation techniques is collaborative filtering (CF) [11], which utilizes users' historical interactions and makes recommendations based on their common preferences. However, CF-based methods usually suffer from the sparsity of user-item interactions and the cold start problem. Therefore, researchers propose using
M. Guo is the corresponding author. This work was partially sponsored by the National Basic Research 973 Program of China under Grant 2015CB352403.
WWW 2019, May 13­17, 2019, San Francisco, USA 2019. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn

side information in recommender systems, including social networks [10], attributes [30], and multimedia (e.g., texts [29], images [40]). Knowledge graphs (KGs) are one type of side information for RS, which usually contain fruitful facts and connections about items. Recently, researchers have proposed several academic and commercial KGs, such as NELL1, DBpedia2, Google Knowledge Graph3 and Microsoft Satori4. Due to its high dimensionality and heterogeneity, a KG is usually pre-processed by knowledge graph embedding (KGE) methods [27], which embeds entities and relations into low-dimensional vector spaces while preserving its inherent structure.
Existing KG-aware methods Inspired by the success of applying KG in a wide variety of tasks,
researchers have recently tried to utilize KG to improve the performance of recommender systems [31, 32, 39, 40, 45]. Personalized Entity Recommendation (PER) [39] and Factorization Machine with Group lasso (FMG) [45] treat KG as a heterogeneous information network, and extract meta-path/meta-graph based latent features to represent the connectivity between users and items along different types of relation paths/graphs. It should be noted that PER and FMG rely heavily on manually designed meta-paths/meta-graphs, which limits its application in generic recommendation scenarios. Deep Knowledge-aware Network (DKN) [32] designs a CNN framework to combine entity embeddings with word embeddings for news recommendation. However, the entity embeddings are required in advance of using DKN, causing DKN to lack an endto-end way of training. Another concern about DKN is that it can hardly incorporate side information other than texts. RippleNet [31] is a memory-network-like model that propagates users' potential preferences in the KG and explores their hierarchical interests. But the importance of relations is weakly characterized in RippleNet, because the embedding matrix of a relation R can hardly be trained to capture the sense of importance in the quadratic form vRh (v and h are embedding vectors of two entities). Collaborative Knowledge base Embedding (CKE) [40] combines CF with structural knowledge, textual knowledge, and visual knowledge in a unified framework. However, the KGE module in CKE (i.e., TransR [13]) is more suitable for in-graph applications (such as KG completion and link prediction) rather than recommendation. In addition, the CF module and the KGE module are loosely coupled
1 http://rtw.ml.cmu.edu/rtw/ 2 http://wiki.dbpedia.org/ 3 https://developers.google.com/knowledge- graph/ 4 https://searchengineland.com/library/bing/bing- satori

<933

in CKE under a Bayesian framework, making the supervision from KG less obvious for recommender systems.
The proposed approach To address the limitations of previous work, we propose MKR, a
multi-task learning (MTL) approach for knowledge graph enhanced recommendation. MKR is a generic, end-to-end deep recommendation framework, which aims to utilize KGE task to assist recommendation task5. Note that the two tasks are not mutually independent, but are highly correlated since an item in RS may associate with one or more entities in KG. Therefore, an item and its corresponding entity are likely to have a similar proximity structure in RS and KG, and share similar features in low-level and non-task-specific latent feature spaces [15]. We will further validate the similarity in the experiments section. To model the shared features between items and entities, we design a cross&compress unit in MKR. The cross&compress unit explicitly models high-order interactions between item and entity features, and automatically control the cross knowledge transfer for both tasks. Through cross&compress units, representations of items and entities can complement each other, assisting both tasks in avoiding fitting noises and improving generalization. The whole framework can be trained by alternately optimizing the two tasks with different frequencies, which endows MKR with high flexibility and adaptability in real recommendation scenarios.
We probe the expressive capability of MKR and show, through theoretical analysis, that the cross&compress unit is capable of approximating sufficiently high order feature interactions between items and entities. We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning, including factorization machines [22, 23], deep&cross network [34], and cross-stitch network [18]. Empirically, we evaluate our method in four recommendation scenarios, i.e., movie, book, music, and news recommendations. The results demonstrate that MKR achieves substantial gains over state-of-theart baselines in both click-through rate (CTR) prediction (e.g., 11.6% AU C improvements on average for movies) and top-K recommendation (e.g., 66.4% Recall@10 improvements on average for books). MKR can also maintain a decent performance in sparse scenarios.
Contribution It is worth noticing that the problem studied in this paper can
also be modelled as cross-domain recommendation [26] or transfer learning [21], since we care more about the performance of recommendation task. However, the key observation is that though cross-domain recommendation and transfer learning have single objective for the target domain, their loss functions still contain constraint terms for measuring data distribution in the source domain or similarity between two domains. In our proposed MKR, the KGE task serves as the constraint term explicitly to provide regularization for recommender systems. We would like to emphasize that the major contribution of this paper is exactly modeling the problem as multi-task learning: We go a step further than cross-domain recommendation and transfer learning by finding that the intertask similarity is helpful to not only recommender systems but also
5KGE task can also benefit from recommendation task empirically as shown in the experiments section.

knowledge graph embedding, as shown in theoretical analysis and experiment results.

2 OUR APPROACH
In this section, we first formulate the knowledge graph enhanced recommendation problem, then introduce the framework of MKR and present the design of the cross&compress unit, recommendation module and KGE module in detail. We lastly discuss the learning algorithm for MKR.

2.1 Problem Formulation

We formulate the knowledge graph enhanced recommendation

problem in this paper as follows. In a typical recommendation

scenario, we have a set of M users U = {u1, u2, ..., uM } and a set

of N items Y  RM×N

V is

= {v1, v2, ..., vN }. The user-item interaction defined according to users' implicit feedback,

matrix where

yuv = 1 indicates that user u engaged with item v, such as behaviors of clicking, watching, browsing, or purchasing; otherwise yuv = 0.
Additionally, we also have access to a knowledge graph G, which is

comprised of entity-relation-entity triples (h, r, t). Here h, r , and t

denote the head, relation, and tail of a knowledge triple, respectively.

For example, the triple (Quentin Tarantino, film.director.film, Pulp

Fiction) states the fact that Quentin Tarantino directs the film Pulp

Fiction. In many recommendation scenarios, an item v  V may

associate with one or more entities in G. For example, in movie

recommendation, the item "Pulp Fiction" is linked with its namesake

in a KG, while in news recommendation, news with the title "Trump

pledges aid to Silicon Valley during tech meeting" is linked with

entities "Donald Trump" and "Silicon Valley" in a KG.

Given the user-item interaction matrix Y as well as the knowl-

edge graph G, we aim to predict whether user u has potential

interest in item v with which he has had no interaction before. Our

goal is to learn a prediction function y^uv = F (u, v |, Y, G), where y^uv denotes the probability that user u will engage with item v, and  is the model parameters of function F .

2.2 Framework
The framework of MKR is illustrated in Figure 1a. MKR consists of three main components: recommendation module, KGE module, and cross&compress units. (1) The recommendation module on the left takes a user and an item as input, and uses a multi-layer perceptron (MLP) and cross&compress units to extract short and dense features for the user and the item, respectively. The extracted features are then fed into another MLP together to output the predicted probability. (2) Similar to the left part, the KGE module in the right part also uses multiple layers to extract features from the head and relation of a knowledge triple, and outputs the representation of the predicted tail under the supervision of a score function f and the real tail. (3) The recommendation module and the KGE module are bridged by specially designed cross&compress units. The proposed unit can automatically learn high-order feature interactions of items in recommender systems and entities in the KG.

2.3 Cross&compress Unit
To model feature interactions between items and entities, we design a cross&compress unit in MKR framework. As shown in Figure 1b,

934> 2019 

Recommendation target 

predicted probability





Knowledge Graph Embedding

tail





predicted tail



[ ]









1





user

item

cross&compress units









[ ]





1



 head

 relation

(a) Framework of MKR

Layer  +1 Compress

+1

  

+1

  

Cross feature matrix



T

Cross

Layer 





(b) Cross&compress unit

Figure 1: (a) The framework of MKR. The left and right part illustrate the recommendation module and the KGE module, respectively, which are bridged by the cross&compress units. (b) Illustration of a cross&compress unit. The cross&compress unit generates a cross feature matrix from item and entity vectors by cross operation, and outputs their vectors for the next layer by compress operation.

for item v and one of its associated entities e, we first construct d ×d pairwise interactions of their latent feature vl  Rd and el  Rd from layer l:

Cl = vl el = vvl(l(d1· ))·ee·l(l(11))

··· ···

vl(1)el(d ) ···
vl(d )el(d )



,

(1)

where Cl  Rd×d is the cross feature matrix of layer l, and d is the dimension of hidden layers. This is called the cross operation, since each possible feature interaction vl(i)el(j), (i, j)  {1, ..., d }2 between item v and its associated entity e is modeled explicitly

in the cross feature matrix. We then output the feature vectors of

items and entities for the next layer by projecting the cross feature

matrix into their latent representation spaces:

vl +1 =Cl wVl V + ClwlEV + bVl = vl elwVl V + el vlwlEV + bVl , el +1 =Cl wVl E + ClwlEE + blE = vl elwVl E + el vlwlEE + blE ,
(2)

where wl··  Rd and bl·  Rd are trainable weight and bias vectors. This is called the compress operation, since the weight vectors project the cross feature matrix from Rd×d space back to the feature spaces Rd . Note that in Eq. (2), the cross feature matrix
is compressed along both horizontal and vertical directions (by operating on Cl and Cl) for the sake of symmetry, but we will provide more insights of the design in Section 3.2. For simplicity,
the cross&compress unit is denoted as:

[vl +1, el +1] = C(vl , el ),

(3)

and we use a suffix [v] or [e] to distinguish its two outputs in the

following of this paper. Through cross&compress units, MKR can

adaptively adjust the weights of knowledge transfer and learn the

relevance between the two tasks.

It should be noted that cross&compress units should only exist

in low-level layers of MKR, as shown in Figure 1a. This is because:

(1) In deep architectures, features usually transform from general to specific along the network, and feature transferability drops significantly in higher layers with increasing task dissimilarity [38]. Therefore, sharing high-level layers risks to possible negative transfer, especially for the heterogeneous tasks in MKR. (2) In highlevel layers of MKR, item features are mixed with user features, and entity features are mixed with relation features. The mixed features are not suitable for sharing since they have no explicit association.

2.4 Recommendation Module

The input of the recommendation module in MKR consists of two raw feature vectors u and v that describe user u and item v, respec-
tively. u and v can be customized as one-hot ID [8], attributes [30],
bag-of-words [29], or their combinations, based on the application scenario. Given user u's raw feature vector u, we use an L-layer MLP to extract his latent condensed feature6:

uL = M(M(· · · M(u))) = ML(u),

(4)

where M(x) =  (Wx+b) is a fully-connected neural network layer7

with weight W, bias b, and nonlinear activation function  (·). For

item v, we use L cross&compress units to extract its feature:





vL = EeS(v) CL(v, e)[v] ,

(5)

where S(v) is the set of associated entities of item v.

After having user u's latent feature uL and item v's latent feature

vL, we combine the two pathways by a predicting function fRS ,

for example, inner product or an H -layer MLP. The final predicted

probability of user u engaging item v is:





y^uv =  fRS (uL, vL) .

(6)

6We use the exponent notation L in Eq. (4) and following equations in the rest of this paper for simplicity, but note that the parameters of L layers are actually different. 7Exploring a more elaborate design of layers in the recommendation module is an important direction of future work.

<935

2.5 Knowledge Graph Embedding Module
Knowledge graph embedding is to embed entities and relations into continuous vector spaces while preserving their structure. Recently, researchers have proposed a great many KGE methods, including translational distance models [2, 13] and semantic matching models [14, 19]. In MKR, we propose a deep semantic matching architecture for KGE module. Similar to the recommendation module, for a given knowledge triple (h, r , t), we first utilize multiple cross&compress units and nonlinear layers to process the raw feature vectors of head h and relation r (including ID [13], types [36], textual description [35], etc.), respectively. Their latent features are then concatenated together, followed by a K-layer MLP for predicting tail t:

hL = EvS(h) CL(v, h)[e] ,

rL = ML(r),

(7)

^t = MK

hL rL

,

where S(h) is the set of associated items of entity h, and ^t is the predicted vector of tail t. Finally, the score of the triple (h, r , t) is calculated using a score (similarity) function fKG :

score(h, r , t) = fKG (t, ^t),

(8)

where t is the real feature vector of t. In this paper, we use the normalized inner product fKG (t, ^t) =  (t^t) as the choice of score function [18], but other forms of (dis)similarity metrics can also be
applied here such as KullbackâLeibler divergence.

2.6 Learning Algorithm
The complete loss function of MKR is as follows:

L =LRS + LKG + LREG

=

J (y^uv , yuv )

u U,v V

- 1

score(h, r , t) -

score(h, r , t ) (9)

(h,r,t ) G

(h,r,t ) G

+ 2 W22.

In Eq. (9), the first term measures loss in the recommendation module, where u and v traverse the set of users and the items, respectively, and J is the cross-entropy function. The second term calculates the loss in the KGE module, in which we aim to increase the score for all true triples while reducing the score for all false triples. The last item is the regularization term for preventing overfitting, 1 and 2 are the balancing parameters.8
Note that the loss function in Eq. (9) traverses all possible useritem pairs and knowledge triples. To make computation more efficient, following [17], we use a negative sampling strategy during training. The learning algorithm of MKR is presented in Algorithm 1, in which a training epoch consists of two stages: recommendation task (line 3-7) and KGE task (line 8-10). In each iteration, we repeat training on recommendation task for t times (t is a hyperparameter and normally t > 1) before training on KGE task once in

81 can be seen as the ratio of two learning rates for the two tasks.

Algorithm 1 Multi-Task Training for MKR
Input: Interaction matrix Y, knowledge graph G Output: Prediction function F (u, v |, Y, G)
1: Initialize all parameters 2: for number of training iteration do
// recommendation task 3: for t steps do 4: Sample minibatch of positive and negative interactions
from Y; 5: Sample e  S(v) for each item v in the minibatch; 6: Update parameters of F by gradient descent on Eq. (1)-(6),
(9);
7: end for // knowledge graph embedding task
8: Sample minibatch of true and false triples from G; 9: Sample v  S(h) for each head h in the minibatch; 10: Update parameters of F by gradient descent on Eq. (1)-(3),
(7)-(9);
11: end for

each epoch, since we are more focused on improving recommendation performance. We will discuss the choice of t in the experiments section.

3 THEORETICAL ANALYSIS
In this section, we prove that cross&compress units have sufficient capability of polynomial approximation. We also show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning.

3.1 Polynomial Approximation
According to the Weierstrass approximation theorem [25], any function under certain smoothness assumption can be approximated by a polynomial to an arbitrary accuracy. Therefore, we examine the ability of high-order interaction approximation of the cross&compress unit. We show that cross&compress units can model the order of item-entity feature interaction up to exponential degree:

Theorem 1. Denote the input of item and entity in MKR network

as v = [v1 · · · vd ] and e = [e1 · · · ed ], respectively. Then the

cross terms about v and e in vL 1 and eL 1 (the L1-norm of vL

and eL) k, 

with R, i

maximal degree is , i  N for i  {1,

k ···

, v11 · · · vdd ,d}, 1 + · · ·

e11 · + d

·

=· ed2dL-, 1w,haenrde

1 + · · · + d = 2L-1 (L  1, v0 = v, e0 = e).

In recommender systems,

d i =1

vii

eii

is

also

called

combina-

torial feature, as it measures the interactions of multiple original

features. Theorem 1 states that cross&compress units can automat-

ically model the combinatorial features of items and entities for

sufficiently high order, which demonstrates the superior approxi-

mation capacity of MKR as compared with existing work such as

Wide&Deep [3], factorization machines [22, 23] and DCN [34]. The

proof of Theorem 1 is provided in the Appendix. Note that The-

orem 1 gives a theoretical view of the polynomial approximation

936> 2019 

ability of the cross&compress unit rather than providing guarantees on its actual performance. We will empirically evaluate the cross&compress unit in the experiments section.

3.2 Unified View of Representative Methods
In the following we provide a unified view of several representative models in recommender systems and multi-task learning, by showing that they are restricted versions of or theoretically related to MKR. This justifies the design of cross&compress unit and conceptually explains its strong empirical performance as compared to baselines.

3.2.1 Factorization machines. Factorization machines [22, 23]

are a generic method for recommender systems. Given an input

feature vector, FMs model all interactions between variables in the

input vector using factorized parameters, thus being able to estimate

interactions in problems with huge sparsity such as recommender

systems. The model equation for a 2-degree factorization machine

is

defined as y^(x) = w0

+

d
i =1

wi

xi

+

d
i =1

d
j =i

+1

vi

,

vj

xi

x

j

,

(10)

where xi is the i-th unit of input vector x, w · is weight scalar, v· is weight vector, and ·, · is dot product of two vectors. We

show that the essence of FM is conceptually similar to an 1-layer

cross&compress unit:

Proposition 1. The L1-norm of v1 and e1 can be written as the

following form: v1 1 (or

e1 1)

=

b

+

d
i =1

d
j =1

wi

,

w

j

vi

e

j



,

(11)

where wi , wj  = wi + wj is the sum of two scalars.

It is interesting to notice that, instead of factorizing the weight parameter of xi xj into the dot product of two vectors as in FM, the weight of term viej is factorized into the sum of two scalars in cross&compress unit to reduce the number of parameters and
increase robustness of the model.

3.2.2 Deep&Cross Network. DCN [34] learns explicit and high-

order cross features by introducing the layers:

xl +1 = x0xlwl + xl + bl ,

(12)

where xl , wl , and bl are representation, weight, and bias of the l-th layer. We demonstrate the link between DCN and MKR by the

following proposition:

Proposition wVl V in the first

2. In term

ttohesaftoirsmfyuelalwofVl vVl

+1
=

in 1

Eq. and

(2), if we restrict restrict el in the

second term to be e0 (and impose similar restrictions on el+1), the

cross&compress unit is then conceptually equivalent to DCN layer in

the sense of multi-task learning:

vl +1 = e0vlwlEV + vl + bVl , el +1 = v0elwVl E + el + blE .

(13)

It can be proven that the polynomial approximation ability of

the above DCN-equivalent version (i.e., the maximal degree of

cross terms in vl and cross&compress units

welit)hisOO(2(ll)),awpphriochximisawtioeankaebriltihtya.n

original

3.2.3 Cross-stitch Networks. Cross-stitch networks [18] is a
multi-task learning model in convolutional networks, in which
the designed cross-stitch unit can learn a combination of shared
and task-specific representations between two tasks. Specifically, given two activation maps xA and xB from layer l for both the tasks, cross-stitch networks learn linear combinations x~A and x~B of both the input activations and feed these combinations as input to the next layers' filters. The formula at location (i, j) in the activation
map is

xx~~ABii jj 

=

 AA BA

 AB BB

xxABii jj 

,

(14)

where 's are trainable transfer weights of representations between

task A and task B. We show that the cross-stitch unit in Eq. (14) is

a simplified version of our cross&compress unit by the following

proposition:

Proposition 3. If we omit all biases in Eq. (2), the cross&compress

unit

can

be

written as  vl +1 el +1

=

elwVl V elwVl E

vlwlEV  vl wlE E

 vl el



.

(15)

The transfer matrix in Eq. (15) serves as the cross-stitch unit [AA AB ; BA BB ] in Eq. (14). Like cross-stitch networks, MKR network can decide to make certain layers task specific by setting vlwlEV (AB ) or elwVl E (BA) to zero, or choose a more shared representation by assigning a higher value to them. But the transfer matrix is more fine-grained in cross&compress unit, because the transfer weights are replaced from scalars to dot products of two vectors. It is rather interesting to notice that Eq. (15) can also be regarded as an attention mechanism [1], as the computation of transfer weights involves the feature vectors vl and el themselves.

4 EXPERIMENTS
In this section, we evaluate the performance of MKR in four realworld recommendation scenarios: movie, book, music, and news9.

4.1 Datasets
We utilize the following four datasets in our experiments:
· MovieLens-1M10 is a widely used benchmark dataset in movie recommendations, which consists of approximately 1
million explicit ratings (ranging from 1 to 5) on the Movie-
Lens website. · Book-Crossing11 dataset contains 1,149,780 explicit ratings
(ranging from 0 to 10) of books in the Book-Crossing com-
munity. · Last.FM12 dataset contains musician listening information
from a set of 2 thousand users from Last.fm online music
system. · Bing-News dataset contains 1,025,192 pieces of implicit
feedback collected from the server logs of Bing News13 from
9The source code is available at https://github.com/hwwang55/MKR. 10 https://grouplens.org/datasets/movielens/1m/ 11 http://www2.informatik.uni- freiburg.de/~cziegler/BX/ 12 https://grouplens.org/datasets/hetrec- 2011/ 13 https://www.bing.com/news

<937

Table 1: Basic statistics and hyper-parameter settings for the four datasets.

Dataset MovieLens-1M Book-Crossing
Last.FM Bing-News

# users 6,036 17,860 1,872 141,487

# items 2,347 14,910 3,846 535,145

# interactions 753,772 139,746 42,346 1,025,192

# KG triples 20,195 19,793 15,518
1,545,217

Hyper-parameters
L = 1, d = 8, t = 3, 1 = 0.5 L = 1, d = 8, t = 2, 1 = 0.1 L = 2, d = 4, t = 2, 1 = 0.1 L = 3, d = 16, t = 5, 1 = 0.2

October 16, 2016 to August 11, 2017. Each piece of news has a title and a snippet.
Since MovieLens-1M, Book-Crossing, and Last.FM are explicit feedback data (Last.FM provides the listening count as weight for each user-item interaction), we transform them into implicit feedback where each entry is marked with 1 indicating that the user has rated the item positively, and sample an unwatched set marked as 0 for each user. The threshold of positive rating is 4 for MovieLens1M, while no threshold is set for Book-Crossing and Last.FM due to their sparsity.
We use Microsoft Satori to construct the KG for each dataset. We first select a subset of triples from the whole KG with a confidence level greater than 0.9. For MovieLens-1M and Book-Crossing, we additionally select a subset of triples from the sub-KG whose relation name contains "film" or "book" respectively to further reduce KG size.
Given the sub-KGs, for MovieLens-1M, Book-Crossing, and Last.FM, we collect IDs of all valid movies, books, or musicians by matching their names with tail of triples (head, film.film.name, tail), (head, book.book.title, tail), or (head, type.object.name, tail), respectively. For simplicity, items with no matched or multiple matched entities are excluded. We then match the IDs with the head and tail of all KG triples and select all well-matched triples from the sub-KG. The constructing process is similar for Bing-News except that: (1) we use entity linking tools to extract entities in news titles; (2) we do not impose restrictions on the names of relations since the entities in news titles are not within one particular domain. The basic statistics of the four datasets are presented in Table 1. Note that the number of users, items, and interactions are smaller than original datasets since we filtered out items with no corresponding entity in the KG.
4.2 Baselines
We compare our proposed MKR with the following baselines. Unless otherwise specified, the hyper-parameter settings of baselines are the same as reported in their original papers or as default in their codes.
· PER [39] treats the KG as heterogeneous information networks and extracts meta-path based features to represent the connectivity between users and items. In this paper, we use manually designed user-item-attribute-item paths as features, i.e., "user-movie-director-movie", "user-moviegenre-movie", and "user-movie-star-movie" for MovieLens20M; "user-book-author-book" and "user-book-genre-book" for Book-Crossing; "user-musician-genre-musician", "usermusician-country-musician", and "user-musician-age-musician"

# common neighbors in KG # common raters in RS

25 20 15 10
5 0
<400 400-800 800-1200 1200-1600 >1600 # common raters of movie pair in RS
(a) RS to KG

30

20

10

0 <6

6-12 12-18 18-24 >24

# common neighbors of entity pair in KG

(b) KG to RS

Figure 2: The correlation between the number of common neighbors of an item pair in KG and their number of common raters in RS.

(age is discretized) for Last.FM. Note that PER cannot be applied to news recommendation because it's hard to pre-define meta-paths for entities in news. · CKE [40] combines CF with structural, textual, and visual knowledge in a unified framework for recommendation. We implement CKE as CF plus structural knowledge module in this paper. The dimension of user and item embeddings for the four datasets are set as 64, 128, 32, 64, respectively. The dimension of entity embeddings is 32. · DKN [32] treats entity embedding and word embedding as multiple channels and combines them together in CNN for CTR prediction. In this paper, we use movie/book names and news titles as textual input for DKN. The dimension of word embedding and entity embedding is 64, and the number of filters is 128 for each window size 1, 2, 3. · RippleNet [31] is a memory-network-like approach that propagates usersâ preferences on the knowledge graph for recommendation. The hyper-parameter settings for Last.FM are d = 8, H = 2, 1 = 10-6, 2 = 0.01,  = 0.02. · LibFM [23] is a widely used feature-based factorization model. We concatenate the raw features of users and items as well as the corresponding averaged entity embeddings learned from TransR [13] as input for LibFM. The dimension is {1, 1, 8} and the number of training epochs is 50. The dimension of TransR is 32. · Wide&Deep [3] is a deep recommendation model combining a (wide) linear channel with a (deep) nonlinear channel. The input for Wide&Deep is the same as in LibFM. The dimension of user, item, and entity is 64, and we use a two-layer deep channel with dimension of 100 and 50 as well as a wide channel.

938> 2019 

Table 2: The results of AUC and Accuracy in CTR prediction.

Model
PER CKE DKN RippleNet LibFM Wide&Deep MKR MKR-1L MKR-DCN MKR-stitch

MovieLens-1M

AU C

ACC

0.710 (-22.6%) 0.664 (-21.2%)

0.801 (-12.6%) 0.742 (-12.0%)

0.655 (-28.6%) 0.589 (-30.1%)

0.920 (+0.3%) 0.842 (-0.1%)

0.892 (-2.7%) 0.812 (-3.7%)

0.898 (-2.1%) 0.820 (-2.7%)

0.917

0.843

-

-

0.883 (-3.7%) 0.802 (-4.9%)

0.905 (-1.3%) 0.830 (-1.5%)

Book-Crossing

AU C

ACC

0.623 (-15.1%) 0.588 (-16.7%)

0.671 (-8.6%) 0.633 (-10.3%)

0.622 (-15.3%) 0.598 (-15.3%)

0.729 (-0.7%) 0.662 (-6.2%)

0.685 (-6.7%) 0.640 (-9.3%)

0.712 (-3.0%) 0.624 (-11.6%)

0.734

0.704

-

-

0.705 (-4.3%) 0.676 (-4.2%)

0.721 (-2.2%) 0.682 (-3.4%)

Last.FM

AU C

ACC

0.633 (-20.6%) 0.596 (-20.7%)

0.744 (-6.6%) 0.673 (-10.5%)

0.602 (-24.5%) 0.581 (-22.7%)

0.768 (-3.6%) 0.691 (-8.1%)

0.777 (-2.5%) 0.709 (-5.7%)

0.756 (-5.1%) 0.688 (-8.5%)

0.797

0.752

0.795 (-0.3%) 0.749 (-0.4%)

0.778 (-2.4%) 0.730 (-2.9%)

0.772 (-3.1%) 0.725 (-3.6%)

Bing-News

AU C

ACC

-

-

0.553 (-19.7%) 0.516 (-20.0%)

0.667 (-3.2%) 0.610 (-5.4%)

0.678 (-1.6%) 0.630 (-2.3%)

0.640 (-7.1%) 0.591 (-8.4%)

0.651 (-5.5%) 0.597 (-7.4%)

0.689

0.645

0.680 (-1.3%) 0.631 (-2.2%)

0.671 (-2.6%) 0.614 (-4.8%)

0.674 (-2.2%) 0.621 (-3.7%)

4.3 Experiments setup
In MKR, we set the number of high-level layers K = 1, fRS as inner product, and 2 = 10-6 for all three datasets, and other hyperparameter are given in Table 1. The settings of hyper-parameters are determined by optimizing AUC on a validation set. For each dataset, the ratio of training, validation, and test set is 6 : 2 : 2. Each experiment is repeated 3 times, and the average performance is reported. We evaluate our method in two experiment scenarios: (1) In click-through rate (CTR) prediction, we apply the trained model to each piece of interactions in the test set and output the predicted click probability. We use AUC and Accuracy to evaluate the performance of CTR prediction. (2) In top-K recommendation, we use the trained model to select K items with highest predicted click probability for each user in the test set, and choose Precision@K and Recall@K to evaluate the recommended sets.
4.4 Empirical study
We conduct an empirical study to investigate the correlation of items in RS and their corresponding entities in KG. Specifically, we aim to reveal how the number of common neighbors of an item pair in KG changes with their number of common raters in RS. To this end, we first randomly sample 1 million item pairs from MovieLens-1M. We then classify each pair into 5 categories based on the number of their common raters in RS, and count their average number of common neighbors in KG for each category. The result is presented in Figure 2a, which clearly shows that if two items have more common raters in RS, they are likely to share more common neighbors in KG. Figure 2b shows the positive correlation from an opposite direction. The above findings empirically demonstrate that items share the similar structure of proximity in KG and RS, thus the cross knowledge transfer of items benefits both recommendation and KGE tasks in MKR.
4.5 Results
4.5.1 Comparison with baselines. The results of all methods in CTR prediction and top-K recommendation are presented in Table 2 and Figure 3, 4, respectively. We have the following observations:
· PER performs poor on movie, book, and music recommendation because the user-defined meta-paths can hardly be

optimal in reality. Moreover, PER cannot be applied to news recommendation. · CKE performs better in movie, book, and music recommendation than news. This may be because MovieLens-1M, BookCrossing, and Last.FM are much denser than Bing-News, which is more favorable for the collaborative filtering part in CKE. · DKN performs best in news recommendation compared with other baselines, but performs worst in other scenarios. This is because movie, book, and musician names are too short and ambiguous to provide useful information. · RippleNet performs best among all baselines, and even outperforms MKR on MovieLens-1M. This demonstrates that RippleNet can precisely capture user interests, especially in the case where user-item interactions are dense. However, RippleNet is more sensitive to the density of datasets, as it performs worse than MKR in Book-Crossing, Last.FM, and Bing-News. We will further study their performance in sparse scenarios in Section 4.5.3. · In general, our MKR performs best among all methods on the four datasets. Specifically, MKR achieves average Accuracy gains of 11.6%, 11.5%, 12.7%, and 8.7% in movie, book, music, and news recommendation, respectively, which demonstrates the efficacy of the multi-task learning framework in MKR. Note that the top-K metrics are much lower for BingNews because the number of news is significantly larger than movies, books, and musicians.
4.5.2 Comparison with MKR variants. We further compare MKR with its three variants to demonstrate the efficacy of cross&compress unit:
· MKR-1L is MKR with one layer of cross&compress unit, which corresponds to FM model according to Proposition 1. Note that MKR-1L is actually MKR in the experiments for MovieLens-1M.
· MKR-DCN is a variant of MKR based on Eq. (13), which corresponds to DCN model.
· MKR-stitch is another variant of MKR corresponding to the cross-stitch network, in which the transfer weights in Eq. (15) are replaced by four trainable scalars.

<939

Precision@K Precision@K Precision@K Precision@K

PER
0.2

CKE
0.05

DKN

0R.02ippleNet 0 0.04

LibFM

Wide&Deep
6 ×10-3

MKR

0.04

5

0.15

0.03

4 0.03

0.1

0.02

3

0.02 2

0.05

0.01

0.01

1

0

2

5

10

20

50

K

0

2

5

10

20

50

K

0

0

2

5

10

20

50

2

K

5

10

20

50

K

(a) MovieLens-1M

(b) Book-Crossing

(c) Last.FM

(d) Bing-News

Figure 3: The results of Precision@K in top-K recommendation.

Recall@K Recall@K Recall@K Recall@K

PER
0.4

CKE
0.2

DKN

0R.02ippleNet 0 0.3

LibFM

Wide&Deep
0.2

MKR

0.25

0.3

0.15

0.15

0.2

0.2

0.1

0.15

0.1

0.1

0.1

0.05

0.05

0.05

0

2

5

10

20

50

K

0

2

5

10

20

50

K

0

2

5

10

20

50

K

0

2

5

10

20

50

K

(a) MovieLens-1M

(b) Book-Crossing

(c) Last.FM

(d) Bing-News

Figure 4: The results of Recall@K in top-K recommendation.

Table 3: Results of AUC on MovieLens-1M in CTR prediction with different ratios of training set r .

Model
PER CKE DKN RippleNet LibFM Wide&Deep MKR

10% 0.598 0.674 0.579 0.843 0.801 0.788 0.868

20% 0.607 0.692 0.582 0.851 0.810 0.802 0.874

30% 0.621 0.705 0.589 0.859 0.816 0.809 0.881

40% 0.638 0.716 0.601 0.862 0.829 0.815 0.882

r 50% 60% 0.647 0.662 0.739 0.754 0.612 0.620 0.870 0.878 0.837 0.850 0.821 0.840 0.889 0.897

70% 0.675 0.768 0.631 0.890 0.864 0.858 0.903

80% 0.688 0.775 0.638 0.901 0.875 0.876 0.908

90% 0.697 0.797 0.646 0.912 0.886 0.884 0.913

100% 0.710 0.801 0.655 0.920 0.892 0.898 0.917

From Table 2 we observe that MKR outperforms MKR-1L and MKR-DCN, which shows that modeling high-order interactions between item and entity features is helpful for maintaining decent performance. MKR also achieves better scores than MKR-stitch. This validates the efficacy of fine-grained control on knowledge transfer in MKR compared with the simple cross-stitch units.
4.5.3 Results in sparse scenarios. One major goal of using knowledge graph in MKR is to alleviate the sparsity and the cold start problem of recommender systems. To investigate the efficacy of

the KGE module in sparse scenarios, we vary the ratio of training set of MovieLens-1M from 100% to 10% (while the validation and test set are kept fixed), and report the results of AU C in CTR prediction for all methods. The results are shown in Table 3. We observe that the performance of all methods deteriorates with the reduce of the training set. When r = 10%, the AU C score decreases by 15.8%, 15.9%, 11.6%, 8.4%, 10.2%, 12.2% for PER, CKE, DKN, RippleNet, LibFM, and Wide&Deep, respectively, compared with the case when full training set is used (r = 100%). In contrast, the

940> 2019 

AUC / Accuracy AUC / Accuracy AUC / Accuracy

0.7

0.65

0.6

0.55 0.5 0.2

AUC Accuracy

0.4

0.6

0.8

1.0

Ratio of KG triples

(a) KG size

0.75 0.7

AUC Accuracy

0.65

0.6

0.55 1 2 3 4 5 6 7 8 9 10 t
(b) Training frequency

0.7

0.68 0.66

AUC Accuracy

0.64
2 4 8 16 32 64 128 d
(c) Dimension of embeddings

Figure 5: Parameter sensitivity of MKR on Bing-News w.r.t. (a) the size of the knowledge graph; (b) training frequency of the RS module t; and (c) dimension of embeddings d.

Table 4: The results of RMSE on the KGE module for the three datasets. "KGE" means only KGE module is trained, while "KGE + RS" means KGE module and RS module are trained together.

dataset MovieLens-1M Book-Crossing
Last.FM Bing-News

KGE 0.319 0.596 0.480 0.488

KGE + RS
0.302 0.558 0.471 0.459

AU C score of MKR only decreases by 5.3%, which demonstrates that MKR can still maintain a decent performance even when the user-item interaction is sparse. We also notice that MKR performs better than RippleNet in sparse scenarios, which is accordance with our observation in Section 4.5.1 that RippleNet is more sensitive to the density of user-item interactions.
4.5.4 Results on KGE side. Although the goal of MKR is to utilize KG to assist with recommendation, it is still interesting to investigate whether the RS task benefits the KGE task, since the principle of multi-task learning is to leverage shared information to help improve the performance of all tasks [42]. We present the result of RMSE (rooted mean square error) between predicted and real vectors of tails in the KGE task in Table 4. Fortunately, we find that the existence of RS module can indeed reduce the prediction error by 1.9%  6.4%. The results show that the cross&compress units are able to learn general and shared features that mutually benefit both sides of MKR.
4.6 Parameter Sensitivity
4.6.1 Impact of KG size. We vary the size of KG to further investigate the efficacy of usage of KG. The results of AU C on Bing-News are plotted in Figure 5a. Specifically, the AU C and Accuracy is enhanced by 13.6% and 11.8% with the KG ratio increasing from 0.1 to 1.0 in three scenarios, respectively. This is because the Bing-News dataset is extremely sparse, making the effect of KG usage rather obvious.
4.6.2 Impact of RS training frequency. We investigate the influence of parameters t in MKR by varying t from 1 to 10, while

keeping other parameters fixed. The results are presented in Figure 5b. We observe that MKR achieves the best performance when t = 5. This is because a high training frequency of the KGE module will mislead the objective function of MKR, while too small of a training frequency of KGE cannot make full use of the transferred knowledge from the KG.
4.6.3 Impact of embedding dimension. We also show how the dimension of users, items, and entities affects the performance of MKR in Figure 5c. We find that the performance is initially improved with the increase of dimension, because more bits in embedding layer can encode more useful information. However, the performance drops when the dimension further increases, as too large number of dimensions may introduce noises which mislead the subsequent prediction.
5 RELATED WORK
5.1 Knowledge Graph Embedding
The KGE module in MKR connects to a large body of work in KGE methods. KGE is used to embed entities and relations in a knowledge into low-dimensional vector spaces while still preserving the structural information [33]. KGE methods can be classified into the following two categories: (1) Translational distance models exploit distance-based scoring functions when learning representations of entities and relations, such as TransE [2], TransH [35], and TransR [13]; (2) Semantic matching models measure plausibility of knowledge triples by matching latent semantics of entities and relations, such as RESCAL [20], ANALOGY [19], and HolE [14]. Recently, researchers also propose incorporating auxiliary information, such as entity types [36], logic rules [24], and textual descriptions [46] to assist KGE. The above KGE methods can also be incorporated into MKR as the implementation of the KGE module, but note that the cross&compress unit in MKR needs to be redesigned accordingly. Exploring other designs of KGE module as well as the corresponding bridging unit is also an important direction of future work.
5.2 Multi-Task Learning
Multi-task learning is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks [42]. All of the learning tasks are assumed to be related to

<941

each other, and it is found that learning these tasks jointly can lead to performance improvement compared with learning them individually. In general, MTL algorithms can be classified into several categories, including feature learning approach [34, 41], low-rank approach [7, 16], task clustering approach [47], task relation learning approach [12], and decomposition approach [6]. For example, the cross-stitch network [41] determines the inputs of hidden layers in different tasks by a knowledge transfer matrix; Zhou et. al [47] aims to cluster tasks by identifying representative tasks which are a subset of the given m tasks, i.e., if task Ti is selected by task Tj as a representative task, then it is expected that model parameters for Tj are similar to those of Ti . MTL can also be combined with other learning paradigms to improve the performance of learning tasks further, including semi-supervised learning, active learning, unsupervised learning,and reinforcement learning.
Our work can be seen as an asymmetric multi-task learning framework [37, 43, 44], in which we aim to utilize the connection between RS and KG to help improve their performance, and the two tasks are trained with different frequencies.
5.3 Deep Recommender Systems
Recently, deep learning has been revolutionizing recommender systems and achieves better performance in many recommendation scenarios. Roughly speaking, deep recommender systems can be classified into two categories: (1) Using deep neural networks to process the raw features of users or items [5, 28­30, 40]; For example, Collaborative Deep Learning [29] designs autoencoders to extract short and dense features from textual input and feeds the features into a collaborative filtering module; DeepFM [5] combines factorization machines for recommendation and deep learning for feature learning in a neural network architecture. (2) Using deep neural networks to model the interaction among users and items [3, 4, 8, 9]. For example, Neural Collaborative Filtering [8] replaces the inner product with a neural architecture to model the user-item interaction. The major difference between these methods and ours is that MKR deploys a multi-task learning framework that utilizes the knowledge from a KG to assist recommendation.
6 CONCLUSIONS AND FUTURE WORK
This paper proposes MKR, a multi-task learning approach for knowledge graph enhanced recommendation. MKR is a deep and endto-end framework that consists of two parts: the recommendation module and the KGE module. Both modules adopt multiple nonlinear layers to extract latent features from inputs and fit the complicated interactions of user-item and head-relation pairs. Since the two tasks are not independent but connected by items and entities, we design a cross&compress unit in MKR to associate the two tasks, which can automatically learn high-order interactions of item and entity features and transfer knowledge between the two tasks. We conduct extensive experiments in four recommendation scenarios. The results demonstrate the significant superiority of MKR over strong baselines and the efficacy of the usage of KG.
For future work, we plan to investigate other types of neural networks (such as CNN) in MKR framework. We will also incorporate other KGE methods as the implementation of KGE module in MKR by redesigning the cross&compress unit.

APPENDIX

A Proof of Theorem 1

Proof. We prove the theorem by induction: Base case: When l = 1,

v1 =vewV0 V + evw0EV + bV0

d

d



= v1

ei

w

V 0

V

(i

)

···

vd

ei

w

V 0

V

(i

)

i =1

i =1

+ e1 d viw0EV (i) · · · ed d viw0EV (i) 

i =1

i =1

+

b0V (0)

· · · b0V (d)


.

Therefore, we have

v1 1 = d vj d eiw0V V (i) + d ej d viw0EV (i) + d b0V (d)

j=1 i=1

j=1 i=1

i =1

dd

d

=

(w0EV

(i )

+

w

V 0

V

(j ) )vi

e

j

+

b0V (d) .

i=1 j=1

i =1

It is clear that the cross terms about v and e with maximal degree

is k, vi ej , so we have 1 + · · · +d = 1 = 21-1, and 1 + · · · + d =

1 = 21-1 for v1. The proof for e1 is similar. Induction step: Suppose 1 + · · · + d = 2l-1 and 1 + · · · +
d = 2l-1 hold for the maximal-degree term x and y in vl 1 and

el 1. Since vl 1 =

d i =1

vl(i

)

and el 1

=

d i =1

el(i )

,

without

loss of generosity, we assume that x and y exist in vl(a) and el(b),

respectively. Then for l + 1, we have

dd

d

vl +1 1 =

(wlEV (i) + wlV V (j))vl(i)el(j) + blV (d).

i=1 j=1

i =1

xy

Obviously, the maximal-degree in vl(a)el(b). Since we have 1 +·

term in  · ·+d =

vl +1 1 is 2l -1 and

the cross 1 +· · ·+

term d =

2l-1 for both x and y, the degree of cross term xy therefore satisfies

1 + · · · + d = 2(l+1)-1 and 1 + · · · + d = 2(l+1)-1. The proof for

el+1 1 is similar.



B Proof of Proposition 1
Proof. In the proof of Theorem 1 in Appendix A, we have shown that

dd

d

v1 1 =

(w0EV (i) + w0V V (j))vi ej + b0V (d) .

i=1 j=1

i =1

It
d i =1

is easy to b0V (d). The

see that proof is

wi = similar

w0EV (i), w for e1 1.

j

=

w0V V (j), and b

= 

We omit the proofs for Proposition 2 and Proposition 3 as they are straightforward.

942> 2019 

REFERENCES
[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations.
[2] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In Advances in Neural Information Processing Systems. 2787­2795.
[3] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. 2016. Wide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 7­10.
[4] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM Conference on Recommender Systems. ACM, 191­198.
[5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction. In Proceedings of the 26th International Joint Conference on Artificial Intelligence.
[6] Lei Han and Yu Zhang. 2015. Learning tree structure in multi-task learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 397­406.
[7] Lei Han and Yu Zhang. 2016. Multi-Stage Multi-Task Learning with Reduced Rank.. In AAAI. 1638­1644.
[8] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web. 173­182.
[9] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM. ACM, 2333­2338.
[10] Mohsen Jamali and Martin Ester. 2010. A matrix factorization technique with trust propagation for recommendation in social networks. In Proceedings of the 4th ACM conference on Recommender systems. ACM, 135­142.
[11] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009).
[12] Giwoong Lee, Eunho Yang, and Sung Hwang. 2016. Asymmetric multi-task learning based on task relatedness and loss. In International Conference on Machine Learning. 230­238.
[13] Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation Embeddings for Knowledge Graph Completion.. In The 29th AAAI Conference on Artificial Intelligence. 2181­2187.
[14] Hanxiao Liu, Yuexin Wu, and Yiming Yang. 2017. Analogical Inference for MultiRelational Embeddings. In Proceedings of the 34th International Conference on Machine Learning. 2168­2178.
[15] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and S Yu Philip. 2017. Learning Multiple Tasks with Multilinear Relationship Networks. In Advances in Neural Information Processing Systems. 1593­1602.
[16] Andrew M McDonald, Massimiliano Pontil, and Dimitris Stamos. 2014. Spectral k-support norm regularization. In Advances in Neural Information Processing Systems. 3644­3652.
[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems. 3111­3119.
[18] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016. Cross-stitch networks for multi-task learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3994­4003.
[19] Maximilian Nickel, Lorenzo Rosasco, Tomaso A Poggio, et al. 2016. Holographic Embeddings of Knowledge Graphs.. In The 30th AAAI Conference on Artificial Intelligence. 1955­1961.
[20] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A Three-Way Model for Collective Learning on Multi-Relational Data. In Proceedings of the 28th International Conference on Machine Learning. 809­816.
[21] Sinno Jialin Pan, Qiang Yang, et al. 2010. A survey on transfer learning. IEEE Transactions on knowledge and data engineering 22, 10 (2010), 1345­1359.
[22] Steffen Rendle. 2010. Factorization machines. In Proceedings of the 10th IEEE International Conference on Data Mining. IEEE, 995­1000.
[23] Steffen Rendle. 2012. Factorization machines with libfm. ACM Transactions on Intelligent Systems and Technology (TIST) 3, 3 (2012), 57.
[24] Tim Rocktäschel, Sameer Singh, and Sebastian Riedel. 2015. Injecting logical background knowledge into embeddings for relation extraction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 1119­1129.
[25] Walter Rudin et al. 1964. Principles of mathematical analysis. Vol. 3. McGraw-hill New York.
[26] Jie Tang, Sen Wu, Jimeng Sun, and Hang Su. 2012. Cross-domain collaboration recommendation. In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 1285­1293.
[27] Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. Graphgan: Graph representation learning

with generative adversarial nets. In AAAI. 2508­2515. [28] Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, and Minyi Guo. 2017. Joint
Topic-Semantic-aware Social Recommendation for Online Voting. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. ACM, 347­356. [29] Hao Wang, Naiyan Wang, and Dit-Yan Yeung. 2015. Collaborative deep learning for recommender systems. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 1235­1244. [30] Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, and Qi Liu. 2018. Shine: Signed heterogeneous information network embedding for sentiment link prediction. In Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. ACM, 592­600. [31] Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018. RippleNet: Propagating User Preferences on the Knowledge Graph for Recommender Systems. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. ACM. [32] Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. 2018. DKN: Deep Knowledge-Aware Network for News Recommendation. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 1835­1844. [33] Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering 29, 12 (2017), 2724­2743. [34] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network for Ad Click Predictions. In Proceedings of the ADKDD'17. ACM, 12. [35] Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph and text jointly embedding. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). 1591­1601. [36] Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with Hierarchical Types.. In IJCAI. 2965­2971. [37] Ya Xue, Xuejun Liao, Lawrence Carin, and Balaji Krishnapuram. 2007. Multitask learning for classification with dirichlet process priors. Journal of Machine Learning Research 8, Jan (2007), 35­63. [38] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks?. In Advances in Neural Information Processing Systems. 3320­3328. [39] Xiao Yu, Xiang Ren, Yizhou Sun, Quanquan Gu, Bradley Sturt, Urvashi Khandelwal, Brandon Norick, and Jiawei Han. 2014. Personalized entity recommendation: A heterogeneous information network approach. In Proceedings of the 7th ACM International Conference on Web Search and Data Mining. 283­292. [40] Fuzheng Zhang, Nicholas Jing Yuan, Defu Lian, Xing Xie, and Wei-Ying Ma. 2016. Collaborative knowledge base embedding for recommender systems. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 353­362. [41] Wenlu Zhang, Rongjian Li, Tao Zeng, Qian Sun, Sudhir Kumar, Jieping Ye, and Shuiwang Ji. 2015. Deep model based transfer and multi-task learning for biological image analysis. In 21st ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2015. Association for Computing Machinery. [42] Yu Zhang and Qiang Yang. 2017. A survey on multi-task learning. arXiv preprint arXiv:1707.08114 (2017). [43] Yu Zhang and Dit-Yan Yeung. 2012. A convex formulation for learning task relationships in multi-task learning. arXiv preprint arXiv:1203.3536 (2012). [44] Yu Zhang and Dit-Yan Yeung. 2014. A regularization approach to learning task relationships in multitask learning. ACM Transactions on Knowledge Discovery from Data (TKDD) 8, 3 (2014), 12. [45] Huan Zhao, Quanming Yao, Jianda Li, Yangqiu Song, and Dik Lun Lee. 2017. Metagraph based recommendation fusion over heterogeneous information networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 635­644. [46] Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen. 2015. Aligning knowledge and text embeddings by entity descriptions. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 267­272. [47] Qiang Zhou and Qi Zhao. 2016. Flexible Clustered Multi-Task Learning by Learning Representative Tasks. IEEE Trans. Pattern Anal. Mach. Intell. 38, 2 (2016), 266­278.

<943
 The 15th International Conference on Document Analysis and Recognition

ICDAR 2019 Robust Reading Challenge on Reading Chinese Text on Signboard
Xi Liu1, Rui Zhang1, Yongsheng Zhou1, Qianyi Jiang1, Qi Song1, Nan Li1, Kai Zhou1, Lei Wang1, Dong Wang1, Minghui Liao2, Mingkun Yang2, Xiang Bai2, Baoguang Shi3, Dimosthenis Karatzas4, Shijian Lu5
1Meituan-Dianping Group, China, 2School of EIC, Huazhong University of Science and Technology, China, 3Microsoft Redmond, USA, 4Computer Vision Centre, UAB, Spain, 5Nanyang Technological University, Singapore

Abstract--Chinese scene text reading is one of the most challenging problems in computer vision and has attracted great interest. Different from English text, Chinese has more than 6000 commonly used characters and Chinese characters can be arranged in various layouts with numerous fonts. The Chinese signboards in street view are a good choice for Chinese scene text images since they have different backgrounds, fonts and layouts. We organized a competition called ICDAR2019-ReCTS, which mainly focuses on reading Chinese text on signboard. This report presents the final results of the competition. A large-scale dataset of 25,000 annotated signboard images, in which all the text lines and characters are annotated with locations and transcriptions, were released. Four tasks, namely character recognition, text line recognition, text line detection and end-to-end recognition were set up. Besides, considering the Chinese text ambiguity issue, we proposed a multi ground truth (multi-GT) evaluation method to make evaluation fairer. The competition started on March 1, 2019 and ended on April 30, 2019. 262 submissions from 46 teams are received. Most of the participants come from universities, research institutes, and tech companies in China. There are also some participants from the United States, Australia, Singapore, and Korea. 21 teams submit results for Task 1, 23 teams submit results for Task 2, 24 teams submit results for Task 3, and 13 teams submit results for Task 4. The official website for the competition is http://rrc.cvc.uab.es/?ch=12.
I. INTRODUCTION Texts in natural images carry much important semantic information. Reading text in natural scene images has been widely studied recently since it is an important prerequisite for many content-based image analysis tasks such as photo translation, fine-grained image classification and autonomous driving. It is widely recognized that large-scale, well-annotated datasets are crucial to today's deep learning based techniques. In scene text reading field, many scene text datasets have been collected. Especially for Chinese text reading, more and more Chinese scene text datasets are proposed, such as MSRA-500 [1], RCTW [2], SCUT-CTW1500 [3], CTW [4]. Chinese text reading is a huge challenge task. Different from English text reading, Chinese has more than 6000 commonly used characters. Besides, owing to the Chinese culture, the layouts, arrangements and fonts of Chinese characters are always in a great variety, as shown in Figure 1. The Chinese signboards in street view may be the best source for Chinese scene text images since they have different

Figure 1. Characters with various layouts and fonts. backgrounds, fonts and layouts. In Meituan-Dianping Group, a Chinese leading company for food delivery services, consumer products and retail services, there are many signboard images collected by Meituan business merchants. Based on this, we propose a competition for Chinese text reading on signboard and construct a large-scale challenging natural scene text dataset of 25,000 signboard images. About 200,000 text lines and 600,000 characters are labeled with locations and transcriptions. We set up four tasks for this competition, namely character recognition, text line recognition, text line detection and end-to-end recognition. Besides, we propose a multi ground truth (multi-GT) evaluation method considering the Chinese text ambiguity. As illustrated in Figure 2, it is difficult to determine whether some words should be merged to a text instance or not. We thus provide one or more ground truths for each test image and compare the predicted result with all the ground truths when evaluating. The best matched GT will be used to calculate the evaluation metrics.
Figure 2. Chinese text ambiguity in signboard image.

XXX-X-XXXX-XXXX-X/XX/$XX.00 ©20XX IEEE

944> 2019 

Figure 3. Chinese character test images.

Figure 4. Text line test images.

The competition lasts from March 1st to April 30, 2019. It receives lots of attention from the community. For all the four tasks, there are all together 46 valid teams participating in the competition and hundreds of valid submissions are received. In this report, we will present their evaluation results.
II. DATASET AND ANNOTATIONS The dataset, named ReCTS-25k, comprises 25,000 signboard images. All the images are from Meituan-Dianping Group, collected by Meituan business merchants, using phone cameras under uncontrolled conditions. Different from other datasets, this dataset mainly focuses on Chinese text reading on the signboards. The layout and arrangement of Chinese characters in signboards are much more complex for the sake of aesthetics appearance or highlighting certain elements. Figure 1 shows some example images. We manually annotate the locations and transcriptions for all the text lines and characters in the signboard images. Note that the utterly obscure and small text lines and characters are marked with a difficult flag. Locations are annotated in terms of polygons with four vertices, which are in clockwise order starting from the upper left vertice. Transcriptions are UTF-8 encoded strings. The dataset is split into two subsets. The training set consists of 20,000 images, and the test set consists of 5,000 images. Moreover, 29335 character images and 10789 text lines images, cropped from the 5000 test images, are used for task 1 and task 2 evaluation respectively.
III. CHALLENGE TASKS Robust reading challenge on Chinese signboard consists of four tasks: 1) Character recognition, 2) Text line recognition, 3) Text line detection, 4) End-to-end recognition. Given that Chinese signboards have various layouts, fonts and orientations, character and text line reading are concerned. Therefore, in our competition, character based and text line based tasks are both evaluated. Note that the half-width character and its corresponding full-width character are regarded as one character in the evaluation of task 2 and task 4. Moreover, the English letters are not case sensitive.

A. Task 1 ­ Character Recognition The aim of this task is to recognize characters of the cropped
character images from Chinese signboards. As illustrated in Figure 3, the Chinese characters take the largest portion and are in diverse fonts. Participant is asked to submit a text file containing character results for all test images. The recognition accuracy is given as the metric:

where

N

is

the

 number of

characte r,s

predicted

correctly

(1) and

N is the total number of the test characters.

B. Task 2 ­ Text Line Recognition

The target of text line recognition is to recognize the cropped word images of scene text. The cropped text line images as well as the coordinates of the polygon bounding boxes in the images are given. The given points are arranged in the clockwise order, starting from the top-left point. Figure 4 shows some examples of the test set. The text line images may contain perspective and arbitrary arranged text lines.

The results are evaluated by the Normalized Edit Distance between the recognition result and the ground truth. The edit distances are summarized and divided by the number of test images. The resulting average edit distance is taken as the metric and is formulated as follows:









 



 

,

(2)

wptrruhetedhri,ectedissttteahxnedt tsolitnfaoelrnautnhmdebLeredvoeefnntoeshtxettseliintnheeDs.icsotarnrecsep,onddinegnogtersoutnhde

C. Task 3 ­ Text Line Detection The aim of this task is to localize text lines in the signboard.
The input image is the full signboard images. The detection results submitted by the participants are required to give four vertices of the polygon in clockwise order.
In some signboard, there always exist the following case, as shown in Figure 2. It is difficult to determine whether the
boxes "" """""""" should be merged to a large text box or not. Therefore, we regard the two cases (Figure 2(a) and Figure 2(b)) as correct ground truth. We provide one or more ground truths for each test image. When

<945

evaluating, we compare the predicted result with all the ground truths and use the best matched one to calculate the evaluation metrics.
Following the evaluation protocols of ICDAR 2017RCTW [2] dataset, the detection task is evaluated in terms of Precision, Recall and F-score with intersection-over-union (IoU) threshold of 0.5 and 0.7. The F-score at IoU=0.5 will be used as the only metric for the final ranking. All detected or missed ignored ground truths will not contribute to the evaluation result. D. Task 4 ­ End-to-End Recognition
The aim of this task is to localize and recognize every text instance in the signboard. The input image is the full signboard images. Participants are required to submit the text file containing all the recognized text lines locations and transcriptions for each test image. Similar to Task 3, the locations are four vertices in clock-wise order and the transcripts are UTF-8 encoded strings.
The evaluation process consists of two steps. First, each detection is matched to a ground truth polygon that has the maximum IOU, or it is matched to `None' if none IOU is larger than 0.5. If multiple detections are matched to the same groundtruth, only the one with the maximum IOU will be kept and the others are recorded as `None'. Then, we calculate the edit distances between all matching pairs by Formula (2). Since one test image may have multiple ground truths, as stated in Task 3, we also compare the predicted result with all the ground truths and use the best matched one to calculate the evaluation metrics.
IV. ORGANIZATION The competition starts on March 1, 2019, when the RRC website is ready and open for registration. The training set is released on March 18, the first part of test set is released on April 12 and the second part of test set released on April 20. We revise the test set more than once to fixed some errors before releasing the test set. The RRC website opens for result submission on April 20 and closes at 11:59 PM PST, April 30. There are all together 46 valid teams participated in the competition. Most of the participants come from universities, research institutes, and tech companies in China. There are also some participants from the United States, Australia, Singapore, and Korea. All the teams submit their results through the RRC website. Each team is allowed to submit 5 results at most and we choose the best result among the 5 results as the final result. 21 teams submit results for Task 1, 23 teams submit results for Task 2, 24 teams submit results for Task 3, and 13 teams submit results for Task 4.
V. SUBMISSIONS AND RESULTS The evaluation script is implemented in Python. We run the script to evaluate all the submissions. Table I summarizes the top 5 results of Task 1. Methods are ranked by their accuracy. Table II summarizes the top 5 results of Task 2. Methods are ranked by their normalized edit distance. Table III summarizes the top 5 results of Task 3. Methods are ranked by their F-score.

Table IV summarizes the top 5 results of Task 4. Methods are ranked by their normalized edit distance. You can view the complete ranking in the home page of the competition https://rrc.cvc.uab.es/?ch=12.
A. Top 3 submissions for Task 1 1. "BASELINE v1" (USTC-iFLYTEK) The method uses
image classification methods and its ensemble. 2. "Amap_CVLab" (Alibaba AMAP) The method adds
res-block [5] (for the lower dimension feature collapse avoiding) and se-block [6]. Their training dataset contains both the ReCTS-25k and other data.
3. "TPS-ResNet v1" (Clova AI OCR Team, NAVER/LINE Corp) The method uses Thin-platespline(TPS) [7] based Spatial transformer network (STN) [8], which normalizes the input text images. They use ResNet [5], BiLSTM [9] and attention mechanism. Their training dataset contains the Chinese synthetic datasets (MJSynth and SynthText [10]) and real dataset (ArT [11], LSVT [12], RCTW [2], ReCTS-25k).
B. Top 3 submissions for Task 2 1. "SANHL" (South China University of Technology,
Northwestern Polytechnical University, The University of Adelaide, Lenovo and Huawei) The method uses an ensemble framework, which consists of attention-based network, transformer network and CTC-based [13] network. Apart from the official training dataset, about 2 million synthesized samples are used for training.
2. "Tencent-DPPR Team" (Tencent-DPPR Team) The method uses five types of deep models, which mainly include CTC-based nets and multi-head attention based nets. All samples are resized to the same height before feeding into the network. Furthermore, besides ReCTS, they use a synthetic dataset containing more than fifty million images, as well as open-source datasets including LSVT [12], COCO-Text [14], RCTW [2] and ICPR-2018-MTWI. In terms of data augmentation, they mainly use Gaussian blur, Gaussian noise and so on.
3. "HUST_VLRGROUP" (Huazhong University of Science and Technology) A CRNN based method.
C. Top 3 submissions for Task 3 1. "SANHL_v4" (South China University of Technology,
The University of Adelaide, Northwestern Polytechnical University, Lenovo, HUAWEI) The method uses a sequentialfree box discretization method to localize the text instances. Multi-scale testing and model ensemble are used to generate the final result. Their training dataset contains LSVT [12], ArT [11], MLT [15] and ReCTS-25k.
2. "Tencent-DPPR Team" (Tencent Data Platform Precision Recommendation) Their text detector is based on two-stage method with multi-scale training policy, and ResNet101 [5] is used as the backbone network. They use feature pyramid layers [16] to extract features instead of choosing one layer according to box sizes. They use LSVT [12] pre-trained model.

946> 2019 

3. "Amap-CVLab" (Alibaba AMAP, Alibaba DAMO Academy for Discovery, Adventure, Momentum and Outlook) The method is based on Mask R-CNN [17]. Their training dataset contains RCTW[2], ICDAR2017-MLT[15], LSVT[12], ReCTS-25k. D. Top 3 submissions for Task 4
1. "Tencent-DPPR Team" (Tencent-DPPR Team) In the detection part, they use a text detector based on two-stage method. This method uses ResNet101 [5] as feature extractor, and they design a policy to help proposals select feature pyramid layers [16] to extract features instead of choosing one layer according to box sizes. In detection ensemble stage, they apply a multi-scale test method with different backbones. When ensembling all the results, they develop an approach to vote boxes after scoring each box. In the recognition part, they use an ensemble model, which includes CTC-based nets and multihead attention based nets. For this task, they use the predicted confidence scores of cropped words and the ensemble results to select the reliable one among results predicted by all models.
2. "SANHL" (South China University of Technology, Northwestern Polytechnical University, The University of Adelaide, Lenovo and Huawei) The method firstly detect possible text lines, and then predict strings by an ensembled recognition model.
3. "HUST_VLRGROUP" (Huazhong University of Science and Technology) The method uses Mask R-CNN as text detector and a CRNN based approach to predict strings. E. Baseline submissions
For reference, we submit a baseline method to Task 1, Task 2, Task 3 and Task 4 respectively. The methods are implemented by ourselves. Their results are shown in Table I, II, III and IV.
For Task 1, the character Recognition method is based on the densely connected convolutional network (DenseNet) [18]. Our network inherits from the DenseNet-169 network model with dense blocks, but we reduce the number of last dense block to 24 and all the growth rates in the networks are 32. The training dataset consists of ReCTS and synthetic data.
For Task 2, We took the Chinese text line recognition as a sequence recognition task. We utilized a modified version of Inception-V4 [19], integrated with attention module to extract feature maps. The CTC layer for transcription is adopted. The baseline result is obtained by a single recognition model, the training dataset consists of ReCTS, RCTW [2], and LSVT [12], no synthetic data is utilized.
For Task 3, the text detection method is based on SEG-FPN [20] and Pixel-link [21]. We build a unified framework, which combines pixel link and segment link in feature pyramid network to detect scene text. The training dataset only consists of ReCTS.
For Task 4, we first detect the text line in the image. If the text line is horizontal, recognize it by the line recognition model; if the text line is vertical, character detection and character recognition model will be used. The text line detection part is the same as that for Task 3, the character

recognition part is the same as that for Task 1, and the text line recognition part is the same as that for Task 2. A Faster-RCNN [22] based detection approach is adopted to detect Chinese character regions.
VI. CONCLUSIONS
We organize the competition on reading Chinese text on signboard (ReCTS). A large-scale challenging natural scene text dataset of 25,000 signboard images are released and four tasks are set up. We also propose a multi-GT evaluation strategy intended for Chinese text ambiguity. During the challenge, we receive hundreds of submissions from 46 teams, which shows the broad interest in the community. In the future, we plan to make the evaluation scripts available on the website https://rrc.cvc.uab.es/ and users can get the evaluation results shortly after they submit the results to the website.
REFERENCES
1. C. Yao, X. Bai, W. Liu, Y. Ma, Z. Tu, "Detecting Texts of Arbitrary Orientations in Natural Images," CVPR, 2012.
2. B. Shi, C. Yao, M. Liao, M. Yang, P. Xu, L. Cui, S. J. Belongie, S. Lu, X. Bai, "ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)," ICDAR, 2017.
3. Y. L. Liu, L. W. Jin, S. T. Zhang, S. Zhang, "Detecting Curve Text in the Wild: New Dataset and New Solution," arXiv, 2017.
4. T. L. Yuan, Z. Zhu, K. Xu, "Chinese Text in the Wild," arXiv, 2018. 5. K. He, X. Zhang, S. Ren, "Deep Residual Learning for Image
Recognition," CVPR, 2016. 6. J. Hu, L. Shen, G. Sun, "Squeeze-and-Excitation Networks," TPAMI,
2017. 7. F. L. Bookstein, "Principal warps: Thin-plate splines and the
decomposition of deformations," TPAMI, 1989. 8. M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuoglu, "Spatial
Transformer Networks," CVPR, 2016. 9. A. Graves, J. Schmidhuber, "Framewise phoneme classification with
bidirectional LSTM and other neural network architectures," Neural Networks, 2005. 10. A. Gupta, A. Vedaldi, and A. Zisserman, "Synthetic data for text localisation in natural images," In CVPR, pages 2315­2324, 2016. 11. https://rrc.cvc.uab.es/?ch=14, 2019. 12. https://rrc.cvc.uab.es/?ch=16, 2019. 13. A. Graves, S. Fernandez, F. Gomez, J. Schmidhuber, "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks," ICML, 2006. 14. V. Andreas, M. Tomas, N. Lukas, M. Jiri, B. Serge, "COCO-Text: Dataset and Benchmark for Text Detection and Recognition in Natural Images," arXiv, 2016. 15. N. Nayef, et al, "ICDAR2017 Robust Reading Challenge on Multi-lingual Scene Text Detection and Script Identification," ICDAR 2017. 16. Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, Serge Belongie, "Feature Pyramid Networks for Object Detection," arXiv preprint. arXiv: 1612.03144, 2017. 17. K. He, G. Gkioxari, P. Dollar, R. Girshick, "Mask r-cnn," ICCV, 2017. 18. G. Hunag, Z. Liu, L. V. DerMaaten, K. Q. Weinberger, "Densely connected convolutional networks." CVPR, 2017. 19. C. Szegedy, et al, "Inception-V4, inception-resnet and the impact of residual connections on learning," AAAI, 2017. 20. X. Liu, R. Zhang, Y. S. Zhou, D. Wang, "Scene Text Detection with Feature Pyramid Network and Linking Segments," ICDAR, 2019. 21. D. Deng, H. Liu, X. Li, and D. Cai, "Pixellink: Detecting scene text via instance segmentation," In AAAI, pages 6773­ 6780, 2018. 22. S. Q. Ren, K. He, R. Girshick, J. Sun, "Faster r-cnn: Towards real-time object detection with region proposal networks," NIPS, 2015.

<947

TABLE I: RESULTS SUMMARY FOR THE TOP-5 SUBMISSIONS OF TASK 1.

Ranking 1 2 3
4 5 Baseline

Team Name BASELINE-v1 Amap_CVLab TPS-ResNet-v1 SANHL_v4 Tencent-DPPR

Affiliation iFLYTEK, University of Science and Technology of China Alibaba AMAP Clova AI OCR Team, NAVER/LINE Corp South China University of Technology, The University of Adelaide, Northwestern Polytechnical University, Lenovo, HUAWEI Tencent (Data Platform Precision Recommendation) Meituan Dianping

Accuracy 0.9737 0.9728 0.9612
0.9594 0.9512 0.9140

TABLE II: RESULTS SUMMARY FOR THE TOP-5 SUBMISSIONS OF TASK 2.

Ranking 1
2 3 4 5 Baseline

Team Name SANHL_v1 Tencent-DPPR HH-Lab-v4 * TPS-ResNet-v1 Baseline-Beihang*

Affiliation South China University of Technology, The University of Adelaide, Northwestern Polytechnical University, Lenovo, HUAWEI Tencent (Data Platform Precision Recommendation) Huazhong University of Science and Technology (Visual and Learning Representation Group) Clova AI OCR Team, NAVER/LINE Corp Beihang University Meituan Dianping

N.E.D 0.9555
0.9486 0.9483 0.9477 0.9437 0.9089

TABLE III: RESULTS SUMMARY FOR THE TOP-5 SUBMISSIONS OF TASK 3.

Ranking
1
2 3 4 5 Baseline

Team Name SANHL_v4 Tencent-DPPR Amap-CVLab HH-Lab * maskrcnn_text *

Affiliation South China University of Technology, The University of Adelaide, Northwestern Polytechnical University, Lenovo, HUAWEI Tencent (Data Platform Precision Recommendation) Alibaba AMAP, Alibaba DAMO Academy for Discovery, Adventure, Momentum and Outlook Huazhong University of Science and Technology (Visual and Learning Representation Group) Huazhong University of Science and Technology (Media and Communication Laboratory, Text detection) Meituan Dianping

F-score
0.9336
0.9303 0.9250 0.9127 0.9102 0.9001

TABLE IV: RESULTS SUMMARY FOR THE TOP-5 SUBMISSIONS OF TASK 4.

Ranking 1

Team Name Tencent-DPPR

2

SANHL_v1

3

HH-Lab *

4

baseline_Beihang *

5

SECAI *

Baseline
* means student contestant

Affiliation Tencent (Data Platform Precision Recommendation) South China University of Technology, The University of Adelaide, Northwestern Polytechnical University, Lenovo, HUAWEI Huazhong University of Science and Technology (Visual and Learning Representation Group) Beihang University Institute of Information Engineering, Chinese Academy of Sciences, University of Science & Technology Beijing Meituan Dianping

N.E.D 0.8150 0.8144
0.7943 0.7661 0.7437 0.7298

